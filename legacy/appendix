\medskip


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Appendix / supplemental material}

\subsection{Schematic of MR-LoRA}
Figure~\ref{fig:schematic} 


\subsection{Pseudo-code for MR-LoRA Forward Pass}
Algorithm~\ref{alg:forward} details the forward pass of a linear layer equipped with MR-LoRA. In practice, the sum over blocks can be computed efficiently by concatenating the $B_k$ and $A_k$ matrices along the rank dimension and performing a single batched low-rank multiplication.

\begin{algorithm}[ht]
\caption{MR-LoRA forward pass for a single linear layer}
\label{alg:forward}
\begin{algorithmic}[1]
\REQUIRE Input $x \in \mathbb{R}^{d_{\text{in}}}$, frozen weight $W_0 \in \mathbb{R}^{d_{\text{out}} \times d_{\text{in}}}$, blocks $\{(B_k, A_k, \lambda_k)\}_{k=1}^K$ with $B_k \in \mathbb{R}^{d_{\text{out}} \times r_k}$, $A_k \in \mathbb{R}^{r_k \times d_{\text{in}}}$
\STATE $y \gets W_0 x$ \COMMENT{Frozen base projection}
\FOR{$k = 1$ to $K$}
    \STATE $y \gets y + \lambda_k \cdot (B_k (A_k x))$ \COMMENT{Add scaled low-rank update}
\ENDFOR
\STATE \RETURN $y$
\end{algorithmic}
\end{algorithm}

\subsection{Additional Results}
\subsubsection{Comparison of LoRA Variants}
Tables~\ref{tab:teacher-variants} and~\ref{tab:student-variants} compare the performance and parameter efficiency of various LoRA variants in teacher fine-tuning and student distillation settings, respectively. All values are expressed as percentages of full fine-tuning (FFT) and averaged across BERT, RoBERTa, and DeBERTa model families. Note that only LoRA and MR-LoRA were evaluated in the student distillation setting; other variants lack student-distillation results.

In teacher fine-tuning (Table~\ref{tab:teacher-variants}), MR-LoRA variants achieve the highest GLUE scores relative to FFT, with MR-LoRA-RS reaching 91.6\% of FFT performance using only 0.57\% of FFT parameters. Standard LoRA variants (LoRA, DoRA, OLoRA) achieve 87--89\% of FFT performance with 0.46\% of parameters, while AdaLoRA performs significantly worse (77.7\% of FFT). The multi-rank approaches (MR-LoRA, MR-LoRA-RS) consistently outperform single-rank variants.

In student distillation (Table~\ref{tab:student-variants}), MR-LoRA retains 92.4\% of the teacher's FFT performance with 0.56\% of parameters, outperforming standard LoRA (90.6\% of FFT). This demonstrates that multi-rank decomposition is particularly beneficial for knowledge distillation, where capturing both coarse and fine discrepancies between teacher and student is crucial.

\begin{table}[ht]
\centering
\caption{Average performance and parameter efficiency of LoRA variants in \textit{teacher} fine-tuning, expressed as percentage of full fine-tuning (FFT). Values are averaged across BERT, RoBERTa, and DeBERTa model families.}
\label{tab:teacher-variants}
\begin{tabular}{lcc}
\toprule
Method & GLUE (\% FFT) & Parameters (\% FFT) \\
\midrule
MR-LoRA-RS & 91.6 & 0.567 \\
MR-LoRA & 89.9 & 0.563 \\
RS-LoRA & 89.1 & 0.457 \\
LoRA & 88.3 & 0.457 \\
DoRA & 87.8 & 0.465 \\
OLoRA & 87.7 & 0.457 \\
AdaLoRA & 77.7 & 0.506 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{Average performance and parameter efficiency of LoRA variants in \textit{student} distillation, expressed as percentage of teacher's full fine-tuning (FFT). Values are averaged across BERT, RoBERTa, and DeBERTa model families. Only LoRA and MR-LoRA were evaluated in the student distillation setting; other variants lack student-distillation results.}
\label{tab:student-variants}
\begin{tabular}{lcc}
\toprule
Method & GLUE (\% FFT) & Parameters (\% FFT) \\
\midrule
MR-LoRA & 92.4 & 0.563 \\
LoRA & 90.6 & 0.457 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Hyper-parameter Settings}
All experiments use the following hyper-parameters unless noted otherwise:
\begin{itemize}
    \item Learning rate of FFT: $5\times10^{-5}$
    \item Learning rate of LoRA finetuning: $5\times10^{-4}$
    \item Batch size: 32
    \item Optimizer: AdamW ($\beta_1=0.9$, $\beta_2=0.999$)
    \item Weight decay: 0.01
    \item Training epochs: 3 (GLUE tasks)
    \item Maximum LoRA rank $R$: 64
    \item Number of blocks $K$: 3 (ranks $\{8,4,2\}$)
    \item Distillation weight $\alpha$: 0.5
    \item $\lambda_k$ initialization: sampled from $\mathcal{N}(0, 1)$
\end{itemize}

\subsection{Reproducibility}
All experiments were conducted on NVIDIA A800 GPUs with 80GB memory. We used three random seeds (42, 43, 44) for each configuration; reported scores are averages across seeds. The full set of results (including per-task metrics, efficiency measurements, and aggregated tables) is available in the supplementary material. Code and data will be released upon acceptance.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%