# KD-LoRA Experimental Results Analysis

This directory contains scripts and notebooks for analyzing experimental results for the KD-LoRA paper, specifically generating Table I comparing three finetuning strategies across three encoder-only language models on GLUE tasks.

## Generated Files

### Core Analysis Files

1. **`create_table_i.py`** - Main script to generate Table I from results directory
   - Collects data from `results/` directory
   - Creates multi-index DataFrame matching Table I specification
   - Saves CSV and LaTeX table
   - Run: `python create_table_i.py`

2. **`visualize_results.py`** - Visualization module
   - Generates multiple plots: heatmaps, bar plots, performance deltas
   - Requires `table_i_results.csv` or DataFrame input
   - Run: `python visualize_results.py table_i_results.csv`

3. **`analysis_table_i.ipynb`** - Jupyter notebook with complete analysis
   - Includes data loading, Table I generation, visualizations, and discussion
   - Self-contained: run all cells to reproduce analysis
   - Open with: `jupyter notebook analysis_table_i.ipynb`

4. **`analyze_results.py`** - Initial analysis script (focuses only on mrlora)
   - Used for early exploration
   - Still functional but superseded by `create_table_i.py`

### Output Files (Generated by scripts)

5. **`table_i_results.csv`** - Table I in CSV format with multi-index columns
   - Rows: GLUE tasks (including MNLI matched/mismatched) + Average
   - Columns: Model Family Ã— Strategy (FFT, LoRA, KD-LoRA)

6. **`table_i_latex.tex`** - LaTeX code for Table I ready for paper inclusion

7. **`table_i_heatmap.png`** - Heatmap visualization of Table I
8. **`strategy_comparison_by_model.png`** - Bar plots comparing strategies per model
9. **`model_comparison_by_task.png`** - Bar plots comparing models per task
10. **`performance_delta.png`** - Performance difference (FFT - LoRA, FFT - KD-LoRA)
11. **`average_performance.png`** - Average performance across tasks

12. **`analysis_report.txt`** - Summary report of analysis

## Table I Specification

Table I compares three finetuning strategies:
- **FFT**: Full fine-tuning of teacher model (no PEFT)
- **LoRA**: Low-Rank Adaptation using MrLoRA variant
- **KD-LoRA**: Knowledge Distillation + LoRA using MrLoRA variant

Across three encoder-only language models:
- **BERT**: `bert-base-uncased`
- **RoBERTa**: `roberta-base`
- **DeBERTa-v3**: `deberta-v3-base`

On GLUE benchmark tasks:
- **CoLA**: Matthews correlation coefficient
- **SST-2**: Accuracy
- **MRPC**: Accuracy
- **QQP**: Accuracy
- **STS-B**: Pearson correlation
- **MNLI**: Matched and mismatched accuracy (reported separately)
- **QNLI**: Accuracy
- **RTE**: Accuracy
- **WNLI**: Accuracy

## Data Collection Logic

The analysis focuses on the **MrLoRA variant** as specified in the paper:
- **FFT**: All files in `results/fft/` directory (regardless of `peft` value)
- **LoRA**: Files in `results/lora/` with `peft='mrlora'`
- **KD-LoRA**: Files in `results/kd-lora/` with `peft='mrlora'`

Missing combinations are represented as `NaN` in the table.

## Current Data Availability (as of analysis)

Based on available experiments:
- **FFT**: Complete for all tasks and models
- **LoRA (MrLoRA)**: Available for MRPC, QQP, RTE, QNLI, WNLI, STS-B, MNLI (BERT only)
- **KD-LoRA (MrLoRA)**: Available for MRPC, QQP, RTE, QNLI, WNLI (missing for STS-B, MNLI for RoBERTa/DeBERTa)

Missing experiments (marked as NaN):
- LoRA/KD-LoRA for CoLA, SST-2
- LoRA/KD-LoRA for STS-B (DeBERTa)
- LoRA/KD-LoRA for MNLI (RoBERTa, DeBERTa)

## Usage Instructions

### Quick Start
```bash
# Generate Table I and all visualizations
python create_table_i.py
python visualize_results.py table_i_results.csv

# Or open the notebook for interactive analysis
jupyter notebook analysis_table_i.ipynb
```

### Re-running Analysis
The scripts automatically scan the `results/` directory. As new experiments are added, simply re-run the scripts to update Table I.

### Customizing Analysis
- Modify `create_table_i.py` to change strategy mappings or metric selection
- Edit `visualize_results.py` to add new plot types
- Extend `analysis_table_i.ipynb` for additional analyses

## Dependencies

- Python 3.8+
- pandas
- numpy
- matplotlib
- seaborn
- jupyter (for notebook)
- nbformat (for notebook generation)

Install with:
```bash
pip install pandas numpy matplotlib seaborn jupyter nbformat
```

## Notes

- The analysis respects the instruction to use **only MrLoRA variant** for LoRA and KD-LoRA comparisons
- Missing data is shown as NaN/N/A (not filled with other PEFT variants)
- Multiple seeds are averaged for each combination
- MNLI matched and mismatched accuracies are reported separately as `mnli_m` and `mnli_mm`

## Next Steps for Experimental Completion

1. Run missing MrLoRA experiments for CoLA and SST-2 tasks
2. Complete STS-B experiments for DeBERTa with LoRA/KD-LoRA
3. Run MNLI experiments for RoBERTa and DeBERTa with LoRA/KD-LoRA
4. Consider running additional seeds for statistical significance

After completing experiments, re-run analysis to get complete Table I.

---

*Generated by analysis scripts for KD-LoRA project*