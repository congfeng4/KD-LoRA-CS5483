{
    "eval_loss": 2.966865301132202,
    "eval_pearson": 0.8862132883272639,
    "eval_spearman": 0.8864429311595816,
    "eval_runtime": 0.4112,
    "eval_samples_per_second": 3648.001,
    "eval_steps_per_second": 29.184,
    "epoch": 71.11111111111111,
    "log_history": [
        {
            "loss": 7.0132,
            "grad_norm": 6.0923871994018555,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 2.650822162628174,
            "eval_pearson": 0.38601242423461607,
            "eval_spearman": 0.3701672897729924,
            "eval_runtime": 0.3905,
            "eval_samples_per_second": 3841.601,
            "eval_steps_per_second": 30.733,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 1.113,
            "grad_norm": 25.91266632080078,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 3.0207314491271973,
            "eval_pearson": 0.859934986658305,
            "eval_spearman": 0.8646011308876967,
            "eval_runtime": 0.4011,
            "eval_samples_per_second": 3739.765,
            "eval_steps_per_second": 29.918,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 0.6646,
            "grad_norm": 6.293082237243652,
            "learning_rate": 9.62962962962963e-05,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 3.0592610836029053,
            "eval_pearson": 0.8715130461926321,
            "eval_spearman": 0.8745883526609102,
            "eval_runtime": 0.3853,
            "eval_samples_per_second": 3892.71,
            "eval_steps_per_second": 31.142,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.5349,
            "grad_norm": 27.30895233154297,
            "learning_rate": 9.135802469135802e-05,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 2.860240936279297,
            "eval_pearson": 0.8762412886983365,
            "eval_spearman": 0.8799591546423585,
            "eval_runtime": 0.3862,
            "eval_samples_per_second": 3884.248,
            "eval_steps_per_second": 31.074,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.4562,
            "grad_norm": 7.863726615905762,
            "learning_rate": 8.641975308641975e-05,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 3.084137439727783,
            "eval_pearson": 0.8807866875871904,
            "eval_spearman": 0.8812765583150529,
            "eval_runtime": 0.3965,
            "eval_samples_per_second": 3782.928,
            "eval_steps_per_second": 30.263,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.3958,
            "grad_norm": 9.019003868103027,
            "learning_rate": 8.148148148148148e-05,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 3.097720146179199,
            "eval_pearson": 0.8786437855869752,
            "eval_spearman": 0.8814813130732867,
            "eval_runtime": 0.3873,
            "eval_samples_per_second": 3873.262,
            "eval_steps_per_second": 30.986,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.356,
            "grad_norm": 4.330350399017334,
            "learning_rate": 7.65432098765432e-05,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 3.1180074214935303,
            "eval_pearson": 0.8808618052327605,
            "eval_spearman": 0.8810327636068187,
            "eval_runtime": 0.2889,
            "eval_samples_per_second": 5192.898,
            "eval_steps_per_second": 41.543,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.3257,
            "grad_norm": 18.739330291748047,
            "learning_rate": 7.160493827160494e-05,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 3.1324753761291504,
            "eval_pearson": 0.8797306217149439,
            "eval_spearman": 0.8842964037954123,
            "eval_runtime": 0.385,
            "eval_samples_per_second": 3896.08,
            "eval_steps_per_second": 31.169,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.3041,
            "grad_norm": 3.676970958709717,
            "learning_rate": 6.666666666666667e-05,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 3.1528308391571045,
            "eval_pearson": 0.8789789377818734,
            "eval_spearman": 0.8809396215952685,
            "eval_runtime": 0.3965,
            "eval_samples_per_second": 3782.821,
            "eval_steps_per_second": 30.263,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.2757,
            "grad_norm": 5.887233734130859,
            "learning_rate": 6.17283950617284e-05,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 2.8784403800964355,
            "eval_pearson": 0.8867408391443444,
            "eval_spearman": 0.8851209152204694,
            "eval_runtime": 0.3938,
            "eval_samples_per_second": 3808.978,
            "eval_steps_per_second": 30.472,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.2623,
            "grad_norm": 4.115793704986572,
            "learning_rate": 5.679012345679012e-05,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 2.9968161582946777,
            "eval_pearson": 0.8865348316210581,
            "eval_spearman": 0.8871904519284142,
            "eval_runtime": 0.4024,
            "eval_samples_per_second": 3727.782,
            "eval_steps_per_second": 29.822,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.2426,
            "grad_norm": 4.946836471557617,
            "learning_rate": 5.185185185185185e-05,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 3.0645413398742676,
            "eval_pearson": 0.8819988046864576,
            "eval_spearman": 0.8812262696006651,
            "eval_runtime": 0.3912,
            "eval_samples_per_second": 3834.327,
            "eval_steps_per_second": 30.675,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 0.2296,
            "grad_norm": 3.487157106399536,
            "learning_rate": 4.691358024691358e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 3.0206732749938965,
            "eval_pearson": 0.8851346196885618,
            "eval_spearman": 0.8847346418365462,
            "eval_runtime": 0.3865,
            "eval_samples_per_second": 3881.358,
            "eval_steps_per_second": 31.051,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "loss": 0.2234,
            "grad_norm": 3.3317103385925293,
            "learning_rate": 4.197530864197531e-05,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 3.094752550125122,
            "eval_pearson": 0.8822183947521695,
            "eval_spearman": 0.8834395453228198,
            "eval_runtime": 0.3834,
            "eval_samples_per_second": 3912.58,
            "eval_steps_per_second": 31.301,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "loss": 0.2122,
            "grad_norm": 4.450754642486572,
            "learning_rate": 3.7037037037037037e-05,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 3.044024705886841,
            "eval_pearson": 0.8834187504549421,
            "eval_spearman": 0.8842806137225954,
            "eval_runtime": 0.2847,
            "eval_samples_per_second": 5269.036,
            "eval_steps_per_second": 42.152,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "loss": 0.2055,
            "grad_norm": 5.122833728790283,
            "learning_rate": 3.209876543209876e-05,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "eval_loss": 2.943347692489624,
            "eval_pearson": 0.886760571378911,
            "eval_spearman": 0.8862847312933848,
            "eval_runtime": 0.3868,
            "eval_samples_per_second": 3877.724,
            "eval_steps_per_second": 31.022,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "train_runtime": 280.0715,
            "train_samples_per_second": 2052.69,
            "train_steps_per_second": 16.067,
            "total_flos": 1.3657946609156096e+16,
            "train_loss": 0.8009199595451355,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "eval_loss": 2.966865301132202,
            "eval_pearson": 0.8862132883272639,
            "eval_spearman": 0.8864429311595816,
            "eval_runtime": 0.4112,
            "eval_samples_per_second": 3648.001,
            "eval_steps_per_second": 29.184,
            "epoch": 71.11111111111111,
            "step": 3200
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "mrlora-lcoef",
        "rank": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.05,
        "type": 1,
        "model_family": "deberta",
        "task": "stsb",
        "seed": 123,
        "student_model_name": "./models/deberta-v3-small",
        "teacher_model_name": "./models/deberta-v3-base",
        "use_olora": false,
        "use_lcoef": true,
        "use_bias": false,
        "train_size": 5749
    },
    "train": {
        "train_time": 280.0715,
        "trainable_params_count": 0.295729,
        "memory_allocated": [
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552,
            590.871552
        ],
        "memory_reserved": [
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736,
            3183.476736
        ]
    },
    "variant": "kd-lora"
}