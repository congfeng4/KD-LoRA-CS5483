{
    "eval_loss": 2.9428882598876953,
    "eval_pearson": 0.885399838232527,
    "eval_spearman": 0.8836884711824945,
    "eval_runtime": 0.3086,
    "eval_samples_per_second": 4860.448,
    "eval_steps_per_second": 38.884,
    "epoch": 80.0,
    "log_history": [
        {
            "loss": 8.5111,
            "grad_norm": 30.644283294677734,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 4.168764114379883,
            "eval_pearson": -0.049718366685726156,
            "eval_spearman": -0.039389516502707334,
            "eval_runtime": 0.3058,
            "eval_samples_per_second": 4904.617,
            "eval_steps_per_second": 39.237,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 2.1287,
            "grad_norm": 13.91051959991455,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.7513017654418945,
            "eval_pearson": 0.8299039841537497,
            "eval_spearman": 0.8372943074976963,
            "eval_runtime": 0.3008,
            "eval_samples_per_second": 4987.136,
            "eval_steps_per_second": 39.897,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 0.8935,
            "grad_norm": 10.265128135681152,
            "learning_rate": 9.62962962962963e-05,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 3.0631237030029297,
            "eval_pearson": 0.8541676859854713,
            "eval_spearman": 0.8550192511792932,
            "eval_runtime": 0.3007,
            "eval_samples_per_second": 4988.828,
            "eval_steps_per_second": 39.911,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.7276,
            "grad_norm": 2.496314764022827,
            "learning_rate": 9.135802469135802e-05,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 2.8385543823242188,
            "eval_pearson": 0.8660424390587148,
            "eval_spearman": 0.8658335412110582,
            "eval_runtime": 0.3015,
            "eval_samples_per_second": 4974.336,
            "eval_steps_per_second": 39.795,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.6357,
            "grad_norm": 10.030080795288086,
            "learning_rate": 8.641975308641975e-05,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 2.915985107421875,
            "eval_pearson": 0.8703863171670146,
            "eval_spearman": 0.8697270960588723,
            "eval_runtime": 0.3015,
            "eval_samples_per_second": 4975.355,
            "eval_steps_per_second": 39.803,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.5783,
            "grad_norm": 7.809011459350586,
            "learning_rate": 8.148148148148148e-05,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 3.050259828567505,
            "eval_pearson": 0.8738465323997642,
            "eval_spearman": 0.8730695819926872,
            "eval_runtime": 0.3006,
            "eval_samples_per_second": 4989.861,
            "eval_steps_per_second": 39.919,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.5411,
            "grad_norm": 5.318299293518066,
            "learning_rate": 7.65432098765432e-05,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 3.1004555225372314,
            "eval_pearson": 0.8743745343846939,
            "eval_spearman": 0.8744449340349365,
            "eval_runtime": 0.298,
            "eval_samples_per_second": 5033.866,
            "eval_steps_per_second": 40.271,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.5136,
            "grad_norm": 8.132567405700684,
            "learning_rate": 7.160493827160494e-05,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 3.0485005378723145,
            "eval_pearson": 0.8792129894535499,
            "eval_spearman": 0.8786961691414882,
            "eval_runtime": 0.3453,
            "eval_samples_per_second": 4343.764,
            "eval_steps_per_second": 34.75,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.4885,
            "grad_norm": 2.1765296459198,
            "learning_rate": 6.666666666666667e-05,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 3.0062639713287354,
            "eval_pearson": 0.8812236170541226,
            "eval_spearman": 0.8795349643626955,
            "eval_runtime": 0.3025,
            "eval_samples_per_second": 4958.295,
            "eval_steps_per_second": 39.666,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.4653,
            "grad_norm": 6.336634159088135,
            "learning_rate": 6.17283950617284e-05,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 2.9106950759887695,
            "eval_pearson": 0.881287064527342,
            "eval_spearman": 0.8793125131749618,
            "eval_runtime": 0.2997,
            "eval_samples_per_second": 5004.877,
            "eval_steps_per_second": 40.039,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.4465,
            "grad_norm": 6.646072864532471,
            "learning_rate": 5.679012345679012e-05,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 2.9422528743743896,
            "eval_pearson": 0.8830701484351194,
            "eval_spearman": 0.8818634843069656,
            "eval_runtime": 0.2985,
            "eval_samples_per_second": 5025.434,
            "eval_steps_per_second": 40.203,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.4259,
            "grad_norm": 4.361300468444824,
            "learning_rate": 5.185185185185185e-05,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 2.991472005844116,
            "eval_pearson": 0.8845037699521191,
            "eval_spearman": 0.8824597775635444,
            "eval_runtime": 0.2967,
            "eval_samples_per_second": 5054.872,
            "eval_steps_per_second": 40.439,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 0.4187,
            "grad_norm": 2.8372793197631836,
            "learning_rate": 4.691358024691358e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 2.9428882598876953,
            "eval_pearson": 0.885399838232527,
            "eval_spearman": 0.8836884711824945,
            "eval_runtime": 0.2992,
            "eval_samples_per_second": 5013.604,
            "eval_steps_per_second": 40.109,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "loss": 0.4093,
            "grad_norm": 3.466398239135742,
            "learning_rate": 4.197530864197531e-05,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 2.954644203186035,
            "eval_pearson": 0.8844224167802804,
            "eval_spearman": 0.8834429544255041,
            "eval_runtime": 0.3453,
            "eval_samples_per_second": 4344.595,
            "eval_steps_per_second": 34.757,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "loss": 0.4,
            "grad_norm": 6.403731822967529,
            "learning_rate": 3.7037037037037037e-05,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 2.932413101196289,
            "eval_pearson": 0.8848112713763991,
            "eval_spearman": 0.883081179718964,
            "eval_runtime": 0.3529,
            "eval_samples_per_second": 4250.895,
            "eval_steps_per_second": 34.007,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "loss": 0.3934,
            "grad_norm": 2.2682290077209473,
            "learning_rate": 3.209876543209876e-05,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "eval_loss": 2.8992440700531006,
            "eval_pearson": 0.8844368258165293,
            "eval_spearman": 0.8822359781671293,
            "eval_runtime": 0.3028,
            "eval_samples_per_second": 4953.388,
            "eval_steps_per_second": 39.627,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "loss": 0.3848,
            "grad_norm": 2.242464542388916,
            "learning_rate": 2.7160493827160493e-05,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "eval_loss": 2.949542999267578,
            "eval_pearson": 0.883736635619358,
            "eval_spearman": 0.8815469856457543,
            "eval_runtime": 0.3012,
            "eval_samples_per_second": 4980.002,
            "eval_steps_per_second": 39.84,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "loss": 0.3763,
            "grad_norm": 5.65142297744751,
            "learning_rate": 2.2222222222222223e-05,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "eval_loss": 2.92885422706604,
            "eval_pearson": 0.8842131866280775,
            "eval_spearman": 0.8822490520918191,
            "eval_runtime": 0.3001,
            "eval_samples_per_second": 4997.697,
            "eval_steps_per_second": 39.982,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "train_runtime": 223.6796,
            "train_samples_per_second": 2570.195,
            "train_steps_per_second": 20.118,
            "total_flos": 1.5312972259786752e+16,
            "train_loss": 1.041027234395345,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "eval_loss": 2.9428882598876953,
            "eval_pearson": 0.885399838232527,
            "eval_spearman": 0.8836884711824945,
            "eval_runtime": 0.3086,
            "eval_samples_per_second": 4860.448,
            "eval_steps_per_second": 38.884,
            "epoch": 80.0,
            "step": 3600
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "rslora",
        "rank": 8,
        "lora_alpha": 8,
        "lora_dropout": 0.05,
        "type": 1,
        "model_family": "deberta",
        "task": "stsb",
        "seed": 123,
        "student_model_name": "./models/deberta-v3-small",
        "teacher_model_name": "./models/deberta-v3-base",
        "train_size": 5749
    },
    "train": {
        "train_time": 223.6796,
        "trainable_params_count": 0.148225,
        "memory_allocated": [
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728
        ],
        "memory_reserved": [
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672
        ]
    },
    "variant": "kd-lora"
}