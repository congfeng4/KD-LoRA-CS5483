{
    "eval_loss": 2.6130564212799072,
    "eval_pearson": 0.8678482224250555,
    "eval_spearman": 0.8660091903870762,
    "eval_runtime": 0.2774,
    "eval_samples_per_second": 5408.223,
    "eval_steps_per_second": 43.266,
    "epoch": 57.77777777777778,
    "log_history": [
        {
            "loss": 5.737,
            "grad_norm": 4.995006084442139,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 2.633488655090332,
            "eval_pearson": 0.22386516876864976,
            "eval_spearman": 0.2287758161708463,
            "eval_runtime": 0.2841,
            "eval_samples_per_second": 5280.65,
            "eval_steps_per_second": 42.245,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 1.6066,
            "grad_norm": 10.221872329711914,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.412668228149414,
            "eval_pearson": 0.8204189679523234,
            "eval_spearman": 0.8182432218666936,
            "eval_runtime": 0.2765,
            "eval_samples_per_second": 5425.283,
            "eval_steps_per_second": 43.402,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 0.8488,
            "grad_norm": 6.694980144500732,
            "learning_rate": 9.62962962962963e-05,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 2.647202491760254,
            "eval_pearson": 0.8544687190186682,
            "eval_spearman": 0.8513202565530008,
            "eval_runtime": 0.2851,
            "eval_samples_per_second": 5261.589,
            "eval_steps_per_second": 42.093,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.741,
            "grad_norm": 4.104348182678223,
            "learning_rate": 9.135802469135802e-05,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 2.5278334617614746,
            "eval_pearson": 0.8591198964230236,
            "eval_spearman": 0.8568284080535529,
            "eval_runtime": 0.2759,
            "eval_samples_per_second": 5435.904,
            "eval_steps_per_second": 43.487,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.6715,
            "grad_norm": 4.557569980621338,
            "learning_rate": 8.641975308641975e-05,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 2.6544644832611084,
            "eval_pearson": 0.8628846099743275,
            "eval_spearman": 0.8598864806762938,
            "eval_runtime": 0.2779,
            "eval_samples_per_second": 5397.931,
            "eval_steps_per_second": 43.183,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.6261,
            "grad_norm": 3.350701332092285,
            "learning_rate": 8.148148148148148e-05,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 2.7889206409454346,
            "eval_pearson": 0.8645359065203035,
            "eval_spearman": 0.8620369872016451,
            "eval_runtime": 0.237,
            "eval_samples_per_second": 6329.904,
            "eval_steps_per_second": 50.639,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.6117,
            "grad_norm": 6.321360111236572,
            "learning_rate": 7.65432098765432e-05,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 2.5996687412261963,
            "eval_pearson": 0.8639949667000921,
            "eval_spearman": 0.8613144960286918,
            "eval_runtime": 0.2432,
            "eval_samples_per_second": 6168.233,
            "eval_steps_per_second": 49.346,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.5792,
            "grad_norm": 2.7015438079833984,
            "learning_rate": 7.160493827160494e-05,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 2.745805263519287,
            "eval_pearson": 0.8673992320820084,
            "eval_spearman": 0.8655227360388619,
            "eval_runtime": 0.2367,
            "eval_samples_per_second": 6336.981,
            "eval_steps_per_second": 50.696,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.5627,
            "grad_norm": 4.0980095863342285,
            "learning_rate": 6.666666666666667e-05,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 2.648979902267456,
            "eval_pearson": 0.867383441225398,
            "eval_spearman": 0.865260175768977,
            "eval_runtime": 0.2824,
            "eval_samples_per_second": 5310.993,
            "eval_steps_per_second": 42.488,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.5455,
            "grad_norm": 3.1984188556671143,
            "learning_rate": 6.17283950617284e-05,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 2.6559383869171143,
            "eval_pearson": 0.8680219071085447,
            "eval_spearman": 0.865856490021418,
            "eval_runtime": 0.279,
            "eval_samples_per_second": 5375.452,
            "eval_steps_per_second": 43.004,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.5267,
            "grad_norm": 7.297240257263184,
            "learning_rate": 5.679012345679012e-05,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 2.5872607231140137,
            "eval_pearson": 0.867576396667746,
            "eval_spearman": 0.865745217313068,
            "eval_runtime": 0.2372,
            "eval_samples_per_second": 6323.491,
            "eval_steps_per_second": 50.588,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.5227,
            "grad_norm": 2.3610763549804688,
            "learning_rate": 5.185185185185185e-05,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 2.6130564212799072,
            "eval_pearson": 0.8678482224250555,
            "eval_spearman": 0.8660091903870762,
            "eval_runtime": 0.2393,
            "eval_samples_per_second": 6268.164,
            "eval_steps_per_second": 50.145,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 0.5015,
            "grad_norm": 4.2191290855407715,
            "learning_rate": 4.691358024691358e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 2.648916006088257,
            "eval_pearson": 0.8673929799784063,
            "eval_spearman": 0.8654732173409732,
            "eval_runtime": 0.2764,
            "eval_samples_per_second": 5427.534,
            "eval_steps_per_second": 43.42,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "train_runtime": 139.8298,
            "train_samples_per_second": 4111.426,
            "train_steps_per_second": 32.182,
            "total_flos": 1.1212281434079232e+16,
            "train_loss": 1.0831536337045522,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 2.6130564212799072,
            "eval_pearson": 0.8678482224250555,
            "eval_spearman": 0.8660091903870762,
            "eval_runtime": 0.2774,
            "eval_samples_per_second": 5408.223,
            "eval_steps_per_second": 43.266,
            "epoch": 57.77777777777778,
            "step": 2600
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "dora",
        "rank": 8,
        "lora_alpha": 8,
        "lora_dropout": 0.05,
        "type": 1,
        "model_family": "roberta",
        "task": "stsb",
        "seed": 2024,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "train_size": 5749
    },
    "train": {
        "train_time": 139.8298,
        "trainable_params_count": 0.748033,
        "memory_allocated": [
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752,
            358.858752
        ],
        "memory_reserved": [
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128,
            1340.080128
        ]
    },
    "variant": "kd-lora"
}