{
    "eval_loss": 2.2485191822052,
    "eval_accuracy": 0.6425992779783394,
    "eval_runtime": 0.086,
    "eval_samples_per_second": 3219.512,
    "eval_steps_per_second": 34.868,
    "epoch": 100.0,
    "log_history": [
        {
            "loss": 1.5722,
            "grad_norm": 0.6642706990242004,
            "learning_rate": 0.0001,
            "epoch": 10.0,
            "step": 200
        },
        {
            "eval_loss": 1.5778417587280273,
            "eval_accuracy": 0.5306859205776173,
            "eval_runtime": 0.0881,
            "eval_samples_per_second": 3144.045,
            "eval_steps_per_second": 34.051,
            "epoch": 10.0,
            "step": 200
        },
        {
            "loss": 1.5316,
            "grad_norm": 0.6149130463600159,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 20.0,
            "step": 400
        },
        {
            "eval_loss": 1.6766245365142822,
            "eval_accuracy": 0.5848375451263538,
            "eval_runtime": 0.0884,
            "eval_samples_per_second": 3135.069,
            "eval_steps_per_second": 33.954,
            "epoch": 20.0,
            "step": 400
        },
        {
            "loss": 1.4131,
            "grad_norm": 2.0253665447235107,
            "learning_rate": 7.777777777777778e-05,
            "epoch": 30.0,
            "step": 600
        },
        {
            "eval_loss": 1.8967773914337158,
            "eval_accuracy": 0.5884476534296029,
            "eval_runtime": 0.0752,
            "eval_samples_per_second": 3684.605,
            "eval_steps_per_second": 39.905,
            "epoch": 30.0,
            "step": 600
        },
        {
            "loss": 1.3173,
            "grad_norm": 1.2321902513504028,
            "learning_rate": 6.666666666666667e-05,
            "epoch": 40.0,
            "step": 800
        },
        {
            "eval_loss": 2.0106139183044434,
            "eval_accuracy": 0.628158844765343,
            "eval_runtime": 0.1133,
            "eval_samples_per_second": 2444.089,
            "eval_steps_per_second": 26.47,
            "epoch": 40.0,
            "step": 800
        },
        {
            "loss": 1.2435,
            "grad_norm": 1.1030515432357788,
            "learning_rate": 5.555555555555556e-05,
            "epoch": 50.0,
            "step": 1000
        },
        {
            "eval_loss": 2.101612091064453,
            "eval_accuracy": 0.6209386281588448,
            "eval_runtime": 0.0987,
            "eval_samples_per_second": 2805.372,
            "eval_steps_per_second": 30.383,
            "epoch": 50.0,
            "step": 1000
        },
        {
            "loss": 1.1822,
            "grad_norm": 3.6424458026885986,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 60.0,
            "step": 1200
        },
        {
            "eval_loss": 2.2485191822052,
            "eval_accuracy": 0.6425992779783394,
            "eval_runtime": 0.0694,
            "eval_samples_per_second": 3992.228,
            "eval_steps_per_second": 43.237,
            "epoch": 60.0,
            "step": 1200
        },
        {
            "loss": 1.1462,
            "grad_norm": 1.0981879234313965,
            "learning_rate": 3.3333333333333335e-05,
            "epoch": 70.0,
            "step": 1400
        },
        {
            "eval_loss": 2.29482364654541,
            "eval_accuracy": 0.6245487364620939,
            "eval_runtime": 0.0911,
            "eval_samples_per_second": 3040.862,
            "eval_steps_per_second": 32.934,
            "epoch": 70.0,
            "step": 1400
        },
        {
            "loss": 1.1106,
            "grad_norm": 1.2118124961853027,
            "learning_rate": 2.2222222222222223e-05,
            "epoch": 80.0,
            "step": 1600
        },
        {
            "eval_loss": 2.360184907913208,
            "eval_accuracy": 0.631768953068592,
            "eval_runtime": 0.1147,
            "eval_samples_per_second": 2414.979,
            "eval_steps_per_second": 26.155,
            "epoch": 80.0,
            "step": 1600
        },
        {
            "loss": 1.0917,
            "grad_norm": 1.1825920343399048,
            "learning_rate": 1.1111111111111112e-05,
            "epoch": 90.0,
            "step": 1800
        },
        {
            "eval_loss": 2.3985700607299805,
            "eval_accuracy": 0.6389891696750902,
            "eval_runtime": 0.0738,
            "eval_samples_per_second": 3753.735,
            "eval_steps_per_second": 40.654,
            "epoch": 90.0,
            "step": 1800
        },
        {
            "loss": 1.0819,
            "grad_norm": 2.367932081222534,
            "learning_rate": 0.0,
            "epoch": 100.0,
            "step": 2000
        },
        {
            "eval_loss": 2.4144599437713623,
            "eval_accuracy": 0.6389891696750902,
            "eval_runtime": 0.0934,
            "eval_samples_per_second": 2964.573,
            "eval_steps_per_second": 32.107,
            "epoch": 100.0,
            "step": 2000
        },
        {
            "train_runtime": 121.9815,
            "train_samples_per_second": 2041.293,
            "train_steps_per_second": 16.396,
            "total_flos": 8710372856954880.0,
            "train_loss": 1.2690263214111328,
            "epoch": 100.0,
            "step": 2000
        },
        {
            "eval_loss": 2.2485191822052,
            "eval_accuracy": 0.6425992779783394,
            "eval_runtime": 0.086,
            "eval_samples_per_second": 3219.512,
            "eval_steps_per_second": 34.868,
            "epoch": 100.0,
            "step": 2000
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "adalora",
        "rank": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.05,
        "type": 1,
        "model_family": "roberta",
        "task": "rte",
        "seed": 42,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "train_size": 2490
    },
    "train": {
        "train_time": 121.9815,
        "trainable_params_count": 1.182338,
        "memory_allocated": [
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584,
            367.107584
        ],
        "memory_reserved": [
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184,
            1189.085184
        ]
    },
    "variant": "kd-lora"
}