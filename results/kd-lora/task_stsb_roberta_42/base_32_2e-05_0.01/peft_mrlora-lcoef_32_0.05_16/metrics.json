{
    "eval_loss": 2.9441452026367188,
    "eval_pearson": 0.8737086751440792,
    "eval_spearman": 0.8720942511750548,
    "eval_runtime": 0.2495,
    "eval_samples_per_second": 6012.069,
    "eval_steps_per_second": 48.097,
    "epoch": 66.66666666666667,
    "log_history": [
        {
            "loss": 4.7395,
            "grad_norm": 23.039426803588867,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 2.629444122314453,
            "eval_pearson": 0.8137437022001042,
            "eval_spearman": 0.8226410766985772,
            "eval_runtime": 0.2516,
            "eval_samples_per_second": 5961.013,
            "eval_steps_per_second": 47.688,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 0.8781,
            "grad_norm": 77.41590118408203,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.872957706451416,
            "eval_pearson": 0.8562511906087394,
            "eval_spearman": 0.8587156019764691,
            "eval_runtime": 0.249,
            "eval_samples_per_second": 6023.962,
            "eval_steps_per_second": 48.192,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 0.6847,
            "grad_norm": 19.071842193603516,
            "learning_rate": 9.62962962962963e-05,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 2.4859941005706787,
            "eval_pearson": 0.8629762741491662,
            "eval_spearman": 0.8612247526950209,
            "eval_runtime": 0.2504,
            "eval_samples_per_second": 5990.009,
            "eval_steps_per_second": 47.92,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.5787,
            "grad_norm": 13.682941436767578,
            "learning_rate": 9.135802469135802e-05,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 2.7910237312316895,
            "eval_pearson": 0.8667794342104466,
            "eval_spearman": 0.8649792720344592,
            "eval_runtime": 0.2552,
            "eval_samples_per_second": 5878.503,
            "eval_steps_per_second": 47.028,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.5149,
            "grad_norm": 7.7584614753723145,
            "learning_rate": 8.641975308641975e-05,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 2.8421998023986816,
            "eval_pearson": 0.8717068639087893,
            "eval_spearman": 0.8694704521102288,
            "eval_runtime": 0.2422,
            "eval_samples_per_second": 6194.475,
            "eval_steps_per_second": 49.556,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.4673,
            "grad_norm": 12.193673133850098,
            "learning_rate": 8.148148148148148e-05,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 2.8153083324432373,
            "eval_pearson": 0.8714682447700806,
            "eval_spearman": 0.8683712843479935,
            "eval_runtime": 0.2485,
            "eval_samples_per_second": 6037.425,
            "eval_steps_per_second": 48.299,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.4105,
            "grad_norm": 15.530149459838867,
            "learning_rate": 7.65432098765432e-05,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 2.8692023754119873,
            "eval_pearson": 0.8736227675388718,
            "eval_spearman": 0.8717693978121097,
            "eval_runtime": 0.2488,
            "eval_samples_per_second": 6029.065,
            "eval_steps_per_second": 48.233,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.3813,
            "grad_norm": 7.84507417678833,
            "learning_rate": 7.160493827160494e-05,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 2.7932636737823486,
            "eval_pearson": 0.8690386778866399,
            "eval_spearman": 0.8671911221748573,
            "eval_runtime": 0.1815,
            "eval_samples_per_second": 8264.454,
            "eval_steps_per_second": 66.116,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.3552,
            "grad_norm": 12.687288284301758,
            "learning_rate": 6.666666666666667e-05,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 2.78298020362854,
            "eval_pearson": 0.8710796863576848,
            "eval_spearman": 0.8695852232267739,
            "eval_runtime": 0.2469,
            "eval_samples_per_second": 6074.839,
            "eval_steps_per_second": 48.599,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.3301,
            "grad_norm": 10.717208862304688,
            "learning_rate": 6.17283950617284e-05,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 2.7292189598083496,
            "eval_pearson": 0.875097689291038,
            "eval_spearman": 0.8731051075993064,
            "eval_runtime": 0.2718,
            "eval_samples_per_second": 5519.073,
            "eval_steps_per_second": 44.153,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.3104,
            "grad_norm": 6.4552836418151855,
            "learning_rate": 5.679012345679012e-05,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 2.7603137493133545,
            "eval_pearson": 0.8732864615567175,
            "eval_spearman": 0.870705927702924,
            "eval_runtime": 0.251,
            "eval_samples_per_second": 5976.546,
            "eval_steps_per_second": 47.812,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.3013,
            "grad_norm": 8.781982421875,
            "learning_rate": 5.185185185185185e-05,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 2.8058314323425293,
            "eval_pearson": 0.8746898096370893,
            "eval_spearman": 0.8723760472573964,
            "eval_runtime": 0.2188,
            "eval_samples_per_second": 6855.656,
            "eval_steps_per_second": 54.845,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 0.2848,
            "grad_norm": 6.799313068389893,
            "learning_rate": 4.691358024691358e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 2.7410552501678467,
            "eval_pearson": 0.8723915543485048,
            "eval_spearman": 0.8708004945640991,
            "eval_runtime": 0.2529,
            "eval_samples_per_second": 5930.329,
            "eval_steps_per_second": 47.443,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "loss": 0.2731,
            "grad_norm": 5.776638507843018,
            "learning_rate": 4.197530864197531e-05,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 2.741097927093506,
            "eval_pearson": 0.8713477173303121,
            "eval_spearman": 0.8694668365089835,
            "eval_runtime": 0.2533,
            "eval_samples_per_second": 5921.822,
            "eval_steps_per_second": 47.375,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "loss": 0.2625,
            "grad_norm": 12.142197608947754,
            "learning_rate": 3.7037037037037037e-05,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 2.778447151184082,
            "eval_pearson": 0.874010202270528,
            "eval_spearman": 0.8716917795249353,
            "eval_runtime": 0.2507,
            "eval_samples_per_second": 5983.105,
            "eval_steps_per_second": 47.865,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "train_runtime": 182.7458,
            "train_samples_per_second": 3145.899,
            "train_steps_per_second": 24.624,
            "total_flos": 1.297804462915584e+16,
            "train_loss": 0.7181573206583659,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 2.9441452026367188,
            "eval_pearson": 0.8737086751440792,
            "eval_spearman": 0.8720942511750548,
            "eval_runtime": 0.2495,
            "eval_samples_per_second": 6012.069,
            "eval_steps_per_second": 48.097,
            "epoch": 66.66666666666667,
            "step": 3000
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "mrlora-lcoef",
        "rank": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.05,
        "type": 1,
        "model_family": "roberta",
        "task": "stsb",
        "seed": 42,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "use_olora": false,
        "use_lcoef": true,
        "use_bias": false,
        "train_size": 5749
    },
    "train": {
        "train_time": 182.7458,
        "trainable_params_count": 0.886321,
        "memory_allocated": [
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704,
            359.944704
        ],
        "memory_reserved": [
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032,
            1711.276032
        ]
    },
    "variant": "kd-lora"
}