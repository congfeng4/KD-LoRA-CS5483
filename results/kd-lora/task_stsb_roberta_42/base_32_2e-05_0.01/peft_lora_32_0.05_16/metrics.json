{
    "eval_loss": 2.625075101852417,
    "eval_pearson": 0.8693135394071451,
    "eval_spearman": 0.8669621659611905,
    "eval_runtime": 0.2151,
    "eval_samples_per_second": 6974.465,
    "eval_steps_per_second": 55.796,
    "epoch": 53.333333333333336,
    "log_history": [
        {
            "loss": 5.8177,
            "grad_norm": 7.065193176269531,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 2.446608304977417,
            "eval_pearson": 0.5265740955746242,
            "eval_spearman": 0.5680673119320448,
            "eval_runtime": 0.1942,
            "eval_samples_per_second": 7723.037,
            "eval_steps_per_second": 61.784,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 1.1021,
            "grad_norm": 33.263427734375,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.6385915279388428,
            "eval_pearson": 0.8468366421022793,
            "eval_spearman": 0.84727652962678,
            "eval_runtime": 0.2186,
            "eval_samples_per_second": 6861.802,
            "eval_steps_per_second": 54.894,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 0.778,
            "grad_norm": 11.63399600982666,
            "learning_rate": 9.62962962962963e-05,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 2.607229471206665,
            "eval_pearson": 0.8628651633893075,
            "eval_spearman": 0.8590106166066669,
            "eval_runtime": 0.2138,
            "eval_samples_per_second": 7014.692,
            "eval_steps_per_second": 56.118,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.6596,
            "grad_norm": 10.690749168395996,
            "learning_rate": 9.135802469135802e-05,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 2.7498645782470703,
            "eval_pearson": 0.8658657632845576,
            "eval_spearman": 0.8624711491689009,
            "eval_runtime": 0.2138,
            "eval_samples_per_second": 7017.25,
            "eval_steps_per_second": 56.138,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.6076,
            "grad_norm": 3.7834911346435547,
            "learning_rate": 8.641975308641975e-05,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 2.676720380783081,
            "eval_pearson": 0.8665730041898154,
            "eval_spearman": 0.8634992811948733,
            "eval_runtime": 0.2146,
            "eval_samples_per_second": 6989.839,
            "eval_steps_per_second": 55.919,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.5473,
            "grad_norm": 3.3471837043762207,
            "learning_rate": 8.148148148148148e-05,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 2.7628414630889893,
            "eval_pearson": 0.8669709395779955,
            "eval_spearman": 0.8641084538682099,
            "eval_runtime": 0.2127,
            "eval_samples_per_second": 7052.593,
            "eval_steps_per_second": 56.421,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.5181,
            "grad_norm": 4.24150276184082,
            "learning_rate": 7.65432098765432e-05,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 2.625075101852417,
            "eval_pearson": 0.8693135394071451,
            "eval_spearman": 0.8669621659611905,
            "eval_runtime": 0.2288,
            "eval_samples_per_second": 6557.309,
            "eval_steps_per_second": 52.458,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.4879,
            "grad_norm": 4.804886817932129,
            "learning_rate": 7.160493827160494e-05,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 2.620856285095215,
            "eval_pearson": 0.8657569569496691,
            "eval_spearman": 0.8625490632750052,
            "eval_runtime": 0.216,
            "eval_samples_per_second": 6945.208,
            "eval_steps_per_second": 55.562,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.4688,
            "grad_norm": 2.6757652759552,
            "learning_rate": 6.666666666666667e-05,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 2.7453625202178955,
            "eval_pearson": 0.8675734217195933,
            "eval_spearman": 0.8646586745783568,
            "eval_runtime": 0.2166,
            "eval_samples_per_second": 6925.187,
            "eval_steps_per_second": 55.401,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.4452,
            "grad_norm": 7.01911735534668,
            "learning_rate": 6.17283950617284e-05,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 2.698317289352417,
            "eval_pearson": 0.8693436803841244,
            "eval_spearman": 0.8665001820731678,
            "eval_runtime": 0.2154,
            "eval_samples_per_second": 6964.421,
            "eval_steps_per_second": 55.715,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.4303,
            "grad_norm": 2.454984426498413,
            "learning_rate": 5.679012345679012e-05,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 2.7567265033721924,
            "eval_pearson": 0.8682739377530189,
            "eval_spearman": 0.8655136538207585,
            "eval_runtime": 0.2144,
            "eval_samples_per_second": 6997.854,
            "eval_steps_per_second": 55.983,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.4208,
            "grad_norm": 6.045833587646484,
            "learning_rate": 5.185185185185185e-05,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 2.7251131534576416,
            "eval_pearson": 0.8677393518321419,
            "eval_spearman": 0.8648010940549877,
            "eval_runtime": 0.2146,
            "eval_samples_per_second": 6988.798,
            "eval_steps_per_second": 55.91,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "train_runtime": 109.3748,
            "train_samples_per_second": 5256.238,
            "train_steps_per_second": 41.143,
            "total_flos": 1.0382413154746368e+16,
            "train_loss": 1.0236149946848552,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 2.625075101852417,
            "eval_pearson": 0.8693135394071451,
            "eval_spearman": 0.8669621659611905,
            "eval_runtime": 0.2151,
            "eval_samples_per_second": 6974.465,
            "eval_steps_per_second": 55.796,
            "epoch": 53.333333333333336,
            "step": 2400
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "lora",
        "rank": 16,
        "lora_alpha": 32,
        "lora_dropout": 0.05,
        "type": 1,
        "model_family": "roberta",
        "task": "stsb",
        "seed": 42,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "train_size": 5749
    },
    "train": {
        "train_time": 109.3748,
        "trainable_params_count": 0.886273,
        "memory_allocated": [
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568,
            360.77568
        ],
        "memory_reserved": [
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032,
            1186.988032
        ]
    },
    "variant": "kd-lora"
}