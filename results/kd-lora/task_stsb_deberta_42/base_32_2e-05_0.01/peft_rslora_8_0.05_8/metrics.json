{
    "eval_loss": 2.9133923053741455,
    "eval_pearson": 0.88748938260845,
    "eval_spearman": 0.8894642774222318,
    "eval_runtime": 0.2983,
    "eval_samples_per_second": 5028.816,
    "eval_steps_per_second": 40.231,
    "epoch": 80.0,
    "log_history": [
        {
            "loss": 9.0812,
            "grad_norm": 29.273794174194336,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 4.636860370635986,
            "eval_pearson": 0.014187177063872035,
            "eval_spearman": 0.018131340139212877,
            "eval_runtime": 0.2988,
            "eval_samples_per_second": 5020.028,
            "eval_steps_per_second": 40.16,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 2.368,
            "grad_norm": 11.381515502929688,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.9425947666168213,
            "eval_pearson": 0.8346767306720487,
            "eval_spearman": 0.8422748203715998,
            "eval_runtime": 0.301,
            "eval_samples_per_second": 4983.225,
            "eval_steps_per_second": 39.866,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 0.7931,
            "grad_norm": 3.02197527885437,
            "learning_rate": 9.62962962962963e-05,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 2.9211220741271973,
            "eval_pearson": 0.8655636833898713,
            "eval_spearman": 0.8688378936707787,
            "eval_runtime": 0.3502,
            "eval_samples_per_second": 4282.874,
            "eval_steps_per_second": 34.263,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.6332,
            "grad_norm": 2.7979588508605957,
            "learning_rate": 9.135802469135802e-05,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 2.9563210010528564,
            "eval_pearson": 0.873362737363223,
            "eval_spearman": 0.8731998963065243,
            "eval_runtime": 0.352,
            "eval_samples_per_second": 4261.597,
            "eval_steps_per_second": 34.093,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.555,
            "grad_norm": 2.5583832263946533,
            "learning_rate": 8.641975308641975e-05,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 3.0008351802825928,
            "eval_pearson": 0.877713143086396,
            "eval_spearman": 0.8783951203671492,
            "eval_runtime": 0.2997,
            "eval_samples_per_second": 5004.75,
            "eval_steps_per_second": 40.038,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.5141,
            "grad_norm": 2.502127170562744,
            "learning_rate": 8.148148148148148e-05,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 2.929192304611206,
            "eval_pearson": 0.8777018018521447,
            "eval_spearman": 0.8776239708874519,
            "eval_runtime": 0.2986,
            "eval_samples_per_second": 5023.175,
            "eval_steps_per_second": 40.185,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.4769,
            "grad_norm": 4.6742329597473145,
            "learning_rate": 7.65432098765432e-05,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 2.967135190963745,
            "eval_pearson": 0.8801564760341566,
            "eval_spearman": 0.8838450731820854,
            "eval_runtime": 0.2976,
            "eval_samples_per_second": 5039.482,
            "eval_steps_per_second": 40.316,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.449,
            "grad_norm": 2.764960527420044,
            "learning_rate": 7.160493827160494e-05,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 2.899399518966675,
            "eval_pearson": 0.885872584653212,
            "eval_spearman": 0.885559038039675,
            "eval_runtime": 0.3051,
            "eval_samples_per_second": 4917.182,
            "eval_steps_per_second": 39.337,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.4265,
            "grad_norm": 3.0814197063446045,
            "learning_rate": 6.666666666666667e-05,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 2.9029531478881836,
            "eval_pearson": 0.8885494013798025,
            "eval_spearman": 0.8881902053175005,
            "eval_runtime": 0.3518,
            "eval_samples_per_second": 4263.365,
            "eval_steps_per_second": 34.107,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.4052,
            "grad_norm": 2.502471923828125,
            "learning_rate": 6.17283950617284e-05,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 2.939490556716919,
            "eval_pearson": 0.8858483541081453,
            "eval_spearman": 0.8861786646475104,
            "eval_runtime": 0.2994,
            "eval_samples_per_second": 5009.727,
            "eval_steps_per_second": 40.078,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.3904,
            "grad_norm": 1.966913104057312,
            "learning_rate": 5.679012345679012e-05,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 2.958850860595703,
            "eval_pearson": 0.8855247635539906,
            "eval_spearman": 0.8863769348236572,
            "eval_runtime": 0.2977,
            "eval_samples_per_second": 5038.264,
            "eval_steps_per_second": 40.306,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.3737,
            "grad_norm": 4.848334312438965,
            "learning_rate": 5.185185185185185e-05,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 2.913874626159668,
            "eval_pearson": 0.8860089019080599,
            "eval_spearman": 0.888296740301948,
            "eval_runtime": 0.2982,
            "eval_samples_per_second": 5030.782,
            "eval_steps_per_second": 40.246,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 0.3671,
            "grad_norm": 2.238905191421509,
            "learning_rate": 4.691358024691358e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 2.9133923053741455,
            "eval_pearson": 0.88748938260845,
            "eval_spearman": 0.8894642774222318,
            "eval_runtime": 0.2978,
            "eval_samples_per_second": 5036.195,
            "eval_steps_per_second": 40.29,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "loss": 0.3545,
            "grad_norm": 3.6238057613372803,
            "learning_rate": 4.197530864197531e-05,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 2.8864190578460693,
            "eval_pearson": 0.8889579613681093,
            "eval_spearman": 0.8888204192703744,
            "eval_runtime": 0.3478,
            "eval_samples_per_second": 4313.199,
            "eval_steps_per_second": 34.506,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "loss": 0.3431,
            "grad_norm": 3.10079026222229,
            "learning_rate": 3.7037037037037037e-05,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 2.9521214962005615,
            "eval_pearson": 0.8877221967416193,
            "eval_spearman": 0.8888572917867442,
            "eval_runtime": 0.3442,
            "eval_samples_per_second": 4357.915,
            "eval_steps_per_second": 34.863,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "loss": 0.3414,
            "grad_norm": 6.035473823547363,
            "learning_rate": 3.209876543209876e-05,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "eval_loss": 2.9805586338043213,
            "eval_pearson": 0.8888340257503391,
            "eval_spearman": 0.8885776276162514,
            "eval_runtime": 0.2969,
            "eval_samples_per_second": 5052.469,
            "eval_steps_per_second": 40.42,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "loss": 0.3357,
            "grad_norm": 3.536768913269043,
            "learning_rate": 2.7160493827160493e-05,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "eval_loss": 2.9305152893066406,
            "eval_pearson": 0.8886062957191593,
            "eval_spearman": 0.8889535752255202,
            "eval_runtime": 0.2981,
            "eval_samples_per_second": 5032.666,
            "eval_steps_per_second": 40.261,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "loss": 0.3286,
            "grad_norm": 5.4647321701049805,
            "learning_rate": 2.2222222222222223e-05,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "eval_loss": 2.911118507385254,
            "eval_pearson": 0.8882746023034452,
            "eval_spearman": 0.8879512994844799,
            "eval_runtime": 0.3027,
            "eval_samples_per_second": 4955.697,
            "eval_steps_per_second": 39.646,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "train_runtime": 219.186,
            "train_samples_per_second": 2622.887,
            "train_steps_per_second": 20.531,
            "total_flos": 1.5312972259786752e+16,
            "train_loss": 1.029819869995117,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "eval_loss": 2.9133923053741455,
            "eval_pearson": 0.88748938260845,
            "eval_spearman": 0.8894642774222318,
            "eval_runtime": 0.2983,
            "eval_samples_per_second": 5028.816,
            "eval_steps_per_second": 40.231,
            "epoch": 80.0,
            "step": 3600
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "rslora",
        "rank": 8,
        "lora_alpha": 8,
        "lora_dropout": 0.05,
        "type": 1,
        "model_family": "deberta",
        "task": "stsb",
        "seed": 42,
        "student_model_name": "./models/deberta-v3-small",
        "teacher_model_name": "./models/deberta-v3-base",
        "train_size": 5749
    },
    "train": {
        "train_time": 219.186,
        "trainable_params_count": 0.148225,
        "memory_allocated": [
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728,
            588.105728
        ],
        "memory_reserved": [
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672,
            2644.508672
        ]
    },
    "variant": "kd-lora"
}