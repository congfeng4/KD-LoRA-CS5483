{
    "eval_loss": 2.957813262939453,
    "eval_pearson": 0.889711551768994,
    "eval_spearman": 0.8870943487383741,
    "eval_runtime": 0.3463,
    "eval_samples_per_second": 4332.087,
    "eval_steps_per_second": 34.657,
    "epoch": 62.22222222222222,
    "log_history": [
        {
            "loss": 9.2599,
            "grad_norm": 39.97169876098633,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 4.84537410736084,
            "eval_pearson": 0.1966630340382818,
            "eval_spearman": 0.20410096637227168,
            "eval_runtime": 0.3495,
            "eval_samples_per_second": 4291.875,
            "eval_steps_per_second": 34.335,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 2.2169,
            "grad_norm": 4.244070529937744,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.9728176593780518,
            "eval_pearson": 0.8268294724630573,
            "eval_spearman": 0.8408039764298407,
            "eval_runtime": 0.3464,
            "eval_samples_per_second": 4330.093,
            "eval_steps_per_second": 34.641,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 0.7904,
            "grad_norm": 6.69216251373291,
            "learning_rate": 9.62962962962963e-05,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 3.2901217937469482,
            "eval_pearson": 0.857213306309458,
            "eval_spearman": 0.8599643805218683,
            "eval_runtime": 0.2997,
            "eval_samples_per_second": 5005.383,
            "eval_steps_per_second": 40.043,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.646,
            "grad_norm": 5.59652042388916,
            "learning_rate": 9.135802469135802e-05,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 3.0635201930999756,
            "eval_pearson": 0.8732148698334454,
            "eval_spearman": 0.8735732577950585,
            "eval_runtime": 0.2998,
            "eval_samples_per_second": 5003.11,
            "eval_steps_per_second": 40.025,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.5631,
            "grad_norm": 3.213656425476074,
            "learning_rate": 8.641975308641975e-05,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 3.064941167831421,
            "eval_pearson": 0.8795367500206748,
            "eval_spearman": 0.8785833921206322,
            "eval_runtime": 0.3043,
            "eval_samples_per_second": 4929.615,
            "eval_steps_per_second": 39.437,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.4994,
            "grad_norm": 2.742626428604126,
            "learning_rate": 8.148148148148148e-05,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 3.168029308319092,
            "eval_pearson": 0.8823441955642355,
            "eval_spearman": 0.8807366783440297,
            "eval_runtime": 0.2993,
            "eval_samples_per_second": 5010.892,
            "eval_steps_per_second": 40.087,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.4697,
            "grad_norm": 2.09020733833313,
            "learning_rate": 7.65432098765432e-05,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 3.0318868160247803,
            "eval_pearson": 0.8838487005765905,
            "eval_spearman": 0.8826086218033578,
            "eval_runtime": 0.303,
            "eval_samples_per_second": 4950.067,
            "eval_steps_per_second": 39.601,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.4432,
            "grad_norm": 5.600006580352783,
            "learning_rate": 7.160493827160494e-05,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 3.040992259979248,
            "eval_pearson": 0.8863096652091393,
            "eval_spearman": 0.8845120304627215,
            "eval_runtime": 0.3058,
            "eval_samples_per_second": 4904.884,
            "eval_steps_per_second": 39.239,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.419,
            "grad_norm": 1.945283055305481,
            "learning_rate": 6.666666666666667e-05,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 3.012708902359009,
            "eval_pearson": 0.8878724812056616,
            "eval_spearman": 0.8856427315135023,
            "eval_runtime": 0.299,
            "eval_samples_per_second": 5017.438,
            "eval_steps_per_second": 40.14,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.3917,
            "grad_norm": 2.965975046157837,
            "learning_rate": 6.17283950617284e-05,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 3.051326274871826,
            "eval_pearson": 0.887450431476861,
            "eval_spearman": 0.8862805245632478,
            "eval_runtime": 0.3021,
            "eval_samples_per_second": 4965.558,
            "eval_steps_per_second": 39.724,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.3835,
            "grad_norm": 3.556628942489624,
            "learning_rate": 5.679012345679012e-05,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 2.9417288303375244,
            "eval_pearson": 0.8884678020340111,
            "eval_spearman": 0.886299171454969,
            "eval_runtime": 0.3002,
            "eval_samples_per_second": 4996.149,
            "eval_steps_per_second": 39.969,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.362,
            "grad_norm": 4.29113245010376,
            "learning_rate": 5.185185185185185e-05,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 2.912717819213867,
            "eval_pearson": 0.8886639896447017,
            "eval_spearman": 0.8865257189474302,
            "eval_runtime": 0.3055,
            "eval_samples_per_second": 4909.907,
            "eval_steps_per_second": 39.279,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 0.3503,
            "grad_norm": 2.123396635055542,
            "learning_rate": 4.691358024691358e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 2.9246890544891357,
            "eval_pearson": 0.8891292877757407,
            "eval_spearman": 0.8865570013758376,
            "eval_runtime": 0.2996,
            "eval_samples_per_second": 5007.303,
            "eval_steps_per_second": 40.058,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "loss": 0.3357,
            "grad_norm": 2.1828508377075195,
            "learning_rate": 4.197530864197531e-05,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 2.957813262939453,
            "eval_pearson": 0.889711551768994,
            "eval_spearman": 0.8870943487383741,
            "eval_runtime": 0.3493,
            "eval_samples_per_second": 4294.043,
            "eval_steps_per_second": 34.352,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "train_runtime": 166.0434,
            "train_samples_per_second": 3462.349,
            "train_steps_per_second": 27.101,
            "total_flos": 1.1910089535389696e+16,
            "train_loss": 1.2236356244768416,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 2.957813262939453,
            "eval_pearson": 0.889711551768994,
            "eval_spearman": 0.8870943487383741,
            "eval_runtime": 0.3463,
            "eval_samples_per_second": 4332.087,
            "eval_steps_per_second": 34.657,
            "epoch": 62.22222222222222,
            "step": 2800
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "olora",
        "rank": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.05,
        "type": 1,
        "model_family": "deberta",
        "task": "stsb",
        "seed": 2024,
        "student_model_name": "./models/deberta-v3-small",
        "teacher_model_name": "./models/deberta-v3-base",
        "train_size": 5749
    },
    "train": {
        "train_time": 166.0434,
        "trainable_params_count": 0.148225,
        "memory_allocated": [
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016,
            588.630016
        ],
        "memory_reserved": [
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912,
            2634.022912
        ]
    },
    "variant": "kd-lora"
}