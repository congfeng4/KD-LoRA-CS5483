{
    "eval_loss": 0.4257921278476715,
    "eval_pearson": 0.9045480253213456,
    "eval_spearman": 0.9057969613389567,
    "eval_runtime": 0.977,
    "eval_samples_per_second": 1535.234,
    "eval_steps_per_second": 12.282,
    "epoch": 44.44444444444444,
    "log_history": [
        {
            "loss": 5.545,
            "grad_norm": 46.13009262084961,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 1.6460152864456177,
            "eval_pearson": 0.7175320666086765,
            "eval_spearman": 0.6682841131122709,
            "eval_runtime": 1.0927,
            "eval_samples_per_second": 1372.788,
            "eval_steps_per_second": 10.982,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 0.7935,
            "grad_norm": 6.760507106781006,
            "learning_rate": 0.00017777777777777779,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 0.7667760252952576,
            "eval_pearson": 0.8837454610124903,
            "eval_spearman": 0.8873141228257265,
            "eval_runtime": 1.0974,
            "eval_samples_per_second": 1366.927,
            "eval_steps_per_second": 10.935,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 0.3812,
            "grad_norm": 3.8875842094421387,
            "learning_rate": 0.0001925925925925926,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 0.5042548179626465,
            "eval_pearson": 0.9065419285372387,
            "eval_spearman": 0.9082843807671124,
            "eval_runtime": 1.0751,
            "eval_samples_per_second": 1395.216,
            "eval_steps_per_second": 11.162,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.2808,
            "grad_norm": 3.9981801509857178,
            "learning_rate": 0.00018271604938271605,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 0.5467886328697205,
            "eval_pearson": 0.9083724472283013,
            "eval_spearman": 0.9092787274205641,
            "eval_runtime": 1.0297,
            "eval_samples_per_second": 1456.696,
            "eval_steps_per_second": 11.654,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.2242,
            "grad_norm": 3.558330535888672,
            "learning_rate": 0.0001728395061728395,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 0.43623411655426025,
            "eval_pearson": 0.909237013829954,
            "eval_spearman": 0.9110782009948611,
            "eval_runtime": 0.9837,
            "eval_samples_per_second": 1524.807,
            "eval_steps_per_second": 12.198,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.1862,
            "grad_norm": 2.782824754714966,
            "learning_rate": 0.00016296296296296295,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 0.4638993442058563,
            "eval_pearson": 0.9106406908179246,
            "eval_spearman": 0.9111481589984253,
            "eval_runtime": 1.0765,
            "eval_samples_per_second": 1393.432,
            "eval_steps_per_second": 11.147,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.1658,
            "grad_norm": 1.9765949249267578,
            "learning_rate": 0.0001530864197530864,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 0.43341299891471863,
            "eval_pearson": 0.9099349332256609,
            "eval_spearman": 0.909259358488288,
            "eval_runtime": 1.0157,
            "eval_samples_per_second": 1476.876,
            "eval_steps_per_second": 11.815,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.1418,
            "grad_norm": 1.7677249908447266,
            "learning_rate": 0.00014320987654320989,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 0.4569692313671112,
            "eval_pearson": 0.9085979449486021,
            "eval_spearman": 0.9083124735832104,
            "eval_runtime": 1.0511,
            "eval_samples_per_second": 1427.105,
            "eval_steps_per_second": 11.417,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.1292,
            "grad_norm": 2.2443201541900635,
            "learning_rate": 0.00013333333333333334,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 0.4460827708244324,
            "eval_pearson": 0.9085520419413381,
            "eval_spearman": 0.9077293628971242,
            "eval_runtime": 1.0254,
            "eval_samples_per_second": 1462.819,
            "eval_steps_per_second": 11.703,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.1159,
            "grad_norm": 1.1697123050689697,
            "learning_rate": 0.0001234567901234568,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 0.46936532855033875,
            "eval_pearson": 0.9057859480971084,
            "eval_spearman": 0.9061990331590801,
            "eval_runtime": 1.0465,
            "eval_samples_per_second": 1433.302,
            "eval_steps_per_second": 11.466,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "train_runtime": 553.4144,
            "train_samples_per_second": 1038.824,
            "train_steps_per_second": 8.131,
            "total_flos": 1.689742005501952e+16,
            "train_loss": 0.7963476591110229,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 0.4257921278476715,
            "eval_pearson": 0.9045480253213456,
            "eval_spearman": 0.9057969613389567,
            "eval_runtime": 0.977,
            "eval_samples_per_second": 1535.234,
            "eval_steps_per_second": 12.282,
            "epoch": 44.44444444444444,
            "step": 2000
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "mrlora-lcoef",
        "rank": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.05,
        "type": 2,
        "model_family": "deberta",
        "task": "stsb",
        "seed": 2024,
        "student_model_name": "./models/deberta-v3-small",
        "teacher_model_name": "./models/deberta-v3-base",
        "use_olora": false,
        "use_lcoef": true,
        "use_bias": false,
        "train_size": 5749
    },
    "train": {
        "train_time": 553.4144,
        "trainable_params_count": 0.295753,
        "memory_allocated": [
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848,
            762.190848
        ],
        "memory_reserved": [
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384,
            5593.104384
        ]
    },
    "variant": "lora"
}