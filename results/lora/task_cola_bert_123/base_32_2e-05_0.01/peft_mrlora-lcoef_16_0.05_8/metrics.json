{
    "eval_loss": 0.6977540850639343,
    "eval_matthews_correlation": 0.5552845087956161,
    "eval_runtime": 0.4683,
    "eval_samples_per_second": 2227.07,
    "eval_steps_per_second": 19.217,
    "epoch": 38.80597014925373,
    "log_history": [
        {
            "loss": 0.6449,
            "grad_norm": 1.4385855197906494,
            "learning_rate": 5.970149253731343e-05,
            "epoch": 2.9850746268656714,
            "step": 200
        },
        {
            "eval_loss": 0.5048548579216003,
            "eval_matthews_correlation": 0.37295466564828866,
            "eval_runtime": 0.6863,
            "eval_samples_per_second": 1519.78,
            "eval_steps_per_second": 13.114,
            "epoch": 2.9850746268656714,
            "step": 200
        },
        {
            "loss": 0.4552,
            "grad_norm": 3.8118557929992676,
            "learning_rate": 0.00011940298507462686,
            "epoch": 5.970149253731344,
            "step": 400
        },
        {
            "eval_loss": 0.47704070806503296,
            "eval_matthews_correlation": 0.48969105795935336,
            "eval_runtime": 0.5943,
            "eval_samples_per_second": 1755.089,
            "eval_steps_per_second": 15.145,
            "epoch": 5.970149253731344,
            "step": 400
        },
        {
            "loss": 0.379,
            "grad_norm": 1.8785396814346313,
            "learning_rate": 0.0001791044776119403,
            "epoch": 8.955223880597014,
            "step": 600
        },
        {
            "eval_loss": 0.4287417531013489,
            "eval_matthews_correlation": 0.5521589562965672,
            "eval_runtime": 0.574,
            "eval_samples_per_second": 1816.932,
            "eval_steps_per_second": 15.678,
            "epoch": 8.955223880597014,
            "step": 600
        },
        {
            "loss": 0.3262,
            "grad_norm": 2.446617603302002,
            "learning_rate": 0.00019568822553897182,
            "epoch": 11.940298507462687,
            "step": 800
        },
        {
            "eval_loss": 0.422436386346817,
            "eval_matthews_correlation": 0.5735110679870338,
            "eval_runtime": 0.5834,
            "eval_samples_per_second": 1787.93,
            "eval_steps_per_second": 15.428,
            "epoch": 11.940298507462687,
            "step": 800
        },
        {
            "loss": 0.268,
            "grad_norm": 2.9921505451202393,
            "learning_rate": 0.00018905472636815922,
            "epoch": 14.925373134328359,
            "step": 1000
        },
        {
            "eval_loss": 0.44724878668785095,
            "eval_matthews_correlation": 0.5657894359264041,
            "eval_runtime": 0.4592,
            "eval_samples_per_second": 2271.11,
            "eval_steps_per_second": 19.597,
            "epoch": 14.925373134328359,
            "step": 1000
        },
        {
            "loss": 0.2199,
            "grad_norm": 3.380117893218994,
            "learning_rate": 0.0001824212271973466,
            "epoch": 17.91044776119403,
            "step": 1200
        },
        {
            "eval_loss": 0.49488556385040283,
            "eval_matthews_correlation": 0.555617951053262,
            "eval_runtime": 0.4437,
            "eval_samples_per_second": 2350.755,
            "eval_steps_per_second": 20.285,
            "epoch": 17.91044776119403,
            "step": 1200
        },
        {
            "loss": 0.185,
            "grad_norm": 3.808722496032715,
            "learning_rate": 0.000175787728026534,
            "epoch": 20.895522388059703,
            "step": 1400
        },
        {
            "eval_loss": 0.5294426083564758,
            "eval_matthews_correlation": 0.5857056435790745,
            "eval_runtime": 0.4918,
            "eval_samples_per_second": 2120.8,
            "eval_steps_per_second": 18.3,
            "epoch": 20.895522388059703,
            "step": 1400
        },
        {
            "loss": 0.1572,
            "grad_norm": 4.063817501068115,
            "learning_rate": 0.0001691542288557214,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "eval_loss": 0.5677366256713867,
            "eval_matthews_correlation": 0.5905946226995304,
            "eval_runtime": 0.5163,
            "eval_samples_per_second": 2020.073,
            "eval_steps_per_second": 17.431,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "loss": 0.1403,
            "grad_norm": 1.7272536754608154,
            "learning_rate": 0.00016252072968490878,
            "epoch": 26.865671641791046,
            "step": 1800
        },
        {
            "eval_loss": 0.5761303305625916,
            "eval_matthews_correlation": 0.5750821529922306,
            "eval_runtime": 0.4435,
            "eval_samples_per_second": 2351.985,
            "eval_steps_per_second": 20.295,
            "epoch": 26.865671641791046,
            "step": 1800
        },
        {
            "loss": 0.1164,
            "grad_norm": 3.690821409225464,
            "learning_rate": 0.00015588723051409618,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "eval_loss": 0.6362676024436951,
            "eval_matthews_correlation": 0.5861472777603488,
            "eval_runtime": 0.4794,
            "eval_samples_per_second": 2175.463,
            "eval_steps_per_second": 18.772,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "loss": 0.1075,
            "grad_norm": 4.750306606292725,
            "learning_rate": 0.0001492537313432836,
            "epoch": 32.83582089552239,
            "step": 2200
        },
        {
            "eval_loss": 0.6567235589027405,
            "eval_matthews_correlation": 0.5817890663339899,
            "eval_runtime": 0.4145,
            "eval_samples_per_second": 2516.405,
            "eval_steps_per_second": 21.714,
            "epoch": 32.83582089552239,
            "step": 2200
        },
        {
            "loss": 0.0924,
            "grad_norm": 3.619605779647827,
            "learning_rate": 0.000142620232172471,
            "epoch": 35.82089552238806,
            "step": 2400
        },
        {
            "eval_loss": 0.7553583383560181,
            "eval_matthews_correlation": 0.5575782477551274,
            "eval_runtime": 0.515,
            "eval_samples_per_second": 2025.361,
            "eval_steps_per_second": 17.477,
            "epoch": 35.82089552238806,
            "step": 2400
        },
        {
            "loss": 0.0854,
            "grad_norm": 3.8749895095825195,
            "learning_rate": 0.0001359867330016584,
            "epoch": 38.80597014925373,
            "step": 2600
        },
        {
            "eval_loss": 0.7908996343612671,
            "eval_matthews_correlation": 0.565579540997454,
            "eval_runtime": 0.4744,
            "eval_samples_per_second": 2198.775,
            "eval_steps_per_second": 18.973,
            "epoch": 38.80597014925373,
            "step": 2600
        },
        {
            "train_runtime": 497.5018,
            "train_samples_per_second": 1718.788,
            "train_steps_per_second": 13.467,
            "total_flos": 2.1966646071525376e+16,
            "train_loss": 0.24440331972562349,
            "epoch": 38.80597014925373,
            "step": 2600
        },
        {
            "eval_loss": 0.6977540850639343,
            "eval_matthews_correlation": 0.5552845087956161,
            "eval_runtime": 0.4683,
            "eval_samples_per_second": 2227.07,
            "eval_steps_per_second": 19.217,
            "epoch": 38.80597014925373,
            "step": 2600
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "mrlora-lcoef",
        "rank": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.05,
        "type": 2,
        "model_family": "bert",
        "task": "cola",
        "seed": 123,
        "student_model_name": "./models/distilbert-base-uncased",
        "teacher_model_name": "./models/bert-base-uncased",
        "use_olora": false,
        "use_lcoef": true,
        "use_bias": false,
        "train_size": 8551
    },
    "train": {
        "train_time": 497.5018,
        "trainable_params_count": 0.296522,
        "memory_allocated": [
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496,
            462.186496
        ],
        "memory_reserved": [
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336,
            2764.046336
        ]
    },
    "variant": "lora"
}