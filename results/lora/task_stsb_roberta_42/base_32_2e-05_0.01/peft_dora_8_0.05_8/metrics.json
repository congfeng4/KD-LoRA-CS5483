{
    "eval_loss": 0.40144839882850647,
    "eval_pearson": 0.9083016852837282,
    "eval_spearman": 0.9053902694444329,
    "eval_runtime": 0.289,
    "eval_samples_per_second": 5189.836,
    "eval_steps_per_second": 41.519,
    "epoch": 66.66666666666667,
    "log_history": [
        {
            "loss": 4.5693,
            "grad_norm": 4.832097053527832,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 0.9158133268356323,
            "eval_pearson": 0.7850444290295994,
            "eval_spearman": 0.7963116527375924,
            "eval_runtime": 0.3345,
            "eval_samples_per_second": 4483.872,
            "eval_steps_per_second": 35.871,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 0.697,
            "grad_norm": 4.629645347595215,
            "learning_rate": 0.00017777777777777779,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 0.608471155166626,
            "eval_pearson": 0.8890190948027004,
            "eval_spearman": 0.8887001933816748,
            "eval_runtime": 0.4438,
            "eval_samples_per_second": 3379.941,
            "eval_steps_per_second": 27.04,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 0.4702,
            "grad_norm": 6.9144392013549805,
            "learning_rate": 0.0001925925925925926,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 0.5367645025253296,
            "eval_pearson": 0.899520952599159,
            "eval_spearman": 0.89810128092571,
            "eval_runtime": 0.4678,
            "eval_samples_per_second": 3206.622,
            "eval_steps_per_second": 25.653,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.4151,
            "grad_norm": 13.564596176147461,
            "learning_rate": 0.00018271604938271605,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 0.45349597930908203,
            "eval_pearson": 0.8995355562663209,
            "eval_spearman": 0.8989922123751785,
            "eval_runtime": 0.3866,
            "eval_samples_per_second": 3879.723,
            "eval_steps_per_second": 31.038,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.3679,
            "grad_norm": 5.047686576843262,
            "learning_rate": 0.0001728395061728395,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 0.4979652464389801,
            "eval_pearson": 0.8994110141861489,
            "eval_spearman": 0.8986792050362461,
            "eval_runtime": 0.4267,
            "eval_samples_per_second": 3515.689,
            "eval_steps_per_second": 28.126,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.3414,
            "grad_norm": 4.81866979598999,
            "learning_rate": 0.00016296296296296295,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 0.5043169856071472,
            "eval_pearson": 0.9006931527335893,
            "eval_spearman": 0.8984527109187228,
            "eval_runtime": 0.3876,
            "eval_samples_per_second": 3870.029,
            "eval_steps_per_second": 30.96,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.3169,
            "grad_norm": 3.8556082248687744,
            "learning_rate": 0.0001530864197530864,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 0.4465160667896271,
            "eval_pearson": 0.9058271769077181,
            "eval_spearman": 0.9031763560355789,
            "eval_runtime": 0.3627,
            "eval_samples_per_second": 4135.103,
            "eval_steps_per_second": 33.081,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.2899,
            "grad_norm": 2.6499946117401123,
            "learning_rate": 0.00014320987654320989,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 0.4261232614517212,
            "eval_pearson": 0.9037071289809123,
            "eval_spearman": 0.9021169610119975,
            "eval_runtime": 0.3589,
            "eval_samples_per_second": 4178.945,
            "eval_steps_per_second": 33.432,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.2741,
            "grad_norm": 1.919668197631836,
            "learning_rate": 0.00013333333333333334,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 0.4300389885902405,
            "eval_pearson": 0.9047622942958221,
            "eval_spearman": 0.9028381303812048,
            "eval_runtime": 0.574,
            "eval_samples_per_second": 2613.454,
            "eval_steps_per_second": 20.908,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.2573,
            "grad_norm": 3.262564182281494,
            "learning_rate": 0.0001234567901234568,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 0.40144839882850647,
            "eval_pearson": 0.9083016852837282,
            "eval_spearman": 0.9053902694444329,
            "eval_runtime": 0.5524,
            "eval_samples_per_second": 2715.45,
            "eval_steps_per_second": 21.724,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.245,
            "grad_norm": 7.773654937744141,
            "learning_rate": 0.00011358024691358025,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 0.3995804786682129,
            "eval_pearson": 0.907050988750088,
            "eval_spearman": 0.904782323877759,
            "eval_runtime": 0.5076,
            "eval_samples_per_second": 2955.091,
            "eval_steps_per_second": 23.641,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.2386,
            "grad_norm": 3.086198091506958,
            "learning_rate": 0.0001037037037037037,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 0.41797584295272827,
            "eval_pearson": 0.9067104023873102,
            "eval_spearman": 0.9046428882177374,
            "eval_runtime": 0.3456,
            "eval_samples_per_second": 4339.869,
            "eval_steps_per_second": 34.719,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 0.2261,
            "grad_norm": 2.453671932220459,
            "learning_rate": 9.382716049382717e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 0.41403719782829285,
            "eval_pearson": 0.9081199065245987,
            "eval_spearman": 0.9052531814008236,
            "eval_runtime": 0.3252,
            "eval_samples_per_second": 4612.105,
            "eval_steps_per_second": 36.897,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "loss": 0.2217,
            "grad_norm": 2.070021629333496,
            "learning_rate": 8.395061728395062e-05,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 0.4053916931152344,
            "eval_pearson": 0.9072244144354198,
            "eval_spearman": 0.9041395887705022,
            "eval_runtime": 0.4299,
            "eval_samples_per_second": 3489.246,
            "eval_steps_per_second": 27.914,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "loss": 0.2171,
            "grad_norm": 3.4716529846191406,
            "learning_rate": 7.407407407407407e-05,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 0.4396354854106903,
            "eval_pearson": 0.9052916146039712,
            "eval_spearman": 0.9035154161142959,
            "eval_runtime": 0.37,
            "eval_samples_per_second": 4053.515,
            "eval_steps_per_second": 32.428,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "train_runtime": 559.7159,
            "train_samples_per_second": 1027.128,
            "train_steps_per_second": 8.04,
            "total_flos": 2.5525242298368e+16,
            "train_loss": 0.6098306897481283,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 0.40144839882850647,
            "eval_pearson": 0.9083016852837282,
            "eval_spearman": 0.9053902694444329,
            "eval_runtime": 0.289,
            "eval_samples_per_second": 5189.836,
            "eval_steps_per_second": 41.519,
            "epoch": 66.66666666666667,
            "step": 3000
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "dora",
        "rank": 8,
        "lora_alpha": 8,
        "lora_dropout": 0.05,
        "type": 2,
        "model_family": "roberta",
        "task": "stsb",
        "seed": 42,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "train_size": 5749
    },
    "train": {
        "train_time": 559.7159,
        "trainable_params_count": 0.904705,
        "memory_allocated": [
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304,
            533.026304
        ],
        "memory_reserved": [
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888,
            2399.141888
        ]
    },
    "variant": "lora"
}