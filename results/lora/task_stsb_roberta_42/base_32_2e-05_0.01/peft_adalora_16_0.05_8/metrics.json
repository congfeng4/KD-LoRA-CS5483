{
    "eval_loss": 0.43868035078048706,
    "eval_pearson": 0.9025647651179578,
    "eval_spearman": 0.902168889967508,
    "eval_runtime": 0.4841,
    "eval_samples_per_second": 3098.829,
    "eval_steps_per_second": 24.791,
    "epoch": 80.0,
    "log_history": [
        {
            "loss": 6.0557,
            "grad_norm": 5.51250696182251,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 2.6369130611419678,
            "eval_pearson": 0.06035982539095662,
            "eval_spearman": 0.060518154369691886,
            "eval_runtime": 0.5915,
            "eval_samples_per_second": 2535.766,
            "eval_steps_per_second": 20.286,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 2.1,
            "grad_norm": 4.607749938964844,
            "learning_rate": 0.00017777777777777779,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.0859830379486084,
            "eval_pearson": 0.5134043819657076,
            "eval_spearman": 0.5035870805550171,
            "eval_runtime": 0.5668,
            "eval_samples_per_second": 2646.354,
            "eval_steps_per_second": 21.171,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 1.1032,
            "grad_norm": 2.663682460784912,
            "learning_rate": 0.0001925925925925926,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 0.6114247441291809,
            "eval_pearson": 0.869216860484127,
            "eval_spearman": 0.8697270167310116,
            "eval_runtime": 0.644,
            "eval_samples_per_second": 2329.175,
            "eval_steps_per_second": 18.633,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.6864,
            "grad_norm": 6.34141206741333,
            "learning_rate": 0.00018271604938271605,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 0.523115873336792,
            "eval_pearson": 0.8841612318291567,
            "eval_spearman": 0.884119250328783,
            "eval_runtime": 0.597,
            "eval_samples_per_second": 2512.653,
            "eval_steps_per_second": 20.101,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.5904,
            "grad_norm": 5.704893589019775,
            "learning_rate": 0.0001728395061728395,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 0.49361488223075867,
            "eval_pearson": 0.888604672861744,
            "eval_spearman": 0.8893044086121713,
            "eval_runtime": 0.3653,
            "eval_samples_per_second": 4106.544,
            "eval_steps_per_second": 32.852,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.5207,
            "grad_norm": 1.3586297035217285,
            "learning_rate": 0.00016296296296296295,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 0.5035834312438965,
            "eval_pearson": 0.8923844162434375,
            "eval_spearman": 0.8922210375626267,
            "eval_runtime": 0.543,
            "eval_samples_per_second": 2762.381,
            "eval_steps_per_second": 22.099,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.4951,
            "grad_norm": 3.4269118309020996,
            "learning_rate": 0.0001530864197530864,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 0.47077247500419617,
            "eval_pearson": 0.8936087750437545,
            "eval_spearman": 0.8950357524783616,
            "eval_runtime": 0.3692,
            "eval_samples_per_second": 4063.306,
            "eval_steps_per_second": 32.506,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.4606,
            "grad_norm": 1.5788664817810059,
            "learning_rate": 0.00014320987654320989,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 0.4825305938720703,
            "eval_pearson": 0.8946537324997281,
            "eval_spearman": 0.8960330248474236,
            "eval_runtime": 0.4636,
            "eval_samples_per_second": 3235.302,
            "eval_steps_per_second": 25.882,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.4432,
            "grad_norm": 3.9837992191314697,
            "learning_rate": 0.00013333333333333334,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 0.4645511209964752,
            "eval_pearson": 0.8970854370915959,
            "eval_spearman": 0.8981499208217874,
            "eval_runtime": 0.4663,
            "eval_samples_per_second": 3216.994,
            "eval_steps_per_second": 25.736,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.4265,
            "grad_norm": 2.4457037448883057,
            "learning_rate": 0.0001234567901234568,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 0.47013553977012634,
            "eval_pearson": 0.8992190683840323,
            "eval_spearman": 0.899575149946305,
            "eval_runtime": 0.4856,
            "eval_samples_per_second": 3088.818,
            "eval_steps_per_second": 24.711,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.4146,
            "grad_norm": 2.553544759750366,
            "learning_rate": 0.00011358024691358025,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 0.4509458541870117,
            "eval_pearson": 0.8991182492244703,
            "eval_spearman": 0.8995229136182934,
            "eval_runtime": 0.5257,
            "eval_samples_per_second": 2853.343,
            "eval_steps_per_second": 22.827,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.3989,
            "grad_norm": 6.196960926055908,
            "learning_rate": 0.0001037037037037037,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 0.44268298149108887,
            "eval_pearson": 0.8994650699594327,
            "eval_spearman": 0.8998229743229577,
            "eval_runtime": 0.7079,
            "eval_samples_per_second": 2119.077,
            "eval_steps_per_second": 16.953,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 0.3943,
            "grad_norm": 3.773392915725708,
            "learning_rate": 9.382716049382717e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 0.4497165381908417,
            "eval_pearson": 0.9019820774989498,
            "eval_spearman": 0.9015846153720595,
            "eval_runtime": 0.5859,
            "eval_samples_per_second": 2560.23,
            "eval_steps_per_second": 20.482,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "loss": 0.3872,
            "grad_norm": 2.4324698448181152,
            "learning_rate": 8.395061728395062e-05,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 0.4417263865470886,
            "eval_pearson": 0.9014751117842048,
            "eval_spearman": 0.9014689968402424,
            "eval_runtime": 0.4463,
            "eval_samples_per_second": 3360.596,
            "eval_steps_per_second": 26.885,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "loss": 0.3814,
            "grad_norm": 1.5724763870239258,
            "learning_rate": 7.407407407407407e-05,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 0.4497886598110199,
            "eval_pearson": 0.9016822790180891,
            "eval_spearman": 0.9018051292879753,
            "eval_runtime": 0.5128,
            "eval_samples_per_second": 2924.87,
            "eval_steps_per_second": 23.399,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "loss": 0.3766,
            "grad_norm": 6.406510353088379,
            "learning_rate": 6.419753086419753e-05,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "eval_loss": 0.4705335199832916,
            "eval_pearson": 0.9022399886563682,
            "eval_spearman": 0.901593997708985,
            "eval_runtime": 0.5013,
            "eval_samples_per_second": 2992.089,
            "eval_steps_per_second": 23.937,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "loss": 0.3738,
            "grad_norm": 1.2367833852767944,
            "learning_rate": 5.4320987654320986e-05,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "eval_loss": 0.4575595557689667,
            "eval_pearson": 0.9021950954813881,
            "eval_spearman": 0.9020269592990525,
            "eval_runtime": 0.5521,
            "eval_samples_per_second": 2716.729,
            "eval_steps_per_second": 21.734,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "loss": 0.3707,
            "grad_norm": 2.695408582687378,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "eval_loss": 0.43868035078048706,
            "eval_pearson": 0.9025647651179578,
            "eval_spearman": 0.902168889967508,
            "eval_runtime": 0.4441,
            "eval_samples_per_second": 3377.589,
            "eval_steps_per_second": 27.021,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "train_runtime": 700.1459,
            "train_samples_per_second": 821.115,
            "train_steps_per_second": 6.427,
            "total_flos": 3.072828043689984e+16,
            "train_loss": 0.8877308146158854,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "eval_loss": 0.43868035078048706,
            "eval_pearson": 0.9025647651179578,
            "eval_spearman": 0.902168889967508,
            "eval_runtime": 0.4841,
            "eval_samples_per_second": 3098.829,
            "eval_steps_per_second": 24.791,
            "epoch": 80.0,
            "step": 3600
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "adalora",
        "rank": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.05,
        "type": 2,
        "model_family": "roberta",
        "task": "stsb",
        "seed": 42,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "train_size": 5749
    },
    "train": {
        "train_time": 700.1459,
        "trainable_params_count": 1.181569,
        "memory_allocated": [
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936
        ],
        "memory_reserved": [
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048
        ]
    },
    "variant": "lora"
}