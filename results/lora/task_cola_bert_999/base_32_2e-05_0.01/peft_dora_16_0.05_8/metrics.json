{
    "eval_loss": 0.4466131925582886,
    "eval_matthews_correlation": 0.577796630208603,
    "eval_runtime": 0.5321,
    "eval_samples_per_second": 1960.196,
    "eval_steps_per_second": 16.914,
    "epoch": 29.850746268656717,
    "log_history": [
        {
            "loss": 0.6445,
            "grad_norm": 0.8634339570999146,
            "learning_rate": 5.970149253731343e-05,
            "epoch": 2.9850746268656714,
            "step": 200
        },
        {
            "eval_loss": 0.5890605449676514,
            "eval_matthews_correlation": -0.007887379670285008,
            "eval_runtime": 0.462,
            "eval_samples_per_second": 2257.572,
            "eval_steps_per_second": 19.48,
            "epoch": 2.9850746268656714,
            "step": 200
        },
        {
            "loss": 0.4931,
            "grad_norm": 0.9686378836631775,
            "learning_rate": 0.00011940298507462686,
            "epoch": 5.970149253731344,
            "step": 400
        },
        {
            "eval_loss": 0.4695502519607544,
            "eval_matthews_correlation": 0.5045859154729114,
            "eval_runtime": 0.511,
            "eval_samples_per_second": 2041.108,
            "eval_steps_per_second": 17.613,
            "epoch": 5.970149253731344,
            "step": 400
        },
        {
            "loss": 0.4156,
            "grad_norm": 1.4415487051010132,
            "learning_rate": 0.0001791044776119403,
            "epoch": 8.955223880597014,
            "step": 600
        },
        {
            "eval_loss": 0.4256528913974762,
            "eval_matthews_correlation": 0.5538778174700708,
            "eval_runtime": 0.6395,
            "eval_samples_per_second": 1630.95,
            "eval_steps_per_second": 14.073,
            "epoch": 8.955223880597014,
            "step": 600
        },
        {
            "loss": 0.3649,
            "grad_norm": 1.4370688199996948,
            "learning_rate": 0.00019568822553897182,
            "epoch": 11.940298507462687,
            "step": 800
        },
        {
            "eval_loss": 0.4150443971157074,
            "eval_matthews_correlation": 0.575435670477595,
            "eval_runtime": 0.524,
            "eval_samples_per_second": 1990.362,
            "eval_steps_per_second": 17.175,
            "epoch": 11.940298507462687,
            "step": 800
        },
        {
            "loss": 0.3168,
            "grad_norm": 1.2596818208694458,
            "learning_rate": 0.00018905472636815922,
            "epoch": 14.925373134328359,
            "step": 1000
        },
        {
            "eval_loss": 0.4466131925582886,
            "eval_matthews_correlation": 0.577796630208603,
            "eval_runtime": 0.5692,
            "eval_samples_per_second": 1832.406,
            "eval_steps_per_second": 15.812,
            "epoch": 14.925373134328359,
            "step": 1000
        },
        {
            "loss": 0.2819,
            "grad_norm": 1.8435968160629272,
            "learning_rate": 0.0001824212271973466,
            "epoch": 17.91044776119403,
            "step": 1200
        },
        {
            "eval_loss": 0.47820642590522766,
            "eval_matthews_correlation": 0.5624066288493853,
            "eval_runtime": 0.5569,
            "eval_samples_per_second": 1872.714,
            "eval_steps_per_second": 16.16,
            "epoch": 17.91044776119403,
            "step": 1200
        },
        {
            "loss": 0.2478,
            "grad_norm": 1.7145607471466064,
            "learning_rate": 0.000175787728026534,
            "epoch": 20.895522388059703,
            "step": 1400
        },
        {
            "eval_loss": 0.5469946265220642,
            "eval_matthews_correlation": 0.5416469931221344,
            "eval_runtime": 0.5837,
            "eval_samples_per_second": 1786.84,
            "eval_steps_per_second": 15.419,
            "epoch": 20.895522388059703,
            "step": 1400
        },
        {
            "loss": 0.2322,
            "grad_norm": 1.310686469078064,
            "learning_rate": 0.0001691542288557214,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "eval_loss": 0.5171849727630615,
            "eval_matthews_correlation": 0.5390322970786539,
            "eval_runtime": 0.4272,
            "eval_samples_per_second": 2441.715,
            "eval_steps_per_second": 21.069,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "loss": 0.2032,
            "grad_norm": 1.6265017986297607,
            "learning_rate": 0.00016252072968490878,
            "epoch": 26.865671641791046,
            "step": 1800
        },
        {
            "eval_loss": 0.5207348465919495,
            "eval_matthews_correlation": 0.5521069582827846,
            "eval_runtime": 0.5135,
            "eval_samples_per_second": 2031.263,
            "eval_steps_per_second": 17.528,
            "epoch": 26.865671641791046,
            "step": 1800
        },
        {
            "loss": 0.1851,
            "grad_norm": 2.395038604736328,
            "learning_rate": 0.00015588723051409618,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "eval_loss": 0.5583398938179016,
            "eval_matthews_correlation": 0.547116568580723,
            "eval_runtime": 0.5764,
            "eval_samples_per_second": 1809.366,
            "eval_steps_per_second": 15.613,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "train_runtime": 306.6869,
            "train_samples_per_second": 2788.186,
            "train_steps_per_second": 21.846,
            "total_flos": 1.690101574795264e+16,
            "train_loss": 0.3385161075592041,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "eval_loss": 0.4466131925582886,
            "eval_matthews_correlation": 0.577796630208603,
            "eval_runtime": 0.5321,
            "eval_samples_per_second": 1960.196,
            "eval_steps_per_second": 16.914,
            "epoch": 29.850746268656717,
            "step": 2000
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "dora",
        "rank": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.05,
        "type": 2,
        "model_family": "bert",
        "task": "cola",
        "seed": 999,
        "student_model_name": "./models/distilbert-base-uncased",
        "teacher_model_name": "./models/bert-base-uncased",
        "train_size": 8551
    },
    "train": {
        "train_time": 306.6869,
        "trainable_params_count": 0.314882,
        "memory_allocated": [
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744,
            463.071744
        ],
        "memory_reserved": [
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568,
            2325.741568
        ]
    },
    "variant": "lora"
}