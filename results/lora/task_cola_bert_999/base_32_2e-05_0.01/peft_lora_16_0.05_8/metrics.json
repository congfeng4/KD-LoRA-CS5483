{
    "eval_loss": 0.4517173171043396,
    "eval_matthews_correlation": 0.559827888599451,
    "eval_runtime": 0.3052,
    "eval_samples_per_second": 3417.588,
    "eval_steps_per_second": 29.49,
    "epoch": 29.850746268656717,
    "log_history": [
        {
            "loss": 0.6448,
            "grad_norm": 0.8645203709602356,
            "learning_rate": 5.970149253731343e-05,
            "epoch": 2.9850746268656714,
            "step": 200
        },
        {
            "eval_loss": 0.5898883938789368,
            "eval_matthews_correlation": -0.007887379670285008,
            "eval_runtime": 0.3712,
            "eval_samples_per_second": 2809.966,
            "eval_steps_per_second": 24.247,
            "epoch": 2.9850746268656714,
            "step": 200
        },
        {
            "loss": 0.4939,
            "grad_norm": 1.0011178255081177,
            "learning_rate": 0.00011940298507462686,
            "epoch": 5.970149253731344,
            "step": 400
        },
        {
            "eval_loss": 0.46947574615478516,
            "eval_matthews_correlation": 0.5072676775530786,
            "eval_runtime": 0.3344,
            "eval_samples_per_second": 3119.004,
            "eval_steps_per_second": 26.914,
            "epoch": 5.970149253731344,
            "step": 400
        },
        {
            "loss": 0.4167,
            "grad_norm": 1.564315915107727,
            "learning_rate": 0.0001791044776119403,
            "epoch": 8.955223880597014,
            "step": 600
        },
        {
            "eval_loss": 0.42934802174568176,
            "eval_matthews_correlation": 0.5434531271960991,
            "eval_runtime": 0.266,
            "eval_samples_per_second": 3920.595,
            "eval_steps_per_second": 33.831,
            "epoch": 8.955223880597014,
            "step": 600
        },
        {
            "loss": 0.3679,
            "grad_norm": 1.338035225868225,
            "learning_rate": 0.00019568822553897182,
            "epoch": 11.940298507462687,
            "step": 800
        },
        {
            "eval_loss": 0.42334648966789246,
            "eval_matthews_correlation": 0.5524246615971198,
            "eval_runtime": 0.3753,
            "eval_samples_per_second": 2779.206,
            "eval_steps_per_second": 23.982,
            "epoch": 11.940298507462687,
            "step": 800
        },
        {
            "loss": 0.3205,
            "grad_norm": 1.5037342309951782,
            "learning_rate": 0.00018905472636815922,
            "epoch": 14.925373134328359,
            "step": 1000
        },
        {
            "eval_loss": 0.4517173171043396,
            "eval_matthews_correlation": 0.559827888599451,
            "eval_runtime": 0.3024,
            "eval_samples_per_second": 3448.711,
            "eval_steps_per_second": 29.759,
            "epoch": 14.925373134328359,
            "step": 1000
        },
        {
            "loss": 0.2851,
            "grad_norm": 1.714590072631836,
            "learning_rate": 0.0001824212271973466,
            "epoch": 17.91044776119403,
            "step": 1200
        },
        {
            "eval_loss": 0.484754353761673,
            "eval_matthews_correlation": 0.5416469931221344,
            "eval_runtime": 0.2989,
            "eval_samples_per_second": 3489.17,
            "eval_steps_per_second": 30.108,
            "epoch": 17.91044776119403,
            "step": 1200
        },
        {
            "loss": 0.2521,
            "grad_norm": 2.122579574584961,
            "learning_rate": 0.000175787728026534,
            "epoch": 20.895522388059703,
            "step": 1400
        },
        {
            "eval_loss": 0.512208878993988,
            "eval_matthews_correlation": 0.5468753188432375,
            "eval_runtime": 0.2832,
            "eval_samples_per_second": 3683.188,
            "eval_steps_per_second": 31.782,
            "epoch": 20.895522388059703,
            "step": 1400
        },
        {
            "loss": 0.2337,
            "grad_norm": 1.2918570041656494,
            "learning_rate": 0.0001691542288557214,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "eval_loss": 0.5298709273338318,
            "eval_matthews_correlation": 0.5390322970786539,
            "eval_runtime": 0.4005,
            "eval_samples_per_second": 2604.564,
            "eval_steps_per_second": 22.475,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "loss": 0.2073,
            "grad_norm": 1.6100143194198608,
            "learning_rate": 0.00016252072968490878,
            "epoch": 26.865671641791046,
            "step": 1800
        },
        {
            "eval_loss": 0.5152347087860107,
            "eval_matthews_correlation": 0.5416905121171213,
            "eval_runtime": 0.2835,
            "eval_samples_per_second": 3678.936,
            "eval_steps_per_second": 31.745,
            "epoch": 26.865671641791046,
            "step": 1800
        },
        {
            "loss": 0.1875,
            "grad_norm": 2.035465717315674,
            "learning_rate": 0.00015588723051409618,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "eval_loss": 0.5658722519874573,
            "eval_matthews_correlation": 0.5395109128667314,
            "eval_runtime": 0.3313,
            "eval_samples_per_second": 3148.258,
            "eval_steps_per_second": 27.166,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "train_runtime": 204.7052,
            "train_samples_per_second": 4177.226,
            "train_steps_per_second": 32.73,
            "total_flos": 1.689739186929664e+16,
            "train_loss": 0.340947208404541,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "eval_loss": 0.4517173171043396,
            "eval_matthews_correlation": 0.559827888599451,
            "eval_runtime": 0.3052,
            "eval_samples_per_second": 3417.588,
            "eval_steps_per_second": 29.49,
            "epoch": 29.850746268656717,
            "step": 2000
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "lora",
        "rank": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.05,
        "type": 2,
        "model_family": "bert",
        "task": "cola",
        "seed": 999,
        "student_model_name": "./models/distilbert-base-uncased",
        "teacher_model_name": "./models/bert-base-uncased",
        "train_size": 8551
    },
    "train": {
        "train_time": 204.7052,
        "trainable_params_count": 0.29645,
        "memory_allocated": [
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912,
            462.78912
        ],
        "memory_reserved": [
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856,
            1998.585856
        ]
    },
    "variant": "lora"
}