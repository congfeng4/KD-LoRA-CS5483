{
    "eval_loss": 0.45487961173057556,
    "eval_pearson": 0.9030384457163614,
    "eval_spearman": 0.9025025837483502,
    "eval_runtime": 0.3984,
    "eval_samples_per_second": 3764.951,
    "eval_steps_per_second": 30.12,
    "epoch": 75.55555555555556,
    "log_history": [
        {
            "loss": 6.0326,
            "grad_norm": 3.642883777618408,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 2.6579983234405518,
            "eval_pearson": 0.009303920270865965,
            "eval_spearman": 0.012646701210505153,
            "eval_runtime": 0.6235,
            "eval_samples_per_second": 2405.912,
            "eval_steps_per_second": 19.247,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 2.1157,
            "grad_norm": 1.4975529909133911,
            "learning_rate": 0.00017777777777777779,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.0796568393707275,
            "eval_pearson": 0.40834903494387315,
            "eval_spearman": 0.37978582737900424,
            "eval_runtime": 0.4122,
            "eval_samples_per_second": 3639.166,
            "eval_steps_per_second": 29.113,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 1.1558,
            "grad_norm": 7.0490031242370605,
            "learning_rate": 0.0001925925925925926,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 0.6262832880020142,
            "eval_pearson": 0.8606057584234186,
            "eval_spearman": 0.8622551655308421,
            "eval_runtime": 0.4363,
            "eval_samples_per_second": 3438.228,
            "eval_steps_per_second": 27.506,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.7015,
            "grad_norm": 1.2470239400863647,
            "learning_rate": 0.00018271604938271605,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 0.551311194896698,
            "eval_pearson": 0.8815764119296697,
            "eval_spearman": 0.8828026151231216,
            "eval_runtime": 0.4158,
            "eval_samples_per_second": 3607.497,
            "eval_steps_per_second": 28.86,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.5808,
            "grad_norm": 1.662541389465332,
            "learning_rate": 0.0001728395061728395,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 0.5201277136802673,
            "eval_pearson": 0.8879010373703418,
            "eval_spearman": 0.8885156568115838,
            "eval_runtime": 0.4545,
            "eval_samples_per_second": 3299.986,
            "eval_steps_per_second": 26.4,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.5231,
            "grad_norm": 1.5652000904083252,
            "learning_rate": 0.00016296296296296295,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 0.5018369555473328,
            "eval_pearson": 0.889959862465755,
            "eval_spearman": 0.8923263936270606,
            "eval_runtime": 0.4026,
            "eval_samples_per_second": 3725.449,
            "eval_steps_per_second": 29.804,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.4877,
            "grad_norm": 3.445274829864502,
            "learning_rate": 0.0001530864197530864,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 0.5100858807563782,
            "eval_pearson": 0.8945435442427752,
            "eval_spearman": 0.8957596210368509,
            "eval_runtime": 0.5176,
            "eval_samples_per_second": 2897.836,
            "eval_steps_per_second": 23.183,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.4643,
            "grad_norm": 1.1034107208251953,
            "learning_rate": 0.00014320987654320989,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 0.4798758625984192,
            "eval_pearson": 0.8951768193242282,
            "eval_spearman": 0.8953269376956672,
            "eval_runtime": 0.3802,
            "eval_samples_per_second": 3945.281,
            "eval_steps_per_second": 31.562,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.447,
            "grad_norm": 1.5234768390655518,
            "learning_rate": 0.00013333333333333334,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 0.4754619598388672,
            "eval_pearson": 0.8964707900469053,
            "eval_spearman": 0.8980590877415913,
            "eval_runtime": 0.4075,
            "eval_samples_per_second": 3681.345,
            "eval_steps_per_second": 29.451,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.4273,
            "grad_norm": 5.896255016326904,
            "learning_rate": 0.0001234567901234568,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 0.46118253469467163,
            "eval_pearson": 0.899092399725203,
            "eval_spearman": 0.8985520906798498,
            "eval_runtime": 0.4922,
            "eval_samples_per_second": 3047.786,
            "eval_steps_per_second": 24.382,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.4167,
            "grad_norm": 3.6849300861358643,
            "learning_rate": 0.00011358024691358025,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 0.4800737798213959,
            "eval_pearson": 0.8998538169939949,
            "eval_spearman": 0.8995734371738031,
            "eval_runtime": 0.4677,
            "eval_samples_per_second": 3207.014,
            "eval_steps_per_second": 25.656,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.4066,
            "grad_norm": 1.2878941297531128,
            "learning_rate": 0.0001037037037037037,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 0.4420432448387146,
            "eval_pearson": 0.9007981633617238,
            "eval_spearman": 0.9017346908027528,
            "eval_runtime": 0.4101,
            "eval_samples_per_second": 3657.33,
            "eval_steps_per_second": 29.259,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 0.3979,
            "grad_norm": 3.183574914932251,
            "learning_rate": 9.382716049382717e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 0.45485830307006836,
            "eval_pearson": 0.9013864371907487,
            "eval_spearman": 0.9007055791266632,
            "eval_runtime": 0.4511,
            "eval_samples_per_second": 3325.079,
            "eval_steps_per_second": 26.601,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "loss": 0.3932,
            "grad_norm": 3.4963111877441406,
            "learning_rate": 8.395061728395062e-05,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 0.48318254947662354,
            "eval_pearson": 0.9000598773471691,
            "eval_spearman": 0.9004556740621683,
            "eval_runtime": 0.594,
            "eval_samples_per_second": 2525.284,
            "eval_steps_per_second": 20.202,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "loss": 0.3855,
            "grad_norm": 1.5057729482650757,
            "learning_rate": 7.407407407407407e-05,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 0.44285082817077637,
            "eval_pearson": 0.9016959160091044,
            "eval_spearman": 0.901253227897592,
            "eval_runtime": 0.6259,
            "eval_samples_per_second": 2396.736,
            "eval_steps_per_second": 19.174,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "loss": 0.376,
            "grad_norm": 1.875504732131958,
            "learning_rate": 6.419753086419753e-05,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "eval_loss": 0.44641515612602234,
            "eval_pearson": 0.9020733347639689,
            "eval_spearman": 0.9023935209523996,
            "eval_runtime": 0.4082,
            "eval_samples_per_second": 3674.564,
            "eval_steps_per_second": 29.397,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "loss": 0.3696,
            "grad_norm": 1.6566636562347412,
            "learning_rate": 5.4320987654320986e-05,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "eval_loss": 0.45487961173057556,
            "eval_pearson": 0.9030384457163614,
            "eval_spearman": 0.9025025837483502,
            "eval_runtime": 0.4148,
            "eval_samples_per_second": 3615.84,
            "eval_steps_per_second": 28.927,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "train_runtime": 502.8933,
            "train_samples_per_second": 1143.185,
            "train_steps_per_second": 8.948,
            "total_flos": 2.902115374596096e+16,
            "train_loss": 0.9224330251357135,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "eval_loss": 0.45487961173057556,
            "eval_pearson": 0.9030384457163614,
            "eval_spearman": 0.9025025837483502,
            "eval_runtime": 0.3984,
            "eval_samples_per_second": 3764.951,
            "eval_steps_per_second": 30.12,
            "epoch": 75.55555555555556,
            "step": 3400
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "adalora",
        "rank": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.05,
        "type": 2,
        "model_family": "roberta",
        "task": "stsb",
        "seed": 999,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "train_size": 5749
    },
    "train": {
        "train_time": 502.8933,
        "trainable_params_count": 1.181569,
        "memory_allocated": [
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936
        ],
        "memory_reserved": [
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048
        ]
    },
    "variant": "lora"
}