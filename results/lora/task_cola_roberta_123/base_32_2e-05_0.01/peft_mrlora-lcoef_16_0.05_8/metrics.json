{
    "eval_loss": 0.8772130012512207,
    "eval_matthews_correlation": 0.5907413763612096,
    "eval_runtime": 0.4679,
    "eval_samples_per_second": 2229.112,
    "eval_steps_per_second": 19.235,
    "epoch": 50.74626865671642,
    "log_history": [
        {
            "loss": 0.5933,
            "grad_norm": 6.007297515869141,
            "learning_rate": 5.970149253731343e-05,
            "epoch": 2.9850746268656714,
            "step": 200
        },
        {
            "eval_loss": 0.5149218440055847,
            "eval_matthews_correlation": 0.45272024342354794,
            "eval_runtime": 0.4702,
            "eval_samples_per_second": 2218.116,
            "eval_steps_per_second": 19.14,
            "epoch": 2.9850746268656714,
            "step": 200
        },
        {
            "loss": 0.432,
            "grad_norm": 7.439405918121338,
            "learning_rate": 0.00011940298507462686,
            "epoch": 5.970149253731344,
            "step": 400
        },
        {
            "eval_loss": 0.5331836938858032,
            "eval_matthews_correlation": 0.45831111658186324,
            "eval_runtime": 0.6846,
            "eval_samples_per_second": 1523.426,
            "eval_steps_per_second": 13.146,
            "epoch": 5.970149253731344,
            "step": 400
        },
        {
            "loss": 0.3756,
            "grad_norm": 3.6411044597625732,
            "learning_rate": 0.0001791044776119403,
            "epoch": 8.955223880597014,
            "step": 600
        },
        {
            "eval_loss": 0.41501399874687195,
            "eval_matthews_correlation": 0.5465259733266157,
            "eval_runtime": 0.4114,
            "eval_samples_per_second": 2535.089,
            "eval_steps_per_second": 21.875,
            "epoch": 8.955223880597014,
            "step": 600
        },
        {
            "loss": 0.3347,
            "grad_norm": 4.7743988037109375,
            "learning_rate": 0.00019568822553897182,
            "epoch": 11.940298507462687,
            "step": 800
        },
        {
            "eval_loss": 0.4193018972873688,
            "eval_matthews_correlation": 0.5836872901947191,
            "eval_runtime": 0.3821,
            "eval_samples_per_second": 2729.941,
            "eval_steps_per_second": 23.557,
            "epoch": 11.940298507462687,
            "step": 800
        },
        {
            "loss": 0.2811,
            "grad_norm": 2.389145612716675,
            "learning_rate": 0.00018905472636815922,
            "epoch": 14.925373134328359,
            "step": 1000
        },
        {
            "eval_loss": 0.4649202823638916,
            "eval_matthews_correlation": 0.5945554194647816,
            "eval_runtime": 0.4815,
            "eval_samples_per_second": 2166.301,
            "eval_steps_per_second": 18.693,
            "epoch": 14.925373134328359,
            "step": 1000
        },
        {
            "loss": 0.2383,
            "grad_norm": 4.0662055015563965,
            "learning_rate": 0.0001824212271973466,
            "epoch": 17.91044776119403,
            "step": 1200
        },
        {
            "eval_loss": 0.5305805802345276,
            "eval_matthews_correlation": 0.5547002704668513,
            "eval_runtime": 0.4541,
            "eval_samples_per_second": 2296.684,
            "eval_steps_per_second": 19.818,
            "epoch": 17.91044776119403,
            "step": 1200
        },
        {
            "loss": 0.2057,
            "grad_norm": 3.3801474571228027,
            "learning_rate": 0.000175787728026534,
            "epoch": 20.895522388059703,
            "step": 1400
        },
        {
            "eval_loss": 0.5404297113418579,
            "eval_matthews_correlation": 0.5806284057433418,
            "eval_runtime": 0.5635,
            "eval_samples_per_second": 1850.846,
            "eval_steps_per_second": 15.971,
            "epoch": 20.895522388059703,
            "step": 1400
        },
        {
            "loss": 0.1752,
            "grad_norm": 3.080127239227295,
            "learning_rate": 0.0001691542288557214,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "eval_loss": 0.6356184482574463,
            "eval_matthews_correlation": 0.5547291984984687,
            "eval_runtime": 0.5018,
            "eval_samples_per_second": 2078.644,
            "eval_steps_per_second": 17.937,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "loss": 0.1598,
            "grad_norm": 5.257805347442627,
            "learning_rate": 0.00016252072968490878,
            "epoch": 26.865671641791046,
            "step": 1800
        },
        {
            "eval_loss": 0.5525506734848022,
            "eval_matthews_correlation": 0.6097804486545971,
            "eval_runtime": 0.5315,
            "eval_samples_per_second": 1962.354,
            "eval_steps_per_second": 16.933,
            "epoch": 26.865671641791046,
            "step": 1800
        },
        {
            "loss": 0.1378,
            "grad_norm": 4.2448906898498535,
            "learning_rate": 0.00015588723051409618,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "eval_loss": 0.6994370222091675,
            "eval_matthews_correlation": 0.6006378318734895,
            "eval_runtime": 0.4855,
            "eval_samples_per_second": 2148.248,
            "eval_steps_per_second": 18.537,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "loss": 0.1231,
            "grad_norm": 4.604537010192871,
            "learning_rate": 0.0001492537313432836,
            "epoch": 32.83582089552239,
            "step": 2200
        },
        {
            "eval_loss": 0.7355513572692871,
            "eval_matthews_correlation": 0.59561622728651,
            "eval_runtime": 0.4662,
            "eval_samples_per_second": 2237.03,
            "eval_steps_per_second": 19.303,
            "epoch": 32.83582089552239,
            "step": 2200
        },
        {
            "loss": 0.1081,
            "grad_norm": 5.477085113525391,
            "learning_rate": 0.000142620232172471,
            "epoch": 35.82089552238806,
            "step": 2400
        },
        {
            "eval_loss": 0.756197988986969,
            "eval_matthews_correlation": 0.6132520976952389,
            "eval_runtime": 0.4689,
            "eval_samples_per_second": 2224.28,
            "eval_steps_per_second": 19.193,
            "epoch": 35.82089552238806,
            "step": 2400
        },
        {
            "loss": 0.0985,
            "grad_norm": 5.476850509643555,
            "learning_rate": 0.0001359867330016584,
            "epoch": 38.80597014925373,
            "step": 2600
        },
        {
            "eval_loss": 0.7839967608451843,
            "eval_matthews_correlation": 0.5955860617587142,
            "eval_runtime": 0.4484,
            "eval_samples_per_second": 2326.107,
            "eval_steps_per_second": 20.072,
            "epoch": 38.80597014925373,
            "step": 2600
        },
        {
            "loss": 0.0905,
            "grad_norm": 3.374779462814331,
            "learning_rate": 0.0001293532338308458,
            "epoch": 41.791044776119406,
            "step": 2800
        },
        {
            "eval_loss": 0.8886455297470093,
            "eval_matthews_correlation": 0.5829964770702437,
            "eval_runtime": 0.4251,
            "eval_samples_per_second": 2453.71,
            "eval_steps_per_second": 21.173,
            "epoch": 41.791044776119406,
            "step": 2800
        },
        {
            "loss": 0.0833,
            "grad_norm": 10.760610580444336,
            "learning_rate": 0.00012271973466003317,
            "epoch": 44.776119402985074,
            "step": 3000
        },
        {
            "eval_loss": 0.9404750466346741,
            "eval_matthews_correlation": 0.5547962503782369,
            "eval_runtime": 0.4722,
            "eval_samples_per_second": 2208.682,
            "eval_steps_per_second": 19.059,
            "epoch": 44.776119402985074,
            "step": 3000
        },
        {
            "loss": 0.0792,
            "grad_norm": 6.004702568054199,
            "learning_rate": 0.00011608623548922057,
            "epoch": 47.76119402985075,
            "step": 3200
        },
        {
            "eval_loss": 0.860063910484314,
            "eval_matthews_correlation": 0.6056766490257713,
            "eval_runtime": 0.539,
            "eval_samples_per_second": 1934.952,
            "eval_steps_per_second": 16.697,
            "epoch": 47.76119402985075,
            "step": 3200
        },
        {
            "loss": 0.0678,
            "grad_norm": 4.623525619506836,
            "learning_rate": 0.00010945273631840796,
            "epoch": 50.74626865671642,
            "step": 3400
        },
        {
            "eval_loss": 0.9257405400276184,
            "eval_matthews_correlation": 0.5982632727547974,
            "eval_runtime": 0.4096,
            "eval_samples_per_second": 2546.55,
            "eval_steps_per_second": 21.974,
            "epoch": 50.74626865671642,
            "step": 3400
        },
        {
            "train_runtime": 636.4081,
            "train_samples_per_second": 1343.635,
            "train_steps_per_second": 10.528,
            "total_flos": 2.892301092467507e+16,
            "train_loss": 0.2108169039557962,
            "epoch": 50.74626865671642,
            "step": 3400
        },
        {
            "eval_loss": 0.8772130012512207,
            "eval_matthews_correlation": 0.5907413763612096,
            "eval_runtime": 0.4679,
            "eval_samples_per_second": 2229.112,
            "eval_steps_per_second": 19.235,
            "epoch": 50.74626865671642,
            "step": 3400
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "mrlora-lcoef",
        "rank": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.05,
        "type": 2,
        "model_family": "roberta",
        "task": "cola",
        "seed": 123,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "use_olora": false,
        "use_lcoef": true,
        "use_bias": false,
        "train_size": 8551
    },
    "train": {
        "train_time": 636.4081,
        "trainable_params_count": 0.887114,
        "memory_allocated": [
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856,
            532.41856
        ],
        "memory_reserved": [
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808,
            2839.543808
        ]
    },
    "variant": "lora"
}