{
    "eval_loss": 0.44768163561820984,
    "eval_pearson": 0.9037822131820547,
    "eval_spearman": 0.9057290451698763,
    "eval_runtime": 0.6569,
    "eval_samples_per_second": 2283.314,
    "eval_steps_per_second": 18.267,
    "epoch": 53.333333333333336,
    "log_history": [
        {
            "loss": 7.1159,
            "grad_norm": 4.857876300811768,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 1.6823009252548218,
            "eval_pearson": 0.7379693244010963,
            "eval_spearman": 0.7457661806089746,
            "eval_runtime": 0.7089,
            "eval_samples_per_second": 2116.05,
            "eval_steps_per_second": 16.928,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 0.8569,
            "grad_norm": 3.0118610858917236,
            "learning_rate": 0.00017777777777777779,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 0.6136423945426941,
            "eval_pearson": 0.8682843357562455,
            "eval_spearman": 0.8731082920857235,
            "eval_runtime": 0.6349,
            "eval_samples_per_second": 2362.756,
            "eval_steps_per_second": 18.902,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 0.4792,
            "grad_norm": 2.3748385906219482,
            "learning_rate": 0.0001925925925925926,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 0.5115709900856018,
            "eval_pearson": 0.8986878565518277,
            "eval_spearman": 0.8992232736845032,
            "eval_runtime": 0.63,
            "eval_samples_per_second": 2381.015,
            "eval_steps_per_second": 19.048,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.3587,
            "grad_norm": 1.1286360025405884,
            "learning_rate": 0.00018271604938271605,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 0.5110901594161987,
            "eval_pearson": 0.8976525390247608,
            "eval_spearman": 0.9011615310853249,
            "eval_runtime": 0.5883,
            "eval_samples_per_second": 2549.815,
            "eval_steps_per_second": 20.399,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.2992,
            "grad_norm": 1.0876716375350952,
            "learning_rate": 0.0001728395061728395,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 0.47143709659576416,
            "eval_pearson": 0.9019044909964304,
            "eval_spearman": 0.9026565659966936,
            "eval_runtime": 0.6287,
            "eval_samples_per_second": 2386.025,
            "eval_steps_per_second": 19.088,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.2644,
            "grad_norm": 2.3547987937927246,
            "learning_rate": 0.00016296296296296295,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 0.44808611273765564,
            "eval_pearson": 0.9024924303490525,
            "eval_spearman": 0.9041106946767286,
            "eval_runtime": 0.6548,
            "eval_samples_per_second": 2290.848,
            "eval_steps_per_second": 18.327,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.2395,
            "grad_norm": 3.835578680038452,
            "learning_rate": 0.0001530864197530864,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 0.44768163561820984,
            "eval_pearson": 0.9037822131820547,
            "eval_spearman": 0.9057290451698763,
            "eval_runtime": 0.6883,
            "eval_samples_per_second": 2179.284,
            "eval_steps_per_second": 17.434,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.2215,
            "grad_norm": 5.189035415649414,
            "learning_rate": 0.00014320987654320989,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 0.5170445442199707,
            "eval_pearson": 0.9032883677916881,
            "eval_spearman": 0.902666399095275,
            "eval_runtime": 0.6345,
            "eval_samples_per_second": 2363.979,
            "eval_steps_per_second": 18.912,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.203,
            "grad_norm": 1.3253257274627686,
            "learning_rate": 0.00013333333333333334,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 0.43888410925865173,
            "eval_pearson": 0.9037915554156168,
            "eval_spearman": 0.9043055641993945,
            "eval_runtime": 0.6319,
            "eval_samples_per_second": 2373.954,
            "eval_steps_per_second": 18.992,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.192,
            "grad_norm": 1.2684881687164307,
            "learning_rate": 0.0001234567901234568,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 0.4632546305656433,
            "eval_pearson": 0.9031496903880422,
            "eval_spearman": 0.9043812025256552,
            "eval_runtime": 0.7082,
            "eval_samples_per_second": 2117.947,
            "eval_steps_per_second": 16.944,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.1777,
            "grad_norm": 1.4171406030654907,
            "learning_rate": 0.00011358024691358025,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 0.44347140192985535,
            "eval_pearson": 0.9034421368881059,
            "eval_spearman": 0.9039810765296672,
            "eval_runtime": 0.7665,
            "eval_samples_per_second": 1957.034,
            "eval_steps_per_second": 15.656,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.1705,
            "grad_norm": 1.2446200847625732,
            "learning_rate": 0.0001037037037037037,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 0.442200630903244,
            "eval_pearson": 0.9017177968318667,
            "eval_spearman": 0.9050279082438033,
            "eval_runtime": 0.7203,
            "eval_samples_per_second": 2082.413,
            "eval_steps_per_second": 16.659,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "train_runtime": 454.0871,
            "train_samples_per_second": 1266.057,
            "train_steps_per_second": 9.91,
            "total_flos": 2.028121889754317e+16,
            "train_loss": 0.881533325513204,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 0.44768163561820984,
            "eval_pearson": 0.9037822131820547,
            "eval_spearman": 0.9057290451698763,
            "eval_runtime": 0.6569,
            "eval_samples_per_second": 2283.314,
            "eval_steps_per_second": 18.267,
            "epoch": 53.333333333333336,
            "step": 2400
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "dora",
        "rank": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.05,
        "type": 2,
        "model_family": "deberta",
        "task": "stsb",
        "seed": 999,
        "student_model_name": "./models/deberta-v3-small",
        "teacher_model_name": "./models/deberta-v3-base",
        "train_size": 5749
    },
    "train": {
        "train_time": 454.0871,
        "trainable_params_count": 0.314113,
        "memory_allocated": [
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288,
            762.68288
        ],
        "memory_reserved": [
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616,
            5154.799616
        ]
    },
    "variant": "lora"
}