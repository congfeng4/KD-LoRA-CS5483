{
    "eval_loss": 0.43339771032333374,
    "eval_pearson": 0.9032721825058986,
    "eval_spearman": 0.9033844507355313,
    "eval_runtime": 0.3289,
    "eval_samples_per_second": 4561.145,
    "eval_steps_per_second": 36.489,
    "epoch": 84.44444444444444,
    "log_history": [
        {
            "loss": 6.2337,
            "grad_norm": 1.5769261121749878,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 2.597768783569336,
            "eval_pearson": 0.025126095479256934,
            "eval_spearman": 0.018728910695841604,
            "eval_runtime": 0.5489,
            "eval_samples_per_second": 2732.968,
            "eval_steps_per_second": 21.864,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 2.106,
            "grad_norm": 2.4154140949249268,
            "learning_rate": 0.00017777777777777779,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.146682024002075,
            "eval_pearson": 0.4270368603784792,
            "eval_spearman": 0.3965102990991882,
            "eval_runtime": 0.4495,
            "eval_samples_per_second": 3336.894,
            "eval_steps_per_second": 26.695,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 1.1813,
            "grad_norm": 2.8106372356414795,
            "learning_rate": 0.0001925925925925926,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 0.7009459733963013,
            "eval_pearson": 0.8626039619461162,
            "eval_spearman": 0.8620858224114446,
            "eval_runtime": 0.3518,
            "eval_samples_per_second": 4263.662,
            "eval_steps_per_second": 34.109,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.696,
            "grad_norm": 3.282526731491089,
            "learning_rate": 0.00018271604938271605,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 0.5363029837608337,
            "eval_pearson": 0.8816363802266319,
            "eval_spearman": 0.8826424437496226,
            "eval_runtime": 0.3822,
            "eval_samples_per_second": 3924.213,
            "eval_steps_per_second": 31.394,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.5849,
            "grad_norm": 3.24057936668396,
            "learning_rate": 0.0001728395061728395,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 0.5043825507164001,
            "eval_pearson": 0.8874647152663728,
            "eval_spearman": 0.8889221511199245,
            "eval_runtime": 0.4051,
            "eval_samples_per_second": 3702.54,
            "eval_steps_per_second": 29.62,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.5235,
            "grad_norm": 10.183150291442871,
            "learning_rate": 0.00016296296296296295,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 0.5021840929985046,
            "eval_pearson": 0.8913447910305007,
            "eval_spearman": 0.8927198677550108,
            "eval_runtime": 0.429,
            "eval_samples_per_second": 3496.782,
            "eval_steps_per_second": 27.974,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.4886,
            "grad_norm": 2.8933470249176025,
            "learning_rate": 0.0001530864197530864,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 0.4852787256240845,
            "eval_pearson": 0.892651834244953,
            "eval_spearman": 0.8944534934390047,
            "eval_runtime": 0.3773,
            "eval_samples_per_second": 3975.49,
            "eval_steps_per_second": 31.804,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.4723,
            "grad_norm": 2.725449562072754,
            "learning_rate": 0.00014320987654320989,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 0.46365225315093994,
            "eval_pearson": 0.8958349068619986,
            "eval_spearman": 0.896874306537571,
            "eval_runtime": 0.4136,
            "eval_samples_per_second": 3626.712,
            "eval_steps_per_second": 29.014,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.4398,
            "grad_norm": 1.7945990562438965,
            "learning_rate": 0.00013333333333333334,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 0.4623643457889557,
            "eval_pearson": 0.8968222038086359,
            "eval_spearman": 0.8983503460358072,
            "eval_runtime": 0.448,
            "eval_samples_per_second": 3348.183,
            "eval_steps_per_second": 26.785,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.4282,
            "grad_norm": 1.522302508354187,
            "learning_rate": 0.0001234567901234568,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 0.44483527541160583,
            "eval_pearson": 0.8996555423375133,
            "eval_spearman": 0.8992880874996251,
            "eval_runtime": 0.4172,
            "eval_samples_per_second": 3595.095,
            "eval_steps_per_second": 28.761,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.4168,
            "grad_norm": 2.0621323585510254,
            "learning_rate": 0.00011358024691358025,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 0.4452587068080902,
            "eval_pearson": 0.8990480855513396,
            "eval_spearman": 0.9001032424140726,
            "eval_runtime": 0.4065,
            "eval_samples_per_second": 3689.874,
            "eval_steps_per_second": 29.519,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.4057,
            "grad_norm": 1.8562437295913696,
            "learning_rate": 0.0001037037037037037,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 0.47372615337371826,
            "eval_pearson": 0.9002379479998655,
            "eval_spearman": 0.9006896121261244,
            "eval_runtime": 0.4153,
            "eval_samples_per_second": 3611.885,
            "eval_steps_per_second": 28.895,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 0.3937,
            "grad_norm": 3.364518165588379,
            "learning_rate": 9.382716049382717e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 0.46173104643821716,
            "eval_pearson": 0.9004461875839408,
            "eval_spearman": 0.9011041567022559,
            "eval_runtime": 0.5044,
            "eval_samples_per_second": 2973.815,
            "eval_steps_per_second": 23.791,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "loss": 0.384,
            "grad_norm": 2.8537814617156982,
            "learning_rate": 8.395061728395062e-05,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 0.44427689909935,
            "eval_pearson": 0.9006432250912463,
            "eval_spearman": 0.9024054288464217,
            "eval_runtime": 0.4832,
            "eval_samples_per_second": 3104.059,
            "eval_steps_per_second": 24.832,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "loss": 0.3794,
            "grad_norm": 1.5696924924850464,
            "learning_rate": 7.407407407407407e-05,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 0.4357708692550659,
            "eval_pearson": 0.9020719809140214,
            "eval_spearman": 0.9028102933531656,
            "eval_runtime": 0.4024,
            "eval_samples_per_second": 3727.301,
            "eval_steps_per_second": 29.818,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "loss": 0.3757,
            "grad_norm": 2.643064260482788,
            "learning_rate": 6.419753086419753e-05,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "eval_loss": 0.45950448513031006,
            "eval_pearson": 0.9036525006258579,
            "eval_spearman": 0.9029309622242565,
            "eval_runtime": 0.4561,
            "eval_samples_per_second": 3288.997,
            "eval_steps_per_second": 26.312,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "loss": 0.3735,
            "grad_norm": 1.2075591087341309,
            "learning_rate": 5.4320987654320986e-05,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "eval_loss": 0.4595281481742859,
            "eval_pearson": 0.903072822055395,
            "eval_spearman": 0.9024948960279948,
            "eval_runtime": 0.525,
            "eval_samples_per_second": 2857.376,
            "eval_steps_per_second": 22.859,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "loss": 0.3667,
            "grad_norm": 4.257986545562744,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "eval_loss": 0.43339771032333374,
            "eval_pearson": 0.9032721825058986,
            "eval_spearman": 0.9033844507355313,
            "eval_runtime": 0.579,
            "eval_samples_per_second": 2590.599,
            "eval_steps_per_second": 20.725,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "loss": 0.3598,
            "grad_norm": 1.0543593168258667,
            "learning_rate": 3.45679012345679e-05,
            "epoch": 84.44444444444444,
            "step": 3800
        },
        {
            "eval_loss": 0.4621753692626953,
            "eval_pearson": 0.9027085554751018,
            "eval_spearman": 0.9029801498274354,
            "eval_runtime": 0.526,
            "eval_samples_per_second": 2851.505,
            "eval_steps_per_second": 22.812,
            "epoch": 84.44444444444444,
            "step": 3800
        },
        {
            "train_runtime": 647.8003,
            "train_samples_per_second": 887.465,
            "train_steps_per_second": 6.947,
            "total_flos": 3.243540712783872e+16,
            "train_loss": 0.8742004133525647,
            "epoch": 84.44444444444444,
            "step": 3800
        },
        {
            "eval_loss": 0.43339771032333374,
            "eval_pearson": 0.9032721825058986,
            "eval_spearman": 0.9033844507355313,
            "eval_runtime": 0.3289,
            "eval_samples_per_second": 4561.145,
            "eval_steps_per_second": 36.489,
            "epoch": 84.44444444444444,
            "step": 3800
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "num_train_epochs": 100,
        "peft": "adalora",
        "rank": 8,
        "lora_alpha": 16,
        "lora_dropout": 0.05,
        "type": 2,
        "model_family": "roberta",
        "task": "stsb",
        "seed": 123,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "train_size": 5749
    },
    "train": {
        "train_time": 647.8003,
        "trainable_params_count": 1.181569,
        "memory_allocated": [
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936,
            538.535936
        ],
        "memory_reserved": [
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048,
            2076.18048
        ]
    },
    "variant": "lora"
}