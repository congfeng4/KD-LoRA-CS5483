{
    "eval_loss": 1.125756025314331,
    "eval_accuracy": 0.6823104693140795,
    "eval_runtime": 0.0615,
    "eval_samples_per_second": 4505.02,
    "eval_steps_per_second": 48.791,
    "epoch": 100.0,
    "log_history": [
        {
            "loss": 0.7038,
            "grad_norm": 2.7496650218963623,
            "learning_rate": 0.000154,
            "epoch": 7.7,
            "step": 154
        },
        {
            "eval_loss": 0.6810659766197205,
            "eval_accuracy": 0.5740072202166066,
            "eval_runtime": 0.1398,
            "eval_samples_per_second": 1980.982,
            "eval_steps_per_second": 21.455,
            "epoch": 7.7,
            "step": 154
        },
        {
            "loss": 0.5181,
            "grad_norm": 2.406402349472046,
            "learning_rate": 0.000188,
            "epoch": 15.4,
            "step": 308
        },
        {
            "eval_loss": 0.804826557636261,
            "eval_accuracy": 0.6642599277978339,
            "eval_runtime": 0.0737,
            "eval_samples_per_second": 3757.498,
            "eval_steps_per_second": 40.695,
            "epoch": 15.4,
            "step": 308
        },
        {
            "loss": 0.2302,
            "grad_norm": 2.3846235275268555,
            "learning_rate": 0.0001708888888888889,
            "epoch": 23.1,
            "step": 462
        },
        {
            "eval_loss": 1.125756025314331,
            "eval_accuracy": 0.6823104693140795,
            "eval_runtime": 0.1491,
            "eval_samples_per_second": 1857.955,
            "eval_steps_per_second": 20.122,
            "epoch": 23.1,
            "step": 462
        },
        {
            "loss": 0.1053,
            "grad_norm": 1.6075634956359863,
            "learning_rate": 0.00015377777777777777,
            "epoch": 30.8,
            "step": 616
        },
        {
            "eval_loss": 1.4875495433807373,
            "eval_accuracy": 0.6570397111913358,
            "eval_runtime": 0.1021,
            "eval_samples_per_second": 2713.257,
            "eval_steps_per_second": 29.385,
            "epoch": 30.8,
            "step": 616
        },
        {
            "loss": 0.0616,
            "grad_norm": 1.8362067937850952,
            "learning_rate": 0.00013666666666666666,
            "epoch": 38.5,
            "step": 770
        },
        {
            "eval_loss": 1.6136394739151,
            "eval_accuracy": 0.6714801444043321,
            "eval_runtime": 0.1032,
            "eval_samples_per_second": 2683.769,
            "eval_steps_per_second": 29.066,
            "epoch": 38.5,
            "step": 770
        },
        {
            "loss": 0.043,
            "grad_norm": 3.5749123096466064,
            "learning_rate": 0.00011955555555555556,
            "epoch": 46.2,
            "step": 924
        },
        {
            "eval_loss": 1.7753617763519287,
            "eval_accuracy": 0.6823104693140795,
            "eval_runtime": 0.1984,
            "eval_samples_per_second": 1396.033,
            "eval_steps_per_second": 15.119,
            "epoch": 46.2,
            "step": 924
        },
        {
            "loss": 0.0308,
            "grad_norm": 1.8943300247192383,
            "learning_rate": 0.00010244444444444446,
            "epoch": 53.9,
            "step": 1078
        },
        {
            "eval_loss": 2.0087926387786865,
            "eval_accuracy": 0.6714801444043321,
            "eval_runtime": 0.1324,
            "eval_samples_per_second": 2091.5,
            "eval_steps_per_second": 22.652,
            "epoch": 53.9,
            "step": 1078
        },
        {
            "loss": 0.0255,
            "grad_norm": 2.9146459102630615,
            "learning_rate": 8.533333333333334e-05,
            "epoch": 61.6,
            "step": 1232
        },
        {
            "eval_loss": 2.0329782962799072,
            "eval_accuracy": 0.6678700361010831,
            "eval_runtime": 0.1376,
            "eval_samples_per_second": 2012.458,
            "eval_steps_per_second": 21.796,
            "epoch": 61.6,
            "step": 1232
        },
        {
            "loss": 0.0232,
            "grad_norm": 2.1015360355377197,
            "learning_rate": 6.822222222222222e-05,
            "epoch": 69.3,
            "step": 1386
        },
        {
            "eval_loss": 2.1170904636383057,
            "eval_accuracy": 0.6823104693140795,
            "eval_runtime": 0.1153,
            "eval_samples_per_second": 2403.439,
            "eval_steps_per_second": 26.03,
            "epoch": 69.3,
            "step": 1386
        },
        {
            "loss": 0.0182,
            "grad_norm": 2.2164852619171143,
            "learning_rate": 5.111111111111111e-05,
            "epoch": 77.0,
            "step": 1540
        },
        {
            "eval_loss": 2.1164779663085938,
            "eval_accuracy": 0.6750902527075813,
            "eval_runtime": 0.1127,
            "eval_samples_per_second": 2458.592,
            "eval_steps_per_second": 26.627,
            "epoch": 77.0,
            "step": 1540
        },
        {
            "loss": 0.0142,
            "grad_norm": 1.2617948055267334,
            "learning_rate": 3.4000000000000007e-05,
            "epoch": 84.7,
            "step": 1694
        },
        {
            "eval_loss": 2.193058729171753,
            "eval_accuracy": 0.6642599277978339,
            "eval_runtime": 0.1102,
            "eval_samples_per_second": 2514.027,
            "eval_steps_per_second": 27.228,
            "epoch": 84.7,
            "step": 1694
        },
        {
            "loss": 0.0138,
            "grad_norm": 2.8198773860931396,
            "learning_rate": 1.688888888888889e-05,
            "epoch": 92.4,
            "step": 1848
        },
        {
            "eval_loss": 2.2204818725585938,
            "eval_accuracy": 0.6750902527075813,
            "eval_runtime": 0.1077,
            "eval_samples_per_second": 2571.7,
            "eval_steps_per_second": 27.852,
            "epoch": 92.4,
            "step": 1848
        },
        {
            "train_runtime": 233.5342,
            "train_samples_per_second": 1066.225,
            "train_steps_per_second": 8.564,
            "total_flos": 1.6897391466643456e+16,
            "train_loss": 0.13866136372089385,
            "epoch": 100.0,
            "step": 2000
        },
        {
            "eval_loss": 1.125756025314331,
            "eval_accuracy": 0.6823104693140795,
            "eval_runtime": 0.0615,
            "eval_samples_per_second": 4505.02,
            "eval_steps_per_second": 48.791,
            "epoch": 100.0,
            "step": 2000
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "lora_dropout": 0.05,
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "type": 2,
        "model_family": "bert",
        "task": "rte",
        "peft": "olora",
        "seed": 42,
        "rank": 8,
        "lora_alpha": 16,
        "student_model_name": "./models/distilbert-base-uncased",
        "teacher_model_name": "./models/bert-base-uncased",
        "train_size": 2490
    },
    "train": {
        "train_time": 233.5342,
        "trainable_params_count": 0.29645,
        "memory_allocated": [
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976
        ],
        "memory_reserved": [
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016
        ]
    },
    "variant": "lora"
}