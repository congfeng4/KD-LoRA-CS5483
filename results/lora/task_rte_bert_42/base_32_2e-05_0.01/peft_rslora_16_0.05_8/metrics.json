{
    "eval_loss": 2.0504586696624756,
    "eval_accuracy": 0.7148014440433214,
    "eval_runtime": 0.1001,
    "eval_samples_per_second": 2768.359,
    "eval_steps_per_second": 29.982,
    "epoch": 100.0,
    "log_history": [
        {
            "loss": 0.6901,
            "grad_norm": 3.352372407913208,
            "learning_rate": 0.000154,
            "epoch": 7.7,
            "step": 154
        },
        {
            "eval_loss": 0.6604098677635193,
            "eval_accuracy": 0.6353790613718412,
            "eval_runtime": 0.1353,
            "eval_samples_per_second": 2047.968,
            "eval_steps_per_second": 22.18,
            "epoch": 7.7,
            "step": 154
        },
        {
            "loss": 0.4482,
            "grad_norm": 4.894737243652344,
            "learning_rate": 0.000188,
            "epoch": 15.4,
            "step": 308
        },
        {
            "eval_loss": 0.803814709186554,
            "eval_accuracy": 0.7003610108303249,
            "eval_runtime": 0.1372,
            "eval_samples_per_second": 2019.026,
            "eval_steps_per_second": 21.867,
            "epoch": 15.4,
            "step": 308
        },
        {
            "loss": 0.2227,
            "grad_norm": 3.268092632293701,
            "learning_rate": 0.0001708888888888889,
            "epoch": 23.1,
            "step": 462
        },
        {
            "eval_loss": 1.0521323680877686,
            "eval_accuracy": 0.6895306859205776,
            "eval_runtime": 0.1346,
            "eval_samples_per_second": 2058.155,
            "eval_steps_per_second": 22.29,
            "epoch": 23.1,
            "step": 462
        },
        {
            "loss": 0.1204,
            "grad_norm": 3.975243330001831,
            "learning_rate": 0.00015377777777777777,
            "epoch": 30.8,
            "step": 616
        },
        {
            "eval_loss": 1.2371429204940796,
            "eval_accuracy": 0.6931407942238267,
            "eval_runtime": 0.1166,
            "eval_samples_per_second": 2375.54,
            "eval_steps_per_second": 25.728,
            "epoch": 30.8,
            "step": 616
        },
        {
            "loss": 0.0739,
            "grad_norm": 3.719729423522949,
            "learning_rate": 0.00013666666666666666,
            "epoch": 38.5,
            "step": 770
        },
        {
            "eval_loss": 1.4301834106445312,
            "eval_accuracy": 0.6859205776173285,
            "eval_runtime": 0.1134,
            "eval_samples_per_second": 2442.537,
            "eval_steps_per_second": 26.453,
            "epoch": 38.5,
            "step": 770
        },
        {
            "loss": 0.0532,
            "grad_norm": 2.561016082763672,
            "learning_rate": 0.00011955555555555556,
            "epoch": 46.2,
            "step": 924
        },
        {
            "eval_loss": 1.471445083618164,
            "eval_accuracy": 0.6931407942238267,
            "eval_runtime": 0.1078,
            "eval_samples_per_second": 2569.192,
            "eval_steps_per_second": 27.825,
            "epoch": 46.2,
            "step": 924
        },
        {
            "loss": 0.0371,
            "grad_norm": 3.4839038848876953,
            "learning_rate": 0.00010244444444444446,
            "epoch": 53.9,
            "step": 1078
        },
        {
            "eval_loss": 1.6791961193084717,
            "eval_accuracy": 0.6967509025270758,
            "eval_runtime": 0.1003,
            "eval_samples_per_second": 2761.076,
            "eval_steps_per_second": 29.903,
            "epoch": 53.9,
            "step": 1078
        },
        {
            "loss": 0.0297,
            "grad_norm": 8.853118896484375,
            "learning_rate": 8.533333333333334e-05,
            "epoch": 61.6,
            "step": 1232
        },
        {
            "eval_loss": 1.7802993059158325,
            "eval_accuracy": 0.7075812274368231,
            "eval_runtime": 0.2083,
            "eval_samples_per_second": 1330.072,
            "eval_steps_per_second": 14.405,
            "epoch": 61.6,
            "step": 1232
        },
        {
            "loss": 0.0247,
            "grad_norm": 2.435276985168457,
            "learning_rate": 6.822222222222222e-05,
            "epoch": 69.3,
            "step": 1386
        },
        {
            "eval_loss": 1.8378320932388306,
            "eval_accuracy": 0.6931407942238267,
            "eval_runtime": 0.1165,
            "eval_samples_per_second": 2377.358,
            "eval_steps_per_second": 25.748,
            "epoch": 69.3,
            "step": 1386
        },
        {
            "loss": 0.0191,
            "grad_norm": 0.12628386914730072,
            "learning_rate": 5.111111111111111e-05,
            "epoch": 77.0,
            "step": 1540
        },
        {
            "eval_loss": 2.002366065979004,
            "eval_accuracy": 0.7111913357400722,
            "eval_runtime": 0.1876,
            "eval_samples_per_second": 1476.468,
            "eval_steps_per_second": 15.991,
            "epoch": 77.0,
            "step": 1540
        },
        {
            "loss": 0.0162,
            "grad_norm": 2.5206453800201416,
            "learning_rate": 3.4000000000000007e-05,
            "epoch": 84.7,
            "step": 1694
        },
        {
            "eval_loss": 2.011491298675537,
            "eval_accuracy": 0.7075812274368231,
            "eval_runtime": 0.0917,
            "eval_samples_per_second": 3020.246,
            "eval_steps_per_second": 32.71,
            "epoch": 84.7,
            "step": 1694
        },
        {
            "loss": 0.015,
            "grad_norm": 5.479553699493408,
            "learning_rate": 1.688888888888889e-05,
            "epoch": 92.4,
            "step": 1848
        },
        {
            "eval_loss": 2.0504586696624756,
            "eval_accuracy": 0.7148014440433214,
            "eval_runtime": 0.0989,
            "eval_samples_per_second": 2800.577,
            "eval_steps_per_second": 30.331,
            "epoch": 92.4,
            "step": 1848
        },
        {
            "train_runtime": 252.7205,
            "train_samples_per_second": 985.278,
            "train_steps_per_second": 7.914,
            "total_flos": 1.6897391466643456e+16,
            "train_loss": 0.13553894621133805,
            "epoch": 100.0,
            "step": 2000
        },
        {
            "eval_loss": 2.0504586696624756,
            "eval_accuracy": 0.7148014440433214,
            "eval_runtime": 0.1001,
            "eval_samples_per_second": 2768.359,
            "eval_steps_per_second": 29.982,
            "epoch": 100.0,
            "step": 2000
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "lora_dropout": 0.05,
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "type": 2,
        "model_family": "bert",
        "task": "rte",
        "peft": "rslora",
        "seed": 42,
        "rank": 8,
        "lora_alpha": 16,
        "student_model_name": "./models/distilbert-base-uncased",
        "teacher_model_name": "./models/bert-base-uncased",
        "train_size": 2490
    },
    "train": {
        "train_time": 252.7205,
        "trainable_params_count": 0.29645,
        "memory_allocated": [
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976,
            462.526976
        ],
        "memory_reserved": [
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016
        ]
    },
    "variant": "lora"
}