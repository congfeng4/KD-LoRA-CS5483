{
    "eval_loss": 1.0138214826583862,
    "eval_accuracy": 0.6931407942238267,
    "eval_runtime": 0.0882,
    "eval_samples_per_second": 3142.115,
    "eval_steps_per_second": 34.03,
    "epoch": 100.0,
    "log_history": [
        {
            "loss": 0.6997,
            "grad_norm": 2.1400909423828125,
            "learning_rate": 0.000154,
            "epoch": 7.7,
            "step": 154
        },
        {
            "eval_loss": 0.6696944832801819,
            "eval_accuracy": 0.5956678700361011,
            "eval_runtime": 0.1477,
            "eval_samples_per_second": 1874.893,
            "eval_steps_per_second": 20.306,
            "epoch": 7.7,
            "step": 154
        },
        {
            "loss": 0.5348,
            "grad_norm": 1.5020166635513306,
            "learning_rate": 0.000188,
            "epoch": 15.4,
            "step": 308
        },
        {
            "eval_loss": 0.7463027238845825,
            "eval_accuracy": 0.6787003610108303,
            "eval_runtime": 0.2489,
            "eval_samples_per_second": 1112.827,
            "eval_steps_per_second": 12.052,
            "epoch": 15.4,
            "step": 308
        },
        {
            "loss": 0.3333,
            "grad_norm": 1.6428498029708862,
            "learning_rate": 0.0001708888888888889,
            "epoch": 23.1,
            "step": 462
        },
        {
            "eval_loss": 0.8865759968757629,
            "eval_accuracy": 0.6823104693140795,
            "eval_runtime": 0.086,
            "eval_samples_per_second": 3221.226,
            "eval_steps_per_second": 34.887,
            "epoch": 23.1,
            "step": 462
        },
        {
            "loss": 0.2184,
            "grad_norm": 3.250706195831299,
            "learning_rate": 0.00015377777777777777,
            "epoch": 30.8,
            "step": 616
        },
        {
            "eval_loss": 1.0138214826583862,
            "eval_accuracy": 0.6931407942238267,
            "eval_runtime": 0.2495,
            "eval_samples_per_second": 1110.194,
            "eval_steps_per_second": 12.024,
            "epoch": 30.8,
            "step": 616
        },
        {
            "loss": 0.1403,
            "grad_norm": 1.937698483467102,
            "learning_rate": 0.00013666666666666666,
            "epoch": 38.5,
            "step": 770
        },
        {
            "eval_loss": 1.1863481998443604,
            "eval_accuracy": 0.6859205776173285,
            "eval_runtime": 0.2488,
            "eval_samples_per_second": 1113.249,
            "eval_steps_per_second": 12.057,
            "epoch": 38.5,
            "step": 770
        },
        {
            "loss": 0.1059,
            "grad_norm": 3.540048122406006,
            "learning_rate": 0.00011955555555555556,
            "epoch": 46.2,
            "step": 924
        },
        {
            "eval_loss": 1.3284164667129517,
            "eval_accuracy": 0.6714801444043321,
            "eval_runtime": 0.1504,
            "eval_samples_per_second": 1841.456,
            "eval_steps_per_second": 19.944,
            "epoch": 46.2,
            "step": 924
        },
        {
            "loss": 0.082,
            "grad_norm": 3.2377278804779053,
            "learning_rate": 0.00010244444444444446,
            "epoch": 53.9,
            "step": 1078
        },
        {
            "eval_loss": 1.349217414855957,
            "eval_accuracy": 0.6787003610108303,
            "eval_runtime": 0.1487,
            "eval_samples_per_second": 1862.99,
            "eval_steps_per_second": 20.177,
            "epoch": 53.9,
            "step": 1078
        },
        {
            "loss": 0.0605,
            "grad_norm": 4.47654914855957,
            "learning_rate": 8.533333333333334e-05,
            "epoch": 61.6,
            "step": 1232
        },
        {
            "eval_loss": 1.58269464969635,
            "eval_accuracy": 0.6931407942238267,
            "eval_runtime": 0.1483,
            "eval_samples_per_second": 1868.2,
            "eval_steps_per_second": 20.233,
            "epoch": 61.6,
            "step": 1232
        },
        {
            "loss": 0.0534,
            "grad_norm": 2.4630162715911865,
            "learning_rate": 6.822222222222222e-05,
            "epoch": 69.3,
            "step": 1386
        },
        {
            "eval_loss": 1.642128586769104,
            "eval_accuracy": 0.6823104693140795,
            "eval_runtime": 0.149,
            "eval_samples_per_second": 1858.588,
            "eval_steps_per_second": 20.129,
            "epoch": 69.3,
            "step": 1386
        },
        {
            "loss": 0.0491,
            "grad_norm": 1.4960674047470093,
            "learning_rate": 5.111111111111111e-05,
            "epoch": 77.0,
            "step": 1540
        },
        {
            "eval_loss": 1.6471346616744995,
            "eval_accuracy": 0.6823104693140795,
            "eval_runtime": 0.1528,
            "eval_samples_per_second": 1812.914,
            "eval_steps_per_second": 19.634,
            "epoch": 77.0,
            "step": 1540
        },
        {
            "loss": 0.0416,
            "grad_norm": 2.4429051876068115,
            "learning_rate": 3.4000000000000007e-05,
            "epoch": 84.7,
            "step": 1694
        },
        {
            "eval_loss": 1.7258968353271484,
            "eval_accuracy": 0.6823104693140795,
            "eval_runtime": 0.1474,
            "eval_samples_per_second": 1878.649,
            "eval_steps_per_second": 20.346,
            "epoch": 84.7,
            "step": 1694
        },
        {
            "loss": 0.0369,
            "grad_norm": 2.0426416397094727,
            "learning_rate": 1.688888888888889e-05,
            "epoch": 92.4,
            "step": 1848
        },
        {
            "eval_loss": 1.7860678434371948,
            "eval_accuracy": 0.6714801444043321,
            "eval_runtime": 0.1213,
            "eval_samples_per_second": 2283.559,
            "eval_steps_per_second": 24.732,
            "epoch": 92.4,
            "step": 1848
        },
        {
            "train_runtime": 327.2918,
            "train_samples_per_second": 760.789,
            "train_steps_per_second": 6.111,
            "total_flos": 1.6901015345299456e+16,
            "train_loss": 0.1839865131378174,
            "epoch": 100.0,
            "step": 2000
        },
        {
            "eval_loss": 1.0138214826583862,
            "eval_accuracy": 0.6931407942238267,
            "eval_runtime": 0.0882,
            "eval_samples_per_second": 3142.115,
            "eval_steps_per_second": 34.03,
            "epoch": 100.0,
            "step": 2000
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "lora_dropout": 0.05,
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "type": 2,
        "model_family": "bert",
        "task": "rte",
        "peft": "dora",
        "seed": 42,
        "rank": 8,
        "lora_alpha": 16,
        "student_model_name": "./models/distilbert-base-uncased",
        "teacher_model_name": "./models/bert-base-uncased",
        "train_size": 2490
    },
    "train": {
        "train_time": 327.2918,
        "trainable_params_count": 0.314882,
        "memory_allocated": [
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096,
            462.8096
        ],
        "memory_reserved": [
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872,
            2329.935872
        ]
    },
    "variant": "lora"
}