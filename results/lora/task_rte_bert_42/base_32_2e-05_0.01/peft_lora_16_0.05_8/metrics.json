{
    "eval_loss": 0.8981430530548096,
    "eval_accuracy": 0.6967509025270758,
    "eval_runtime": 0.0686,
    "eval_samples_per_second": 4040.685,
    "eval_steps_per_second": 43.762,
    "epoch": 100.0,
    "log_history": [
        {
            "loss": 0.7005,
            "grad_norm": 3.1430959701538086,
            "learning_rate": 0.000154,
            "epoch": 7.7,
            "step": 154
        },
        {
            "eval_loss": 0.6682276725769043,
            "eval_accuracy": 0.5956678700361011,
            "eval_runtime": 0.0639,
            "eval_samples_per_second": 4333.928,
            "eval_steps_per_second": 46.938,
            "epoch": 7.7,
            "step": 154
        },
        {
            "loss": 0.5456,
            "grad_norm": 1.2760820388793945,
            "learning_rate": 0.000188,
            "epoch": 15.4,
            "step": 308
        },
        {
            "eval_loss": 0.7564882636070251,
            "eval_accuracy": 0.6787003610108303,
            "eval_runtime": 0.2065,
            "eval_samples_per_second": 1341.446,
            "eval_steps_per_second": 14.528,
            "epoch": 15.4,
            "step": 308
        },
        {
            "loss": 0.3386,
            "grad_norm": 1.648506999015808,
            "learning_rate": 0.0001708888888888889,
            "epoch": 23.1,
            "step": 462
        },
        {
            "eval_loss": 0.8981430530548096,
            "eval_accuracy": 0.6967509025270758,
            "eval_runtime": 0.1428,
            "eval_samples_per_second": 1939.308,
            "eval_steps_per_second": 21.003,
            "epoch": 23.1,
            "step": 462
        },
        {
            "loss": 0.2217,
            "grad_norm": 1.8590859174728394,
            "learning_rate": 0.00015377777777777777,
            "epoch": 30.8,
            "step": 616
        },
        {
            "eval_loss": 1.0234414339065552,
            "eval_accuracy": 0.6931407942238267,
            "eval_runtime": 0.1007,
            "eval_samples_per_second": 2752.083,
            "eval_steps_per_second": 29.806,
            "epoch": 30.8,
            "step": 616
        },
        {
            "loss": 0.1414,
            "grad_norm": 2.1688899993896484,
            "learning_rate": 0.00013666666666666666,
            "epoch": 38.5,
            "step": 770
        },
        {
            "eval_loss": 1.189100742340088,
            "eval_accuracy": 0.6931407942238267,
            "eval_runtime": 0.1397,
            "eval_samples_per_second": 1983.116,
            "eval_steps_per_second": 21.478,
            "epoch": 38.5,
            "step": 770
        },
        {
            "loss": 0.1045,
            "grad_norm": 2.00563383102417,
            "learning_rate": 0.00011955555555555556,
            "epoch": 46.2,
            "step": 924
        },
        {
            "eval_loss": 1.320316195487976,
            "eval_accuracy": 0.6787003610108303,
            "eval_runtime": 0.0973,
            "eval_samples_per_second": 2845.72,
            "eval_steps_per_second": 30.82,
            "epoch": 46.2,
            "step": 924
        },
        {
            "loss": 0.0774,
            "grad_norm": 1.7589337825775146,
            "learning_rate": 0.00010244444444444446,
            "epoch": 53.9,
            "step": 1078
        },
        {
            "eval_loss": 1.444139003753662,
            "eval_accuracy": 0.6895306859205776,
            "eval_runtime": 0.0696,
            "eval_samples_per_second": 3979.92,
            "eval_steps_per_second": 43.104,
            "epoch": 53.9,
            "step": 1078
        },
        {
            "loss": 0.0599,
            "grad_norm": 1.5551533699035645,
            "learning_rate": 8.533333333333334e-05,
            "epoch": 61.6,
            "step": 1232
        },
        {
            "eval_loss": 1.5660121440887451,
            "eval_accuracy": 0.6823104693140795,
            "eval_runtime": 0.109,
            "eval_samples_per_second": 2541.424,
            "eval_steps_per_second": 27.524,
            "epoch": 61.6,
            "step": 1232
        },
        {
            "loss": 0.0534,
            "grad_norm": 1.5511105060577393,
            "learning_rate": 6.822222222222222e-05,
            "epoch": 69.3,
            "step": 1386
        },
        {
            "eval_loss": 1.5667246580123901,
            "eval_accuracy": 0.6859205776173285,
            "eval_runtime": 0.095,
            "eval_samples_per_second": 2916.059,
            "eval_steps_per_second": 31.582,
            "epoch": 69.3,
            "step": 1386
        },
        {
            "loss": 0.0442,
            "grad_norm": 1.406909465789795,
            "learning_rate": 5.111111111111111e-05,
            "epoch": 77.0,
            "step": 1540
        },
        {
            "eval_loss": 1.6673074960708618,
            "eval_accuracy": 0.6787003610108303,
            "eval_runtime": 0.078,
            "eval_samples_per_second": 3552.81,
            "eval_steps_per_second": 38.478,
            "epoch": 77.0,
            "step": 1540
        },
        {
            "loss": 0.0395,
            "grad_norm": 1.9063221216201782,
            "learning_rate": 3.4000000000000007e-05,
            "epoch": 84.7,
            "step": 1694
        },
        {
            "eval_loss": 1.6829023361206055,
            "eval_accuracy": 0.6750902527075813,
            "eval_runtime": 0.0998,
            "eval_samples_per_second": 2774.421,
            "eval_steps_per_second": 30.048,
            "epoch": 84.7,
            "step": 1694
        },
        {
            "loss": 0.0355,
            "grad_norm": 2.507129669189453,
            "learning_rate": 1.688888888888889e-05,
            "epoch": 92.4,
            "step": 1848
        },
        {
            "eval_loss": 1.7154388427734375,
            "eval_accuracy": 0.6714801444043321,
            "eval_runtime": 0.0767,
            "eval_samples_per_second": 3612.014,
            "eval_steps_per_second": 39.119,
            "epoch": 92.4,
            "step": 1848
        },
        {
            "train_runtime": 209.6012,
            "train_samples_per_second": 1187.971,
            "train_steps_per_second": 9.542,
            "total_flos": 1.6897391466643456e+16,
            "train_loss": 0.1841704559326172,
            "epoch": 100.0,
            "step": 2000
        },
        {
            "eval_loss": 0.8981430530548096,
            "eval_accuracy": 0.6967509025270758,
            "eval_runtime": 0.0686,
            "eval_samples_per_second": 4040.685,
            "eval_steps_per_second": 43.762,
            "epoch": 100.0,
            "step": 2000
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./results",
        "lora_dropout": 0.05,
        "use_rslora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "type": 2,
        "model_family": "bert",
        "task": "rte",
        "peft": "lora",
        "seed": 42,
        "rank": 8,
        "lora_alpha": 16,
        "student_model_name": "./models/distilbert-base-uncased",
        "teacher_model_name": "./models/bert-base-uncased",
        "train_size": 2490
    },
    "train": {
        "train_time": 209.6012,
        "trainable_params_count": 0.29645,
        "memory_allocated": [
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176,
            462.002176
        ],
        "memory_reserved": [
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016,
            2002.78016
        ]
    },
    "variant": "lora"
}