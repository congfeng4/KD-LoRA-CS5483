{
    "eval_loss": 0.3838489055633545,
    "eval_matthews_correlation": 0.6579014583080778,
    "eval_runtime": 0.5699,
    "eval_samples_per_second": 1830.014,
    "eval_steps_per_second": 15.791,
    "epoch": 29.850746268656717,
    "log_history": [
        {
            "loss": 0.6306,
            "grad_norm": 0.25247722864151,
            "learning_rate": 5.970149253731343e-05,
            "epoch": 2.9850746268656714,
            "step": 200
        },
        {
            "eval_loss": 0.6149024367332458,
            "eval_matthews_correlation": 0.0,
            "eval_runtime": 0.5124,
            "eval_samples_per_second": 2035.338,
            "eval_steps_per_second": 17.563,
            "epoch": 2.9850746268656714,
            "step": 200
        },
        {
            "loss": 0.5192,
            "grad_norm": 1.4357030391693115,
            "learning_rate": 0.00011940298507462686,
            "epoch": 5.970149253731344,
            "step": 400
        },
        {
            "eval_loss": 0.4412338435649872,
            "eval_matthews_correlation": 0.5676609066599885,
            "eval_runtime": 0.5312,
            "eval_samples_per_second": 1963.631,
            "eval_steps_per_second": 16.944,
            "epoch": 5.970149253731344,
            "step": 400
        },
        {
            "loss": 0.3613,
            "grad_norm": 0.5581138730049133,
            "learning_rate": 0.0001791044776119403,
            "epoch": 8.955223880597014,
            "step": 600
        },
        {
            "eval_loss": 0.3907245099544525,
            "eval_matthews_correlation": 0.6036344190543846,
            "eval_runtime": 0.5465,
            "eval_samples_per_second": 1908.371,
            "eval_steps_per_second": 16.467,
            "epoch": 8.955223880597014,
            "step": 600
        },
        {
            "loss": 0.3058,
            "grad_norm": 0.8601895570755005,
            "learning_rate": 0.00019568822553897182,
            "epoch": 11.940298507462687,
            "step": 800
        },
        {
            "eval_loss": 0.3773910403251648,
            "eval_matthews_correlation": 0.6333042035646441,
            "eval_runtime": 0.5143,
            "eval_samples_per_second": 2027.955,
            "eval_steps_per_second": 17.499,
            "epoch": 11.940298507462687,
            "step": 800
        },
        {
            "loss": 0.2645,
            "grad_norm": 0.7480096817016602,
            "learning_rate": 0.00018905472636815922,
            "epoch": 14.925373134328359,
            "step": 1000
        },
        {
            "eval_loss": 0.3838489055633545,
            "eval_matthews_correlation": 0.6579014583080778,
            "eval_runtime": 0.508,
            "eval_samples_per_second": 2053.2,
            "eval_steps_per_second": 17.717,
            "epoch": 14.925373134328359,
            "step": 1000
        },
        {
            "loss": 0.2376,
            "grad_norm": 0.6421608924865723,
            "learning_rate": 0.0001824212271973466,
            "epoch": 17.91044776119403,
            "step": 1200
        },
        {
            "eval_loss": 0.41673436760902405,
            "eval_matthews_correlation": 0.6553837937800786,
            "eval_runtime": 0.5545,
            "eval_samples_per_second": 1880.982,
            "eval_steps_per_second": 16.231,
            "epoch": 17.91044776119403,
            "step": 1200
        },
        {
            "loss": 0.2134,
            "grad_norm": 0.7715435028076172,
            "learning_rate": 0.000175787728026534,
            "epoch": 20.895522388059703,
            "step": 1400
        },
        {
            "eval_loss": 0.4241185784339905,
            "eval_matthews_correlation": 0.643050604687314,
            "eval_runtime": 0.4749,
            "eval_samples_per_second": 2196.073,
            "eval_steps_per_second": 18.95,
            "epoch": 20.895522388059703,
            "step": 1400
        },
        {
            "loss": 0.1914,
            "grad_norm": 1.510180115699768,
            "learning_rate": 0.0001691542288557214,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "eval_loss": 0.4594971835613251,
            "eval_matthews_correlation": 0.6481761115403615,
            "eval_runtime": 0.5454,
            "eval_samples_per_second": 1912.408,
            "eval_steps_per_second": 16.502,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "loss": 0.1692,
            "grad_norm": 1.1863902807235718,
            "learning_rate": 0.00016252072968490878,
            "epoch": 26.865671641791046,
            "step": 1800
        },
        {
            "eval_loss": 0.4605885446071625,
            "eval_matthews_correlation": 0.6419918232980878,
            "eval_runtime": 0.5836,
            "eval_samples_per_second": 1787.325,
            "eval_steps_per_second": 15.423,
            "epoch": 26.865671641791046,
            "step": 1800
        },
        {
            "loss": 0.1593,
            "grad_norm": 1.6661843061447144,
            "learning_rate": 0.00015588723051409618,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "eval_loss": 0.4802180528640747,
            "eval_matthews_correlation": 0.6529752188824345,
            "eval_runtime": 0.6061,
            "eval_samples_per_second": 1720.856,
            "eval_steps_per_second": 14.849,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "train_runtime": 315.0638,
            "train_samples_per_second": 2714.054,
            "train_steps_per_second": 21.266,
            "total_flos": 1.689771264966656e+16,
            "train_loss": 0.3052190589904785,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "eval_loss": 0.3838489055633545,
            "eval_matthews_correlation": 0.6579014583080778,
            "eval_runtime": 0.5699,
            "eval_samples_per_second": 1830.014,
            "eval_steps_per_second": 15.791,
            "epoch": 29.850746268656717,
            "step": 2000
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./ablation3",
        "lora_dropout": 0.05,
        "use_rslora": true,
        "use_olora": true,
        "lora_alpha": 16,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "model_family": "deberta",
        "task": "cola",
        "peft": "mrlora-rs-olora",
        "seed": 42,
        "rank": 8,
        "student_model_name": "./models/deberta-v3-small",
        "teacher_model_name": "./models/deberta-v3-base",
        "use_lcoef": false,
        "use_bias": false,
        "train_size": 8551
    },
    "train": {
        "train_time": 315.0638,
        "trainable_params_count": 0.29645,
        "memory_allocated": [
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376,
            761.65376
        ],
        "memory_reserved": [
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288,
            4915.724288
        ]
    },
    "variant": "lora"
}