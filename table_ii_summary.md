# Table II Analysis Summary
## KD-LoRA Paper: Comparing FFT Baseline with LoRA Variants for BERT on GLUE Tasks

Generated on: 2026-01-26  
Data source: `results/` directory  
Model: `bert-base-uncased`  
PEFT Variants: adalora, dora, lora, mrlora, olora, rslora

## Table Structure

- **Table IIa**: FFT baseline + LoRA variants (LoRA-only strategy, type=2/3)
- **Table IIb**: FFT baseline + LoRA variants (KD-LoRA strategy, type=1/3)

## Key Findings

### 1. Overall Performance Ranking

#### LoRA-only Strategy (Table IIa)
Ranking by average performance across GLUE tasks (descending):

1. **rslora**: 0.7769 (0.54% drop vs FFT)
2. **mrlora**: 0.7671 (1.80% drop vs FFT)
3. **dora**: 0.7666 (1.85% drop vs FFT)
4. **lora**: 0.7653 (2.02% drop vs FFT)
5. **olora**: 0.7624 (2.39% drop vs FFT)
6. **adalora**: 0.6520 (16.52% drop vs FFT)

#### KD-LoRA Strategy (Table IIb)
Ranking by average performance across GLUE tasks (descending):

1. **mrlora**: 0.7536 (4.75% drop vs FFT)
2. **rslora**: 0.7487 (5.37% drop vs FFT)
3. **dora**: 0.7441 (5.95% drop vs FFT)
4. **lora**: 0.7437 (6.00% drop vs FFT)
5. **olora**: 0.7247 (8.40% drop vs FFT)
6. **adalora**: 0.7004 (11.48% drop vs FFT)

### 2. Performance Relative to FFT Baseline

| Variant | LoRA-only Drop | KD-LoRA Drop | Notes |
|---------|----------------|--------------|-------|
| adalora | 16.52%         | 11.48%       | KD-LoRA improves adalora significantly |
| dora    | 1.85%          | 5.95%        | KD-LoRA degrades performance |
| lora    | 2.02%          | 6.00%        | KD-LoRA degrades performance |
| mrlora  | 1.80%          | 4.75%        | KD-LoRA degrades performance |
| olora   | 2.39%          | 8.40%        | KD-LoRA degrades performance |
| rslora  | 0.54%          | 5.37%        | KD-LoRA degrades performance |

**Observation**: KD-LoRA generally underperforms LoRA-only, except for adalora where KD-LoRA provides relative improvement (still below FFT).

### 3. Task-Specific Insights

#### Tasks where variants outperform FFT baseline:

**Table IIa (LoRA-only)**:
- **WNLI**: All variants outperform FFT (dora, lora, mrlora, olora, rslora)
  - Largest margin: rslora (+0.0892)

**Table IIb (KD-LoRA)**:
- **WNLI**: All variants outperform FFT (adalora, dora, lora, mrlora, olora, rslora)
  - Largest margin: mrlora (+0.0798)

#### Top-performing variant per task:

**LoRA-only**:
- cola, sst2, mrpc, qqp, stsb, qnli, rte, wnli, mnli_mm → **rslora**
- mnli_m → **mrlora**

**KD-LoRA**:
- mrpc, qqp, stsb, mnli_m, mnli_mm → **mrlora**
- qnli, rte → **rslora**
- wnli → **mrlora**

### 4. Data Completeness

- **Table IIa**: Complete data for all 10 tasks (including MNLI matched/mismatched)
- **Table IIb**: Missing data for **cola** and **sst2** tasks (no KD-LoRA experiments)
- **All variants** have at least one seed; LoRA-only experiments have 2-3 seeds per task
- **Seed variability**: Low standard deviations (<0.02 for most tasks)

### 5. Seed Variability Analysis

- **LoRA-only**: Multiple seeds (42, 123, 2024) for most tasks
  - Average standard deviation across variants: 0.0053–0.0100
  - Highest variability: WNLI adalora (std=0.0598)
- **KD-LoRA**: Single seed (42) for all tasks → zero standard deviation
- **FFT**: Three seeds per task, average std=0.0131

### 6. Efficiency Metrics (Preliminary)

Efficiency metrics (eval_runtime, eval_samples_per_second) are available in JSON files but not included in Table II. Future work could analyze trade-offs between performance and efficiency.

## Recommendations for Paper

1. **Highlight top performers**: rslora (LoRA-only) and mrlora (KD-LoRA) are best variants.
2. **Discuss KD-LoRA degradation**: KD-LoRA generally underperforms LoRA-only; investigate causes.
3. **Note data limitations**: Missing KD-LoRA experiments for cola and sst2 tasks.
4. **Include supplementary material**: Seed variability tables, performance drop visualizations.
5. **Consider statistical significance**: Low seed variability suggests robust results.

## Generated Files

- `table_iia_results.csv`, `table_iib_results.csv` – CSV tables
- `table_iia_latex.tex`, `table_iib_latex.tex` – LaTeX tables
- `table_iia_heatmap.png`, `table_iib_heatmap.png` – Heatmap visualizations
- `variant_comparison.png` – Bar plot comparing variants
- `table_ii_performance_drop.png` – Performance drop vs FFT
- `table_ii_summary.csv` – Summary statistics
- `seed_std_lora_only.csv`, `seed_std_kd_lora.csv` – Seed variability
- `scatter_lora_vs_kdlora.png` – Scatter plot comparison
- `difference_kdlora_minus_lora.png` – Difference bar plot

## Next Steps

1. Run missing KD-LoRA experiments for cola and sst2 tasks.
2. Analyze efficiency metrics (runtime, throughput).
3. Extend analysis to other models (RoBERTa, DeBERTa).
4. Perform statistical significance tests.
5. Write paper sections discussing variant performance differences.

---  
*This summary was automatically generated by `analyze_table_ii.py` and related scripts.*