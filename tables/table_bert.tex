\begin{table}[ht]
\centering
\begin{tabular}{lccccccccc}
\toprule
Method & CoLA & MNLI & MRPC & QNLI & QQP & RTE & SST-2 & STS-B & WNLI \\
\midrule
LoRA & 0.509 & 0.821 & 0.852 & 0.901 & 0.843 & 0.588 & 0.910 & 0.868 & 0.563 \\
DoRA & 0.498 & 0.823 & 0.858 & 0.900 & 0.844 & 0.592 & 0.913 & 0.870 & 0.563 \\
MrLoRA & 0.500 & \textbf{0.828} & 0.866 & 0.903 & 0.848 & 0.595 & \textbf{0.916} & 0.873 & 0.567 \\
MrLoRA-RS & 0.518 & 0.826 & \textbf{0.887} & \textbf{0.908} & \textbf{0.853} & \textbf{0.628} & 0.915 & \textbf{0.882} & 0.563 \\
OLoRA & 0.485 & 0.822 & 0.857 & 0.903 & 0.844 & 0.578 & 0.911 & 0.870 & 0.563 \\
RSLoRA & \textbf{0.521} & 0.825 & 0.882 & 0.905 & 0.849 & 0.604 & \textbf{0.916} & 0.876 & \textbf{0.570} \\
\bottomrule
\end{tabular}
\caption{Performance of LoRA variants on GLUE tasks (bert family)}
\label{tab:bert_glue}
\end{table}