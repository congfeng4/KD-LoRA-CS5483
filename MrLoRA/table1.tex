\begin{table}[h]
\centering
\setlength{\tabcolsep}{4pt} % Smaller column gap
\renewcommand{\arraystretch}{1.2} % Better vertical spacing
\caption{Results on GLUE development set for BERT-base (BERT-b), DeBERTa-v3-base (DeB-b), and RoBERTa-base (RoB-b). We compare different fine-tuning strategies: Fully Fine-Tuning (FFT), MR-LoRA Fine-Tuning (MR), and Knowledge Distillation MR-LoRA Fine-Tuning (KD). Results of two total ranks $r=15, 31$ are reported. We report the average correlation for STS-B. We report mean of 3 runs using different random seeds. }
\label{tab:perf-params}
\resizebox{\textwidth}{!}{% <--- Start resize
\begin{tabular}{l|l|c|cccccc}
\toprule
 \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{\# Params}} & \textbf{QQP} & \textbf{QNLI} & \textbf{RTE} & \textbf{MRPC} & \textbf{STS-B} & \textbf{All} \\
 &  &  & Acc/F1 & Acc & Acc & Acc & Corr & Ave. \\
\midrule
BERT-b & FFT & 109.48M & 88.81/85.45 & 91.14 & 64.98 & 84.56 & 89.34 & 83.92 \\
DeB-b & FFT & 184.42M & 90.39/87.39 & 93.81 & 83.03 & 89.22 & 91.16 & 89.53 \\
RoB-b & FFT & 124.65M & 87.66/84.44 & 92.37 & 79.06 & 88.48 & 90.93 & 87.72 \\
\bottomrule
\end{tabular}% <--- End resize
}
\end{table}