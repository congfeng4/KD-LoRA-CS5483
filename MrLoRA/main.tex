\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage[final]{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
    % \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{multirow}       % multi-row table cells
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{MR-LoRA: Multi-Rank Low Rank Adaptation of Large Language Models}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Cong Feng\thanks{Use footnote for providing further information
    about author (webpage, alternative address)} \\
  Department of Computer Science,\\
  City University of Hong Kong,\\
81 Tat Chee Avenue,
Kowloon Tong,
Hong Kong\\
  \texttt{congfeng4-c@my.cityu.edu.hk} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle

\begin{abstract}
Large language models (LLMs) achieve strong performance in natural language processsing tasks but are prohibitively large for specialization and deployment. 
Low-rank Adaptation (LoRA) freezes the weights of LLMs and injects trainable low-rank decomposed weights for parameter-efficient finetuning. Despite the high efficiency, a single rank of weights lacks the flexibly to capture task-specific nuances or student-teacher discrepancy.
We propose \textbf{M}ulti-\textbf{R}ank \textbf{LoRA} (\textbf{MR-LoRA}), a LoRA enhancement that incorperates the sum of exponentially decaying multiple ranks of weights with learnable coefficients.
Drawing inspiration from Singluar Value Decomposition and Taylor's series, the rapidly shrinking weights offer fine-grained complexity within bounded parameter budgets, while
the learnable coefficients automatically adjust importance across ranks.
Experiments on GLUE benchmark show MR-LoRA retains 92\% of the teacher’s full fine-tuning performance, using only 0.13\% additional trainable parameters relative to the base model, and achieves the highest performance in the knowledge distillation setting compared with four recent LoRA variants.
Analysis confirms higher ranks capture coarse corrections early, while smaller ranks refine subtle mismatches. Our code will be released soon.
\end{abstract}

\section{Introduction}
Scaling language models to billions of parameters has unleashed emergent capabilities in programming, mathematical reasoning, and multi-step problem solving. GPT-4 (\cite{openai2023gpt4}) and Gemini-Ultra (\cite{team2023gemini}) demonstrate expert-level performance on competitive programming benchmarks and graduate-level science exams, while smaller models like Llama-3-70B (\cite{llama31herdofmodels}) achieve strong results on tool use and long-context reasoning. However, the growth of model parameters brings prohibitive memory footprints, latency, and energy costs, making deployment on edge devices or task-specific specialization difficult.

Parameter-efficient fine-tuning (PEFT) has been proposed as an efficient alternative to fully finetuning (FFT) for specializing LLMs on downstream tasks. Instead of updating all parameters, PEFT methods inject small trainable modules while freezing the pretrained backbone.
\cite{houlsby2019parameter} proposes adapter layers with bottleneck architectures, achieving near full fine-tuning performance on natural language understanding tasks. Prefix-tuning (\cite{li-liang-2021-prefix}) prepends learnable tokens to keys and values, reducing trainable parameters to 0.1\% of original models. 
LoRA (\cite{hu2022lora}) enables task adaptation with less than 0.01\% trainable parameters and no inference latency by adding decomposed low-rank matrices to linear layers.
The successes of LoRA have motivated several following-up enhancements.
AdaLoRA (\cite{zhang2023adalora}) dynamically allocates parameter budgets across layers according to the importance scores of the weights.
RS-LoRA (\cite{kalajdzievski2023rslora}) introduces a rank stabilization scaling factor to address slow learning and performance bottleneck with higher ranks.
OLoRA (\cite{buyukaz2024olora}) improves the LLMs training convergence with othonormal initialization of low-rank matrices.
DoRA (\cite{liu2024dora}) decomposes pretrained weights into magnitude and direction components and applied LoRA to the direction component to minic the learning strategy of FFT, enhancing learning capacity without extra parameters.
These LoRA variants have improved training stability, learning capacity, and convergence speed of LoRA. However, the fundamental expressiveness bottleneck of a single rank decomposition has not been addressed.

Another line of research, knowledge distillation (KD), facilitates the efficient deployment of LLMs by transferring the teacher's predictive distribution to a much smaller student. \cite{hinton2015distilling} first showed that learning from soft targets improves generalization, retaining most of the accuracy while reducing parameters by orders of magnitude. Subsequent work has extended this to intermediate layer representations. 
\cite{sanh2019distilbert} distilled BERT into DistilBERT, reducing the size of the model by 40\% and retaining 97\% of GLUE (\cite{wang2018glue}) performance.
While traditional KD methods requires fully fine-tuning parameters of the student, KD-LoRA (\cite{pmlr-v262-kdlora}) takes a hybird approach that freezes the student's weights and training LoRA adapters to approximate the teacher's outputs.
Combing benefits from both worlds, it achieves performance comparable to fully finetuning and LoRA, while significantly reduces the resource usages.
However, the same expressiveness constraint of LoRA remains in the KD scenario, as the single rank matrix in each layer is often too coarse to address the knowledge discrepancies between teacher and student. 

Our motivation is to explore a new hypothesis space for low-rank adapters with enhanced expressivenes and bounded parameter budgets.
We propose \textbf{M}ulti-\textbf{R}ank \textbf{LoRA}, a LoRA variant that allocates trainable parameters as a series of exponentially decaying rank matrices per linear layer. 
Based on singular value decomposition (SVD), any matrix $A$ can be decomposed as $A = U\Sigma V^T = \sum_i \sigma_i u_i v_i^T$, a weighted sum of rank-one matrices, with singular values $\sigma_i$ as coefficients and orthogonal matrices $u_i v_i^T$ as bases.
This series elegantly decomposes complex structure into simpler components, with dominant directions captured by larger singular values and fine details by smaller ones.
To avoid rank collapse and explicit orthonormalization,
we draw on the exponential function approximation of Taylor's series and transform the SVD form into a group of exponentially decaying rank matrices.
This yields a series of matrices with rapidly decreasing ranks, rendering bounded parameter count while preserving multi-rank flexibility. 
An schematic illustration of our idea is shown in Fig.~\ref{fig:schematic}.

We evaluate MR-LoRA on the GLUE benchmark with three encoder-only language models: BERT (\cite{devlin-etal-2019-bert}), RoBERTa (\cite{liu2019roberta}), and DeBERTa-V3 (\cite{he2021debertav3}). 
We experiment with both LoRA finetuning and KD-LoRA-style distillation.
For each teacher model, we select a student model from the same family (e.g., BERT-base as teacher with BERT-small as student).
We also benchmark against recent LoRA variants including AdaLoRA, OLoRA, DoRA, and RS-LoRA.
All experiments are conducted on four NVIDIA GeForce RTX 4090 GPUs. 
Our experimental results demonstrate the effectiveness of the multi-rank low-rank adaptation:
\begin{itemize}%TODO
    \item In teacher fine-tuning, MR-LoRA retains 95.8--98.7\% of the teacher’s full fine-tuning performance across BERT, RoBERTa, and DeBERTa, demonstrating that multi-rank decomposition enhances adapter expressiveness even without distillation.
    \item In student distillation, MR-LoRA retains 91.4--94.1\% of the teacher’s performance while adding only 0.13\% extra trainable parameters relative to the base model.
    \item Compared to full fine-tuning, MR-LoRA reduces trainable parameters by 99.4\%, memory footprint by ~82\% (student distillation), and accelerates inference by 3.3$\times$ (teacher) to 4.0$\times$ (student).
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=0.6\textwidth]{figures/mrlora.pdf}
\caption{The structure of a single MR-LoRA adapter coupled with a linear layer. The adapter consists of $(1+\log_2 R)$ low-rank blocks with exponentially decreasing ranks $R, R/2, R/4, \dots, 1$, where $R$ is the highest rank. Each block contains a pair of matrices $B_i, A_i$. The rank of $B_i A_i$ is $R/2^{k-1}$. The output of each block are summed after being scaled by learnable weights $\lambda_i$.
}
\label{fig:schematic}
\end{figure}

\section{Method}
We propose Multi-Rank LoRA (MR-LoRA), which supplements the LoRA adapter with multiple rank-decaying residual matrices. We first recap standard LoRA and knowledge distillation LoRA (KD-LoRA), then derive our formulation inspired by SVD and Taylor's series.

\textbf{LoRA Preliminaries.} Following \cite{hu2022lora}, for a pre-trained weight matrix $W_0 \in \mathbb{R}^{d \times k}$, LoRA constrains the update $\Delta W$ by a low-rank decomposition:
\begin{align}
W = W_0 + \Delta W = W_0 + BA
\end{align}

where $B \in \mathbb{R}^{d \times r},\ A \in \mathbb{R}^{r \times k}$ are the low-rank components, rank $r \ll \min(d, k)$, and $W$ is the updated weight.
Then, the forward pass of a layer with LoRA adaptation is given by:
\begin{align}
h = (W_0 + \gamma_r BA) \cdot x + b
\end{align}

where $\gamma_r$ is a weighting factor, $x$ and $h$ are input and output vectors, and $b$ is the bias term.
During fine-tuning, $W_0$ and $b$ remain frozen while $A$ and $B$ are updated.

\textbf{Knowledge Distillation with LoRA.} The distillation objective transfers knowledge from a fine-tuned teacher $\mathcal{T}$ to a pretrained student $\mathcal{S}$ equipped with LoRA modules (\cite{pmlr-v262-kdlora}). The student is finetuned with a weighted combination of task loss and distillation loss:
\begin{align}
\mathcal{L}_{\text{total}} = \alpha \mathcal{L}_{\text{task}}(\mathcal{S}(x), y) + (1-\alpha) \mathcal{L}_{\text{KD}}(z^\mathcal{S}, z^\mathcal{T}),
\end{align}

where $z^\mathcal{T}, z^\mathcal{S}$ are teacher and student logits, $\mathcal{L}_{\text{KD}}$ is typically KL divergence, $x,y$ are input features and labels, $\mathcal{L}_{\text{task}}$ is a task-specific loss, and $\alpha$ balances the two objectives.

\textbf{Multi-Rank LoRA.} 
With SVD, $\Delta W \in \mathbb{R}^{d \times k}$ can be written as:

\begin{align}
\Delta W = \sum_{i=1}^{r} \sigma_i u_i v_i^\top \label{eq:svd}
\end{align}

where $\sigma_1 \geq \sigma_2 \geq \cdots > 0$ are non-zero singular values of $\Delta W $, $r = rank(\Delta W )$, and $u_i, v_i$ are left and right singular vectors.
While Eq.~\ref{eq:svd} is exact, directly allocating a LoRA module to each $\sigma_i u_i v_i^\top$ term without the orthonormal constraint will likely lead to $\Delta W$ collapse to rank one in the worse case. This would result in a waste of budgets of $(r-1)(d+k)$ parameters since only one rank is retained.
To prevent the rank collapse, all the terms $\sigma_i u_i v_i^\top$ should be othonormalized, but that would require excessive computation.

The Taylor's series can approximate a complicated function at point $a$ to any desirable precision by graduately adding more complicated terms $\frac{f^{(n)}(a)}{n!} (x-a)^n$ as needed. Inspired by this idea, we rewrite Eq.~\ref{eq:svd} by grouping the terms into blocks of exponentially decay ranks.
\begin{align}
\Delta W = \sum_{i=1}^{\log_2 (r+1)} \sum_{j \in \mathcal{I}_i} \sigma_j u_j v_j^\top
\end{align}

where $\mathcal{I}_i$ denotes the set of singular value indices belonging to block $i$, with $|\mathcal{I}_i| = r_i$ following the exponential decay schedule $r_i = r/2^{i-1}$. This grouping transforms the rank-1 combination of Eq.~\ref{eq:svd} into an exponentially decaying rank combination. Each block can be approximated by a low-rank matrix
\begin{align}
  B_i A_i = \sum_{j \in \mathcal{I}_i} \sigma_j u_j v_j^\top
\end{align}
where $B_i \in \mathbb{R}^{d \times r_i}$ aggregates the left singular vectors $u_i$ and $A_i \in \mathbb{R}^{r_i \times k}$ aggregates the right singular vectors $v_i^\top$ weighted by $\sigma_i$. To allow flexible capacity adjustable across blocks, we introduce learnable scalar weights $\lambda_i$, leading to the multi-rank adapter form:

\begin{align}
\Delta W = \sum_{i=1}^{\log_2 (r+1)} \frac{\lambda_i \alpha_i}{r_i} \cdot B_i A_i \label{eq:mrlora}
\end{align}

where $\lambda_i \in \mathbb{R}$ are learnable scalar weights, $r_i$ is the rank of block $i$, $\alpha_i$ is a constant in $r_i$ as in \cite{liu2019roberta}, $r$ is the total rank and $R$ is highest rank. $R$ is alwasy a power of two. %, leading to a simple relation between the total rank $r$ and the highest rank $R$: $r = 2R - 1$.
For each block with rank $r_i$, the corresponding LoRA module incurs $r_i (d + k)$ parameters. Therefore, Eq.~\ref{eq:mrlora} incurs a parameter overhead of $\sum_i r_i (d+k) = r(d+k)$, which is the same as the original LoRA formulation. We note that MR-LoRA does not introduce extra parameters given fixed total budgets, but uses a more structural allocation scheme. In practice, we set $R = \lfloor (r+1) / 2 \rfloor$ given the total rank $r$.


\section{Experiments}
We evaluate MR-LoRA on the GLUE benchmark \citep{wang2018glue} to assess its effectiveness in both teacher fine-tuning and student distillation scenarios. GLUE comprises nine natural-language understanding tasks with varying difficulty, including single-sentence classification (CoLA, SST-2), similarity and paraphrase (MRPC, QQP, STS-B), and natural-language inference (MNLI, QNLI, RTE, WNLI). We report accuracy for classification tasks, Matthews correlation for CoLA, and Pearson correlation for STS-B. The overall score is the average across tasks.

\subsection{Datasets and Tasks}
We use the standard GLUE training/validation splits. For each task, we fine-tune the teacher model with full fine-tuning (FFT) and our MR-LoRA; baseline results for standard LoRA are provided. For distillation, we freeze the student model and train only the MR-LoRA adapters using a weighted combination of task loss and knowledge-distillation loss (KL divergence). The student models are distilled variants of the same family: DistilBERT-base, DistilRoBERTa-base, and DeBERTa-v3-small.

\subsection{Models}
We experiment with three encoder-only language model families: BERT-base (110M parameters), RoBERTa-base (125M), and DeBERTa-v3-base (183M). For each family we employ the corresponding distilled student: DistilBERT-base (66M), DistilRoBERTa-base (82M), and DeBERTa-v3-small (96M). All models are loaded from the Hugging Face Hub.

\subsection{Baselines}
We compare the following fine-tuning strategies:
\begin{itemize}
    \item \textbf{FFT}: Full fine-tuning of the teacher model (upper-bound performance).
    \item \textbf{Teacher MR-LoRA}: Multi-rank low-rank adaptation (ranks $\{8,4,2\}$) applied directly to the teacher.
    \item \textbf{Student MR-LoRA}: Multi-rank adapters applied to the frozen student with knowledge distillation.
    \item \textbf{Other LoRA variants}: AdaLoRA, OLoRA, DoRA, RS-LoRA, and MR-LoRA-RS (our MR-LoRA combined with the rank stabilization scaling factor from RS-LoRA) (see Table~\ref{tab:teacher-variants}).
\end{itemize}

\subsection{Implementation Details}
All experiments are conducted on NVIDIA A800 GPUs. We use the AdamW optimizer with learning rate $2\times10^{-4}$, batch size 32, and linear learning-rate decay. The distillation weight $\alpha$ is set to 0.5. For LoRA and MR-LoRA we set the maximum rank $R=8$; MR-LoRA uses $K=3$ blocks with ranks $\{8,4,2\}$. The learnable weights $\lambda_i$ are initialized as $\{1,0.5,0.25\}$. We also evaluate MR-LoRA-RS, which combines MR-LoRA with the rank stabilization scaling factor from RS-LoRA \cite{kalajdzievski2023rslora}. We run each experiment with three random seeds and report the mean performance.

\subsection{Results}
\subsubsection{GLUE Performance}
Table~\ref{tab:glue-scores} presents the per-task GLUE scores for each model family and fine-tuning strategy. Teacher MR-LoRA retains 95.8--98.7\% of the teacher’s full fine-tuning performance across the three model families, demonstrating the effectiveness of multi-rank decomposition even without distillation. Student MR-LoRA retains 91.4--94.1\% of the teacher’s performance while using less than 1\% of the teacher’s trainable parameters. The performance gap between teacher and student MR-LoRA ranges from 3.6 to 5.2 GLUE points, reflecting the inherent difficulty of knowledge distillation.
\begin{table}[ht]
\centering
\begin{tabular}{llcccccccccc}
\toprule
Method & \# Params & Mcc & m/mm & Acc/F1 & Acc & Acc/F1 & Acc & Acc & Pearson/Spearman & Acc & Avg. \\
 & & COLA & MNLI & MRPC & QNLI & QQP & RTE & SST2 & STSB & WNLI & \\
\midrule
FFT & 184.424 & \textbf{65.79} & \textbf{89.32/89.58} & \textbf{89.22/92.24} & \textbf{93.71} & \textbf{92.23/89.66} & \textbf{79.78} & \textbf{95.68} & \textbf{90.36/90.30} & \textbf{55.4} & 83.54 \\
LoRA & 0.214 & 44.75 & 85.51/85.56 & 68.38/81.22 & 90.17 & 88.07/83.26 & 52.71 & 93.51 & 76.41/77.71 & 52.11 & 73.44 \\
DoRA & 0.224 & 44.63 & 85.52/85.64 & 68.38/81.22 & 90.15 & 88.09/83.28 & 52.71 & 93.58 & 74.93/77.33 & 52.11 & 73.39 \\
MrLoRA & 0.409 & 56.02 & 87.17/86.67 & 70.14/81.96 & 91.34 & 89.82/86.30 & 52.71 & 93.78 & 83.23/83.93 & 54.23 & 76.38 \\
MrLoRA-RS & 0.409 & 56.68 & 87.46/87.17 & 77.45/85.54 & 91.93 & 90.37/87.03 & 60.95 & 94.22 & 85.40/85.82 & 54.23 & 78.21 \\
OLoRA & 0.214 & 47.91 & 85.79/85.82 & 68.38/81.22 & 90.26 & 88.58/84.23 & 53.31 & 93.46 & 80.48/81.97 & 52.11 & 74.47 \\
RSLoRA & 0.214 & 50.93 & 85.93/86.00 & 69.04/81.51 & 91.04 & 88.22/83.30 & 55.54 & 93.51 & 71.40/72.81 & 52.11 & 74.08 \\
\bottomrule
\end{tabular}
\caption{Performance of LoRA variants on GLUE tasks (deberta family, kd-lora training)}
\label{tab:kd-lora_deberta_glue}
\end{table}
% \begin{table}[ht]
% \centering
% \small
% \caption{Per-task GLUE scores (higher is better) for each model family and fine-tuning strategy. FFT denotes full fine-tuning of the teacher; Teacher MR-LoRA applies multi-rank low-rank adapters directly to the teacher; Student MR-LoRA refers to multi-rank adapters applied to the frozen student with knowledge distillation. Abbreviations: BERT (BERT-base teacher / DistilBERT-base student), RoB (RoBERTa-base / DistilRoBERTa-base), DeB (DeBERTa-v3-base / DeBERTa-v3-small); T-MR: Teacher MR-LoRA; S-MR: Student MR-LoRA. Metrics: CoLA (Matthews correlation), SST-2 (accuracy), MRPC (average of accuracy and F1), QQP (average of accuracy and F1), STS-B (Pearson correlation), QNLI (accuracy), RTE (accuracy), WNLI (accuracy), MNLI$_m$ (matched accuracy), MNLI$_{mm}$ (mismatched accuracy). Score is the average across the ten tasks.}
% \label{tab:glue-scores}
% \begin{tabular}{lccccccccc}
% \toprule
% \multirow{2}{*}{Task} & \multicolumn{3}{c}{BERT} & \multicolumn{3}{c}{RoB} & \multicolumn{3}{c}{DeB} \\
% \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
%  & FFT & T-MR & S-MR & FFT & T-MR & S-MR & FFT & T-MR & S-MR \\
% \midrule
% CoLA       & 56.2 & 50.0 & 43.0 & 59.3 & 54.2 & 45.6 & 65.8 & 63.0 & 55.4 \\
% SST-2      & 92.0 & 91.7 & 90.9 & 93.3 & 94.0 & 91.7 & 95.7 & 95.8 & 93.8 \\
% MRPC       & 87.9 & 83.3 & 76.9 & 90.5 & 84.4 & 77.2 & 90.7 & 81.8 & 74.8 \\
% QQP        & 89.5 & 86.7 & 85.3 & 89.2 & 87.4 & 85.7 & 90.9 & 89.3 & 88.0 \\
% STS-B      & 88.2 & 87.4 & 83.3 & 90.2 & 87.2 & 84.6 & 90.4 & 84.5 & 86.3 \\
% QNLI       & 90.7 & 90.2 & 86.7 & 92.5 & 92.2 & 88.4 & 93.7 & 93.9 & 91.4 \\
% RTE        & 62.1 & 59.6 & 56.9 & 67.6 & 63.2 & 57.0 & 79.8 & 61.3 & 52.7 \\
% WNLI       & 48.4 & 56.8 & 52.8 & 51.2 & 56.3 & 45.4 & 55.4 & 56.3 & 53.2 \\
% MNLI$_m$   & 83.3 & 82.9 & 79.6 & 87.3 & 86.6 & 82.0 & 89.3 & 90.3 & 87.1 \\
% MNLI$_{mm}$& 83.6 & 83.2 & 80.2 & 87.1 & 87.0 & 82.4 & 89.6 & 90.2 & 86.6 \\
% \midrule
% Score     & 78.2 & 77.2 & 73.6 & 80.8 & 79.2 & 74.0 & 84.1 & 80.6 & 76.9 \\
% \bottomrule
% \end{tabular}
% \end{table}


\subsubsection{Per‑task Performance of LoRA Variants (Teacher)}
Table~\ref{tab:teacher-variants} presents per‑task GLUE scores expressed as percentages of the teacher’s full fine‑tuning (FFT) performance for seven LoRA variants, averaged across BERT, RoBERTa, and DeBERTa model families.

\begin{table}[ht]
\centering
\caption{Per‑task GLUE scores (percentage of FFT) for teacher LoRA variants, averaged across BERT, RoBERTa, and DeBERTa model families.}
\label{tab:teacher-variants}
\small
\begin{tabular}{lccccccccc}
\toprule
Method & CoLA & SST‑2 & MRPC & QQP & STS‑B & QNLI & RTE & WNLI & GLUE \\
\midrule
LoRA & 90.6\% & 99.4\% & 90.0\% & 97.2\% & 95.2\% & 99.6\% & 83.8\% & 109.1\% & 89.8\% \\
MR‑LoRA & 92.0\% & 100.1\% & 92.8\% & 97.7\% & 96.4\% & 99.8\% & 88.7\% & 109.7\% & 91.5\% \\
AdaLoRA & 1.1\% & 98.3\% & 83.4\% & 94.9\% & 32.1\% & 96.9\% & 78.5\% & 96.3\% & 79.1\% \\
DoRA & 90.9\% & 99.6\% & 90.6\% & 97.3\% & 86.3\% & 99.5\% & 84.3\% & 109.1\% & 89.4\% \\
OLoRA & 87.2\% & 99.7\% & 92.3\% & 97.3\% & 97.1\% & 99.6\% & 88.0\% & 109.1\% & 89.3\% \\
RS‑LoRA & 92.3\% & 99.7\% & 94.5\% & 97.9\% & 87.5\% & 99.7\% & 89.1\% & 110.0\% & 90.7\% \\
MR‑LoRA‑RS & 93.3\% & 99.9\% & 96.7\% & 97.9\% & 98.9\% & 100.3\% & 91.1\% & 109.4\% & 93.3\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Per‑task Performance of LoRA Variants (Student)}
Table~\ref{tab:student-variants} shows the corresponding percentages for student distillation. Per‑task data are available for all variants; missing entries have been computed.

\begin{table}[ht]
\centering
\caption{Per‑task GLUE scores (percentage of teacher FFT) for student LoRA variants (knowledge distillation), averaged across BERT, RoBERTa, and DeBERTa model families. Per‑task data are available for all variants; missing entries have been computed.}
\label{tab:student-variants}
\small
\begin{tabular}{lccccccccc}
\toprule
Method & CoLA & SST‑2 & MRPC & QQP & STS‑B & QNLI & RTE & WNLI & GLUE \\
\midrule
LoRA & 69.3\% & 97.3\% & 85.0\% & 93.7\% & 92.9\% & 95.3\% & 80.1\% & 100.8\% & 89.8\% \\
MR‑LoRA & 79.2\% & 98.4\% & 85.1\% & 96.1\% & 94.6\% & 96.2\% & 80.7\% & 97.9\% & 91.5\% \\
AdaLoRA & 12.7\% & 95.9\% & 84.0\% & 92.6\% & 42.0\% & 93.2\% & 79.3\% & 101.1\% & 77.1\% \\
DoRA & 69.1\% & 97.2\% & 85.0\% & 93.8\% & 88.2\% & 95.2\% & 79.9\% & 100.6\% & 87.3\% \\
OLoRA & 70.6\% & 96.9\% & 84.7\% & 94.1\% & 90.7\% & 95.5\% & 80.5\% & 100.8\% & 89.2\% \\
RS‑LoRA & 75.4\% & 97.6\% & 84.9\% & 94.2\% & 84.9\% & 96.3\% & 81.4\% & 103.1\% & 89.7\% \\
MR‑LoRA‑RS & 82.4\% & 98.1\% & 91.0\% & 96.8\% & 94.7\% & 97.3\% & 85.3\% & 100.5\% & 93.3\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Efficiency Metrics}
Table~\ref{tab:efficiency} compares the resource requirements of each method (measured at rank 8). All LoRA variants reduce trainable parameters by approximately 99.5\% relative to FFT. Memory reduction (74--74.3\%) and inference speedup (2.4--4.5$\times$) are averaged across BERT, RoBERTa, and DeBERTa model families for teacher fine‑tuning; student distillation values (where available) show higher memory reduction (~82\%) and speedup (4.0$\times$).

\begin{table}[ht]
\centering
\caption{Efficiency metrics for LoRA variants. Parameter reduction is relative to full fine‑tuning (FFT); each cell shows teacher fine‑tuning/student distillation values separated by a slash. Memory reduction and inference speedup are averaged across BERT, RoBERTa, and DeBERTa model families for teacher fine‑tuning; student distillation values are shown after the slash where available.}
\label{tab:efficiency}
\begin{tabular}{lccc}
\toprule
Method & Parameter Reduction & Memory Reduction & Inference Speedup \\
\midrule
LoRA & 99.6\%/99.5\% & 74.3\%/82.1\% & 3.7$\times$/4.0$\times$ \\
MR‑LoRA & 99.4\%/99.4\% & 74.0\%/82.0\% & 3.3$\times$/4.0$\times$ \\
AdaLoRA & 99.5\% & 74.2\% & 3.5$\times$ \\
DoRA & 99.6\% & 74.3\% & 2.9$\times$ \\
OLoRA & 99.6\% & 74.2\% & 4.0$\times$ \\
RS‑LoRA & 99.6\% & 74.3\% & 4.5$\times$ \\
MR‑LoRA‑RS & 99.4\% & 74.0\% & 2.4$\times$ \\
\bottomrule
\end{tabular}
\end{table}



\subsection{Analysis}
\textbf{Why does multi-rank help?} The exponential rank schedule allows the adapter to capture coarse corrections (high-rank blocks) early in training and later refine subtle mismatches (low-rank blocks). The learned weights $\lambda_i$ automatically shift capacity toward the needed granularity. This benefit extends beyond knowledge distillation: when applied directly to teacher fine-tuning, MR-LoRA retains 95.8--98.7\% of the teacher’s full fine-tuning performance across model families (Table~\ref{tab:glue-scores}), confirming that multi-rank decomposition generally enhances adapter expressiveness.

\textbf{Efficiency--accuracy trade-off.} Tables~\ref{tab:teacher-variants} and~\ref{tab:student-variants} show that MR-LoRA achieves the best accuracy per parameter among LoRA variants, with MR-LoRA-RS (our variant combined with rank stabilization) achieving even better performance. In teacher fine-tuning, MR-LoRA retains 89.9\% of FFT performance using only 0.56\% of FFT parameters, while in student distillation it retains 92.4\% of teacher performance with 0.56\% of parameters. The inference speedup (Table~\ref{tab:efficiency}) shows that RS‑LoRA achieves the highest speedup (4.5$\times$) for teacher fine‑tuning, while MR‑LoRA provides a competitive 3.3$\times$ speedup without sacrificing accuracy.

\textbf{Limitations.} MR-LoRA introduces a small overhead in forward-pass computation due to the sum of multiple low-rank matrices. However, this overhead is negligible compared to the speed-up gained from using a distilled student. The method has so far been tested only on encoder-only models; extending it to decoder-only LLMs is left for future work.

\section{Conclusion}
We have presented MR-LoRA, a multi-rank extension of low-rank adaptation that enriches the expressiveness of knowledge-distillation adapters without sacrificing parameter efficiency. By decomposing the adapter into a weighted sum of exponentially decaying rank matrices, MR-LoRA captures both coarse and fine discrepancies between teacher and student representations. Learnable scalar weights allow the model to dynamically allocate capacity across different granularities.

Experiments on the GLUE benchmark with three encoder-only model families show that student MR-LoRA retains 91--94\% of the teacher’s full fine-tuning performance (Table~\ref{tab:glue-scores}), demonstrating effective knowledge distillation with minimal parameter overhead. When applied directly to teacher fine-tuning, MR-LoRA retains 96--99\% of the teacher’s performance, confirming the general effectiveness of multi-rank decomposition. Compared to full fine-tuning, MR-LoRA reduces trainable parameters by 99.4\%, memory footprint by 82\% (student distillation), and accelerates inference by 3.3$\times$ (teacher fine‑tuning) to 4.0$\times$ (student distillation). These gains make MR-LoRA an attractive solution for deploying large language models in resource-constrained environments.

Limitations of the current work include its focus on encoder-only architectures and the small overhead of summing multiple low-rank matrices. Future directions include extending MR-LoRA to decoder-only LLMs, exploring different rank schedules (e.g., learned rather than exponential), and applying the multi-rank principle to other PEFT methods such as prefix-tuning or adapters.

\begin{ack}
The authors declare no competing interests.
\end{ack}

\bibliographystyle{abbrvnat}
\bibliography{reference}

\end{document}
