
@InProceedings{pmlr-v262-kdlora,
  title = 	 {{KD-LoRA}: A Hybrid Approach to Efficient Fine-Tuning with LoRA and Knowledge Distillation},
  author =       {Azimi, Rambod and Rishav, Rishav and Teichmann, Marek and Ebrahimi Kahou, Samira},
  booktitle = 	 {Proceedings of The 4th NeurIPS Efficient Natural Language and Speech Processing Workshop},
  pages = 	 {73--80},
  year = 	 {2024},
  editor = 	 {Rezagholizadeh, Mehdi and Passban, Peyman and Samiee, Soheila and Partovi Nia, Vahid and Cheng, Yu and Deng, Yue and Liu, Qun and Chen, Boxing},
  volume = 	 {262},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {14 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v262/main/assets/azimi24a/azimi24a.pdf},
  url = 	 {https://proceedings.mlr.press/v262/azimi24a.html}
}

@inproceedings{wang2018glue,
  author       = {Alex Wang and
                  Amanpreet Singh and
                  Julian Michael and
                  Felix Hill and
                  Omer Levy and
                  Samuel R. Bowman},
  title        = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
                  Language Understanding},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=rJ4km2R5t7},
  timestamp    = {Thu, 25 Jul 2019 14:25:46 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/WangSMHLB19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{buyukaz2024olora,
  author       = {Kerim B{\"{u}}y{\"{u}}kaky{\"{u}}z},
  title        = {OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2406.01775},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.01775},
  doi          = {10.48550/ARXIV.2406.01775},
  eprinttype    = {arXiv},
  eprint       = {2406.01775},
  timestamp    = {Thu, 04 Jul 2024 12:24:28 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-01775.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{kalajdzievski2023rslora,
  author       = {Damjan Kalajdzievski},
  title        = {A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA},
  journal      = {CoRR},
  volume       = {abs/2312.03732},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2312.03732},
  doi          = {10.48550/ARXIV.2312.03732},
  eprinttype    = {arXiv},
  eprint       = {2312.03732},
  timestamp    = {Mon, 01 Jan 2024 17:57:12 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2312-03732.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hu2022lora,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Lu Wang and
                  Weizhu Chen},
  title        = {LoRA: Low-Rank Adaptation of Large Language Models},
  booktitle    = {The Tenth International Conference on Learning Representations, {ICLR}
                  2022, Virtual Event, April 25-29, 2022},
  publisher    = {OpenReview.net},
  year         = {2022},
  url          = {https://openreview.net/forum?id=nZeVKeeFYf9},
  timestamp    = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/HuSWALWWC22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{llama31herdofmodels,
      title={The Llama 3 Herd of Models}, 
      author={Llama Team, Meta},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783},  
}

@article{hinton2015distilling,
  author       = {Geoffrey E. Hinton and
                  Oriol Vinyals and
                  Jeffrey Dean},
  title        = {Distilling the Knowledge in a Neural Network},
  journal      = {CoRR},
  volume       = {abs/1503.02531},
  year         = {2015},
  url          = {http://arxiv.org/abs/1503.02531},
  eprinttype    = {arXiv},
  eprint       = {1503.02531},
  timestamp    = {Mon, 13 Aug 2018 16:48:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/HintonVD15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{houlsby2019parameter,
  title = 	 {Parameter-Efficient Transfer Learning for {NLP}},
  author =       {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2790--2799},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/houlsby19a.html},
}

@article{sanh2019distilbert,
  author       = {Victor Sanh and
                  Lysandre Debut and
                  Julien Chaumond and
                  Thomas Wolf},
  title        = {DistilBERT, a distilled version of {BERT:} smaller, faster, cheaper
                  and lighter},
  journal      = {CoRR},
  volume       = {abs/1910.01108},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.01108},
  eprinttype    = {arXiv},
  eprint       = {1910.01108},
  timestamp    = {Tue, 02 Jun 2020 12:48:59 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-01108.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gu2024knowledge,
  author       = {Yuxian Gu and
                  Li Dong and
                  Furu Wei and
                  Minlie Huang},
  title        = {MiniLLM: Knowledge Distillation of Large Language Models},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=5h0qf7IBZZ},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/Gu0WH24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhang2023adalora,
  author       = {Qingru Zhang and
                  Minshuo Chen and
                  Alexander Bukharin and
                  Pengcheng He and
                  Yu Cheng and
                  Weizhu Chen and
                  Tuo Zhao},
  title        = {Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/forum?id=lq62uWRJjiY},
  timestamp    = {Wed, 24 Jul 2024 16:50:34 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ZhangCBH0CZ23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dettmers2023qlora,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023},
  url={https://arxiv.org/abs/2305.14314}
}

@inproceedings{liu2024dora,
author = {Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
title = {DoRA: weight-decomposed low-rank adaptation},
year = {2024},
publisher = {JMLR.org},
abstract = {Among the widely used parameter-efficient fine-tuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed Low-Rank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding. Code is available at https://github.com/NVlabs/DoRA.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {1299},
numpages = {22},
location = {Vienna, Austria},
series = {ICML'24}
}

@inproceedings{li-liang-2021-prefix,
  author       = {Xiang Lisa Li and
                  Percy Liang},
  editor       = {Chengqing Zong and
                  Fei Xia and
                  Wenjie Li and
                  Roberto Navigli},
  title        = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational
                  Linguistics and the 11th International Joint Conference on Natural
                  Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual
                  Event, August 1-6, 2021},
  pages        = {4582--4597},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  url          = {https://doi.org/10.18653/v1/2021.acl-long.353},
  doi          = {10.18653/V1/2021.ACL-LONG.353},
  timestamp    = {Wed, 16 Mar 2022 23:55:03 +0100},
  biburl       = {https://dblp.org/rec/conf/acl/LiL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}
@misc{team2023gemini,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Gemini Team, Google},
      year={2025},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11805}, 
}

@inproceedings{devlin-etal-2019-bert,
  author       = {Jacob Devlin and
                  Ming{-}Wei Chang and
                  Kenton Lee and
                  Kristina Toutanova},
  editor       = {Jill Burstein and
                  Christy Doran and
                  Thamar Solorio},
  title        = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                  Understanding},
  booktitle    = {Proceedings of the 2019 Conference of the North American Chapter of
                  the Association for Computational Linguistics: Human Language Technologies,
                  {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
                  and Short Papers)},
  pages        = {4171--4186},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/n19-1423},
  doi          = {10.18653/V1/N19-1423},
  timestamp    = {Mon, 26 Sep 2022 12:21:55 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{liu2019roberta,
  author       = {Yinhan Liu and
                  Myle Ott and
                  Naman Goyal and
                  Jingfei Du and
                  Mandar Joshi and
                  Danqi Chen and
                  Omer Levy and
                  Mike Lewis and
                  Luke Zettlemoyer and
                  Veselin Stoyanov},
  title        = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal      = {CoRR},
  volume       = {abs/1907.11692},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.11692},
  eprinttype    = {arXiv},
  eprint       = {1907.11692},
  timestamp    = {Tue, 11 Feb 2025 12:43:01 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{he2021debertav3,
  author       = {Pengcheng He and
                  Jianfeng Gao and
                  Weizhu Chen},
  title        = {DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with
                  Gradient-Disentangled Embedding Sharing},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/forum?id=sE7-XhLxHA},
  timestamp    = {Wed, 24 Jul 2024 16:50:33 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/HeGC23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
