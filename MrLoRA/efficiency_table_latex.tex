\begin{table}[ht]
\centering
\caption{Efficiency metrics for LoRA variants. Parameter reduction is relative to full fine‑tuning (FFT). Memory reduction and inference speedup are averaged across BERT, RoBERTa, and DeBERTa model families.}
\label{tab:efficiency}
\begin{tabular}{lccc}
\toprule
Method & Parameter Reduction (\% FFT) & Memory Reduction (\%) & Inference Speedup \\
\midrule
LoRA & 99.5% & -- & 1.0× \\
MR‑LoRA & 99.4% & -- & 1.1× \\
AdaLoRA & 99.5% & -- & 1.1× \\
DoRA & 99.5% & -- & 0.8× \\
OLoRA & 99.5% & -- & 1.2× \\
RS‑LoRA & 99.5% & -- & 1.5× \\
MR‑LoRA‑RS & 99.4% & -- & 1.3× \\
\bottomrule
\end{tabular}
\end{table}