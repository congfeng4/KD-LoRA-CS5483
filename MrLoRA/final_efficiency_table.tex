\begin{table}[ht]
\centering
\caption{Efficiency metrics for LoRA variants. Parameter reduction is relative to full fine‑tuning (FFT); each cell shows teacher fine‑tuning/student distillation values separated by a slash. Memory reduction and inference speedup are averaged across BERT, RoBERTa, and DeBERTa model families (only available for LoRA and MR‑LoRA).}
\label{tab:efficiency}
\begin{tabular}{lccc}
\toprule
Method & Parameter Reduction & Memory Reduction & Inference Speedup \\
\midrule
LoRA & 99.6%/99.5% & 74.3%/82.1% & 3.7×/4.0× \\
MR‑LoRA & 99.4%/99.4% & 74.0%/82.0% & 3.3×/4.0× \\
AdaLoRA & 99.5% & 74.2% & 3.5× \\
DoRA & 99.6% & 74.3% & 2.9× \\
OLoRA & 99.6% & 74.2% & 4.0× \\
RS‑LoRA & 99.6% & 74.3% & 4.5× \\
MR‑LoRA‑RS & 99.4% & 74.0% & 2.4× \\
\bottomrule
\end{tabular}
\end{table}