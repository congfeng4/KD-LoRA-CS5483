\begin{table}[ht]
\centering
\caption{Efficiency metrics for LoRA variants. Parameter reduction is relative to full fine‑tuning (FFT). Memory reduction and inference speedup are averaged across BERT, RoBERTa, and DeBERTa model families.}
\label{tab:efficiency-complete}
\begin{tabular}{lccc}
\toprule
Method & Parameter Reduction (\% FFT) & Memory Reduction (\%) & Inference Speedup \\
\midrule
LoRA & 99.5% & 82.0% & 4.2$\times$ \\
MR‑LoRA & 99.4% & 82.0% & 4.9$\times$ \\
AdaLoRA & 99.5% & -- & -- \\
DoRA & 99.5% & -- & -- \\
OLoRA & 99.5% & -- & -- \\
RS‑LoRA & 99.5% & -- & -- \\
MR‑LoRA‑RS & 99.4% & -- & -- \\
\bottomrule
\end{tabular}
\end{table}
