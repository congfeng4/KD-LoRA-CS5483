{
    "eval_loss": 2.8891727924346924,
    "eval_pearson": 0.8838618709577947,
    "eval_spearman": 0.8827014897685124,
    "eval_runtime": 0.3781,
    "eval_samples_per_second": 3967.325,
    "eval_steps_per_second": 31.739,
    "epoch": 80.0,
    "log_history": [
        {
            "loss": 9.0557,
            "grad_norm": 30.669921875,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 4.563745975494385,
            "eval_pearson": -0.1268256981048834,
            "eval_spearman": -0.1363894411043073,
            "eval_runtime": 0.3948,
            "eval_samples_per_second": 3799.092,
            "eval_steps_per_second": 30.393,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 2.7059,
            "grad_norm": 6.653481483459473,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.6467442512512207,
            "eval_pearson": 0.7349379271920807,
            "eval_spearman": 0.7427687999237758,
            "eval_runtime": 0.3933,
            "eval_samples_per_second": 3813.614,
            "eval_steps_per_second": 30.509,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 1.1867,
            "grad_norm": 6.785367965698242,
            "learning_rate": 9.62962962962963e-05,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 2.9390130043029785,
            "eval_pearson": 0.8266241623245978,
            "eval_spearman": 0.8330262139044664,
            "eval_runtime": 0.3747,
            "eval_samples_per_second": 4003.029,
            "eval_steps_per_second": 32.024,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.8817,
            "grad_norm": 3.6227989196777344,
            "learning_rate": 9.135802469135802e-05,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 3.0009570121765137,
            "eval_pearson": 0.8441064938628837,
            "eval_spearman": 0.845748664745163,
            "eval_runtime": 0.385,
            "eval_samples_per_second": 3896.321,
            "eval_steps_per_second": 31.171,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.7359,
            "grad_norm": 3.578143358230591,
            "learning_rate": 8.641975308641975e-05,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 3.0390625,
            "eval_pearson": 0.8576625716282268,
            "eval_spearman": 0.8579452781830887,
            "eval_runtime": 0.3866,
            "eval_samples_per_second": 3880.01,
            "eval_steps_per_second": 31.04,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.6613,
            "grad_norm": 3.417234182357788,
            "learning_rate": 8.148148148148148e-05,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 3.0392532348632812,
            "eval_pearson": 0.8634858237345608,
            "eval_spearman": 0.8636113575887358,
            "eval_runtime": 0.3043,
            "eval_samples_per_second": 4929.438,
            "eval_steps_per_second": 39.436,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.5997,
            "grad_norm": 2.996715545654297,
            "learning_rate": 7.65432098765432e-05,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 2.9666500091552734,
            "eval_pearson": 0.8695394796969992,
            "eval_spearman": 0.868865971788677,
            "eval_runtime": 0.3703,
            "eval_samples_per_second": 4050.957,
            "eval_steps_per_second": 32.408,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.5566,
            "grad_norm": 1.8148871660232544,
            "learning_rate": 7.160493827160494e-05,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 2.9284579753875732,
            "eval_pearson": 0.8723271461110278,
            "eval_spearman": 0.8716349554478945,
            "eval_runtime": 0.4111,
            "eval_samples_per_second": 3648.625,
            "eval_steps_per_second": 29.189,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.5309,
            "grad_norm": 2.022016763687134,
            "learning_rate": 6.666666666666667e-05,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 2.9410581588745117,
            "eval_pearson": 0.8727560614421535,
            "eval_spearman": 0.8724124387036781,
            "eval_runtime": 0.4353,
            "eval_samples_per_second": 3446.28,
            "eval_steps_per_second": 27.57,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.4939,
            "grad_norm": 3.4581165313720703,
            "learning_rate": 6.17283950617284e-05,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 2.921297550201416,
            "eval_pearson": 0.8774055411658034,
            "eval_spearman": 0.8764397431421282,
            "eval_runtime": 0.3849,
            "eval_samples_per_second": 3897.55,
            "eval_steps_per_second": 31.18,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.4792,
            "grad_norm": 2.3579702377319336,
            "learning_rate": 5.679012345679012e-05,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 2.9219183921813965,
            "eval_pearson": 0.8793514666153966,
            "eval_spearman": 0.8784762487919662,
            "eval_runtime": 0.3992,
            "eval_samples_per_second": 3757.852,
            "eval_steps_per_second": 30.063,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.4553,
            "grad_norm": 2.3382012844085693,
            "learning_rate": 5.185185185185185e-05,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 2.886901378631592,
            "eval_pearson": 0.8803824593389876,
            "eval_spearman": 0.8797304624627094,
            "eval_runtime": 0.3777,
            "eval_samples_per_second": 3970.956,
            "eval_steps_per_second": 31.768,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 0.4414,
            "grad_norm": 2.2704074382781982,
            "learning_rate": 4.691358024691358e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 2.880136728286743,
            "eval_pearson": 0.8817526749216495,
            "eval_spearman": 0.8813790767762314,
            "eval_runtime": 0.3883,
            "eval_samples_per_second": 3863.1,
            "eval_steps_per_second": 30.905,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "loss": 0.4299,
            "grad_norm": 3.157423973083496,
            "learning_rate": 4.197530864197531e-05,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 2.9195921421051025,
            "eval_pearson": 0.8821667182713225,
            "eval_spearman": 0.8815004762354129,
            "eval_runtime": 0.3906,
            "eval_samples_per_second": 3840.436,
            "eval_steps_per_second": 30.723,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "loss": 0.4128,
            "grad_norm": 2.8261523246765137,
            "learning_rate": 3.7037037037037037e-05,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 2.896920919418335,
            "eval_pearson": 0.8828224002083436,
            "eval_spearman": 0.8823016846170263,
            "eval_runtime": 0.3896,
            "eval_samples_per_second": 3849.803,
            "eval_steps_per_second": 30.798,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "loss": 0.4113,
            "grad_norm": 1.998903512954712,
            "learning_rate": 3.209876543209876e-05,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "eval_loss": 2.8916025161743164,
            "eval_pearson": 0.8837117221187756,
            "eval_spearman": 0.8826867897106174,
            "eval_runtime": 0.37,
            "eval_samples_per_second": 4053.765,
            "eval_steps_per_second": 32.43,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "loss": 0.3974,
            "grad_norm": 4.010342597961426,
            "learning_rate": 2.7160493827160493e-05,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "eval_loss": 2.894235610961914,
            "eval_pearson": 0.8832275556756171,
            "eval_spearman": 0.8821761036904241,
            "eval_runtime": 0.3702,
            "eval_samples_per_second": 4051.888,
            "eval_steps_per_second": 32.415,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "loss": 0.3896,
            "grad_norm": 2.8841426372528076,
            "learning_rate": 2.2222222222222223e-05,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "eval_loss": 2.8624660968780518,
            "eval_pearson": 0.8837417131666677,
            "eval_spearman": 0.8825940551063708,
            "eval_runtime": 0.3704,
            "eval_samples_per_second": 4049.447,
            "eval_steps_per_second": 32.396,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "train_runtime": 302.1976,
            "train_samples_per_second": 1902.397,
            "train_steps_per_second": 14.891,
            "total_flos": 1.5312997626937344e+16,
            "train_loss": 1.1569357172648111,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "eval_loss": 2.8891727924346924,
            "eval_pearson": 0.8838618709577947,
            "eval_spearman": 0.8827014897685124,
            "eval_runtime": 0.3781,
            "eval_samples_per_second": 3967.325,
            "eval_steps_per_second": 31.739,
            "epoch": 80.0,
            "step": 3600
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./ablation",
        "lora_dropout": 0.05,
        "use_rslora": false,
        "use_olora": true,
        "lora_alpha": 16,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "model_family": "deberta",
        "task": "stsb",
        "peft": "mrlora-olora",
        "seed": 42,
        "rank": 8,
        "student_model_name": "./models/deberta-v3-small",
        "teacher_model_name": "./models/deberta-v3-base",
        "use_lcoef": false,
        "use_bias": false,
        "train_size": 5749
    },
    "train": {
        "train_time": 302.1976,
        "trainable_params_count": 0.148225,
        "memory_allocated": [
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872,
            587.855872
        ],
        "memory_reserved": [
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248,
            2776.629248
        ]
    },
    "variant": "kd-lora"
}