{
    "eval_loss": 2.58606219291687,
    "eval_pearson": 0.8469513656121569,
    "eval_spearman": 0.8449374304665326,
    "eval_runtime": 0.2455,
    "eval_samples_per_second": 6110.292,
    "eval_steps_per_second": 48.882,
    "epoch": 53.333333333333336,
    "log_history": [
        {
            "loss": 4.8379,
            "grad_norm": 8.970252990722656,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 2.7434027194976807,
            "eval_pearson": 0.3812306570810681,
            "eval_spearman": 0.3586996346327264,
            "eval_runtime": 0.2403,
            "eval_samples_per_second": 6241.443,
            "eval_steps_per_second": 49.932,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 1.2609,
            "grad_norm": 12.108309745788574,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.526057243347168,
            "eval_pearson": 0.8027102850821285,
            "eval_spearman": 0.807800428530959,
            "eval_runtime": 0.2351,
            "eval_samples_per_second": 6380.153,
            "eval_steps_per_second": 51.041,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 0.8195,
            "grad_norm": 8.566435813903809,
            "learning_rate": 9.62962962962963e-05,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 2.592775344848633,
            "eval_pearson": 0.8373434097700208,
            "eval_spearman": 0.8354446930548923,
            "eval_runtime": 0.2477,
            "eval_samples_per_second": 6055.981,
            "eval_steps_per_second": 48.448,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.684,
            "grad_norm": 2.9969592094421387,
            "learning_rate": 9.135802469135802e-05,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 2.557440996170044,
            "eval_pearson": 0.8426064816233091,
            "eval_spearman": 0.8411941482304511,
            "eval_runtime": 0.2436,
            "eval_samples_per_second": 6158.163,
            "eval_steps_per_second": 49.265,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.5978,
            "grad_norm": 7.897100925445557,
            "learning_rate": 8.641975308641975e-05,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 2.634650468826294,
            "eval_pearson": 0.8485452352729145,
            "eval_spearman": 0.8463014137720426,
            "eval_runtime": 0.2355,
            "eval_samples_per_second": 6370.037,
            "eval_steps_per_second": 50.96,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.5356,
            "grad_norm": 4.36368989944458,
            "learning_rate": 8.148148148148148e-05,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 2.54864239692688,
            "eval_pearson": 0.848632667259253,
            "eval_spearman": 0.846623633052542,
            "eval_runtime": 0.2454,
            "eval_samples_per_second": 6112.655,
            "eval_steps_per_second": 48.901,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.4846,
            "grad_norm": 3.5627739429473877,
            "learning_rate": 7.65432098765432e-05,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 2.505678653717041,
            "eval_pearson": 0.8523881931939983,
            "eval_spearman": 0.8498128919221253,
            "eval_runtime": 0.2407,
            "eval_samples_per_second": 6231.034,
            "eval_steps_per_second": 49.848,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.4465,
            "grad_norm": 3.9850544929504395,
            "learning_rate": 7.160493827160494e-05,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 2.663616418838501,
            "eval_pearson": 0.8506585875942191,
            "eval_spearman": 0.8484217366846459,
            "eval_runtime": 0.2367,
            "eval_samples_per_second": 6337.006,
            "eval_steps_per_second": 50.696,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.4156,
            "grad_norm": 3.557784080505371,
            "learning_rate": 6.666666666666667e-05,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 2.691654682159424,
            "eval_pearson": 0.8485147853270882,
            "eval_spearman": 0.8463791361817761,
            "eval_runtime": 0.2371,
            "eval_samples_per_second": 6326.689,
            "eval_steps_per_second": 50.614,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.3821,
            "grad_norm": 3.3300418853759766,
            "learning_rate": 6.17283950617284e-05,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 2.6216936111450195,
            "eval_pearson": 0.84894916814576,
            "eval_spearman": 0.8469942840319674,
            "eval_runtime": 0.2382,
            "eval_samples_per_second": 6298.302,
            "eval_steps_per_second": 50.386,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.3628,
            "grad_norm": 3.0094687938690186,
            "learning_rate": 5.679012345679012e-05,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 2.6332366466522217,
            "eval_pearson": 0.8474288607196215,
            "eval_spearman": 0.8451136666756832,
            "eval_runtime": 0.2381,
            "eval_samples_per_second": 6300.377,
            "eval_steps_per_second": 50.403,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.336,
            "grad_norm": 3.4868056774139404,
            "learning_rate": 5.185185185185185e-05,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 2.641024589538574,
            "eval_pearson": 0.84669494274282,
            "eval_spearman": 0.8439574583216035,
            "eval_runtime": 0.2357,
            "eval_samples_per_second": 6364.147,
            "eval_steps_per_second": 50.913,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "train_runtime": 143.9352,
            "train_samples_per_second": 3994.159,
            "train_steps_per_second": 31.264,
            "total_flos": 1.0347640831082496e+16,
            "train_loss": 0.9302812480926513,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 2.58606219291687,
            "eval_pearson": 0.8469513656121569,
            "eval_spearman": 0.8449374304665326,
            "eval_runtime": 0.2455,
            "eval_samples_per_second": 6110.292,
            "eval_steps_per_second": 48.882,
            "epoch": 53.333333333333336,
            "step": 2400
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./ablation",
        "lora_dropout": 0.05,
        "use_rslora": true,
        "use_olora": true,
        "lora_alpha": 16,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "model_family": "bert",
        "task": "stsb",
        "peft": "mrlora-olora-rs-lcoef",
        "seed": 42,
        "rank": 8,
        "student_model_name": "./models/distilbert-base-uncased",
        "teacher_model_name": "./models/bert-base-uncased",
        "use_lcoef": true,
        "use_bias": false,
        "train_size": 5749
    },
    "train": {
        "train_time": 143.9352,
        "trainable_params_count": 0.738853,
        "memory_allocated": [
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328,
            296.931328
        ],
        "memory_reserved": [
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792,
            1459.617792
        ]
    },
    "variant": "kd-lora"
}