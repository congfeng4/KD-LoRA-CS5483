{
    "eval_loss": 2.782660722732544,
    "eval_pearson": 0.867491132220986,
    "eval_spearman": 0.8645420884623535,
    "eval_runtime": 0.2368,
    "eval_samples_per_second": 6333.587,
    "eval_steps_per_second": 50.669,
    "epoch": 88.88888888888889,
    "log_history": [
        {
            "loss": 6.2564,
            "grad_norm": 22.599271774291992,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 2.748851776123047,
            "eval_pearson": 0.034276081225033464,
            "eval_spearman": 0.01017444607848908,
            "eval_runtime": 0.2842,
            "eval_samples_per_second": 5277.102,
            "eval_steps_per_second": 42.217,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 2.1728,
            "grad_norm": 29.848304748535156,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.8561041355133057,
            "eval_pearson": 0.6089028714427459,
            "eval_spearman": 0.6046077658199147,
            "eval_runtime": 0.248,
            "eval_samples_per_second": 6047.988,
            "eval_steps_per_second": 48.384,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 1.1844,
            "grad_norm": 10.893285751342773,
            "learning_rate": 9.62962962962963e-05,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 2.607672929763794,
            "eval_pearson": 0.8233245572934244,
            "eval_spearman": 0.8243451407873595,
            "eval_runtime": 0.2853,
            "eval_samples_per_second": 5256.74,
            "eval_steps_per_second": 42.054,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 0.8966,
            "grad_norm": 17.481157302856445,
            "learning_rate": 9.135802469135802e-05,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 2.663208246231079,
            "eval_pearson": 0.8437137931211907,
            "eval_spearman": 0.842627379851813,
            "eval_runtime": 0.2641,
            "eval_samples_per_second": 5678.855,
            "eval_steps_per_second": 45.431,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 0.7715,
            "grad_norm": 7.361433506011963,
            "learning_rate": 8.641975308641975e-05,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 2.760801315307617,
            "eval_pearson": 0.8553717125136049,
            "eval_spearman": 0.8519585276193783,
            "eval_runtime": 0.2357,
            "eval_samples_per_second": 6362.905,
            "eval_steps_per_second": 50.903,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 0.6979,
            "grad_norm": 8.795745849609375,
            "learning_rate": 8.148148148148148e-05,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 2.706244707107544,
            "eval_pearson": 0.8579601906134093,
            "eval_spearman": 0.8553678819931202,
            "eval_runtime": 0.2571,
            "eval_samples_per_second": 5833.449,
            "eval_steps_per_second": 46.668,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 0.651,
            "grad_norm": 14.66522216796875,
            "learning_rate": 7.65432098765432e-05,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 2.669398069381714,
            "eval_pearson": 0.8599298792525997,
            "eval_spearman": 0.8571796335813968,
            "eval_runtime": 0.2934,
            "eval_samples_per_second": 5111.726,
            "eval_steps_per_second": 40.894,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 0.6172,
            "grad_norm": 6.530758380889893,
            "learning_rate": 7.160493827160494e-05,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 2.6591317653656006,
            "eval_pearson": 0.861847159185381,
            "eval_spearman": 0.8587768822676781,
            "eval_runtime": 0.2443,
            "eval_samples_per_second": 6139.929,
            "eval_steps_per_second": 49.119,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 0.5803,
            "grad_norm": 11.707730293273926,
            "learning_rate": 6.666666666666667e-05,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 2.798152446746826,
            "eval_pearson": 0.8649010167062292,
            "eval_spearman": 0.8618365738669341,
            "eval_runtime": 0.2476,
            "eval_samples_per_second": 6059.072,
            "eval_steps_per_second": 48.473,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 0.5516,
            "grad_norm": 6.696344375610352,
            "learning_rate": 6.17283950617284e-05,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 2.7277650833129883,
            "eval_pearson": 0.8661756885000854,
            "eval_spearman": 0.8630199373254859,
            "eval_runtime": 0.2451,
            "eval_samples_per_second": 6121.177,
            "eval_steps_per_second": 48.969,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 0.5405,
            "grad_norm": 18.5401554107666,
            "learning_rate": 5.679012345679012e-05,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 2.6430046558380127,
            "eval_pearson": 0.8665371960222978,
            "eval_spearman": 0.8636393378740292,
            "eval_runtime": 0.2431,
            "eval_samples_per_second": 6170.066,
            "eval_steps_per_second": 49.361,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 0.5301,
            "grad_norm": 9.400764465332031,
            "learning_rate": 5.185185185185185e-05,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 2.701054573059082,
            "eval_pearson": 0.8656536556839707,
            "eval_spearman": 0.862302590004652,
            "eval_runtime": 0.2384,
            "eval_samples_per_second": 6291.217,
            "eval_steps_per_second": 50.33,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 0.5055,
            "grad_norm": 14.575773239135742,
            "learning_rate": 4.691358024691358e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 2.7472786903381348,
            "eval_pearson": 0.8665245696658734,
            "eval_spearman": 0.8635656723372638,
            "eval_runtime": 0.2922,
            "eval_samples_per_second": 5133.506,
            "eval_steps_per_second": 41.068,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "loss": 0.4912,
            "grad_norm": 6.813906192779541,
            "learning_rate": 4.197530864197531e-05,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 2.693885326385498,
            "eval_pearson": 0.8676475477074004,
            "eval_spearman": 0.8641906197821252,
            "eval_runtime": 0.2423,
            "eval_samples_per_second": 6189.405,
            "eval_steps_per_second": 49.515,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "loss": 0.4824,
            "grad_norm": 7.613030433654785,
            "learning_rate": 3.7037037037037037e-05,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 2.7012245655059814,
            "eval_pearson": 0.8685543718484423,
            "eval_spearman": 0.8655809151274242,
            "eval_runtime": 0.2962,
            "eval_samples_per_second": 5064.708,
            "eval_steps_per_second": 40.518,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "loss": 0.472,
            "grad_norm": 6.7024712562561035,
            "learning_rate": 3.209876543209876e-05,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "eval_loss": 2.6972734928131104,
            "eval_pearson": 0.8672222838467162,
            "eval_spearman": 0.8642564557299783,
            "eval_runtime": 0.239,
            "eval_samples_per_second": 6275.967,
            "eval_steps_per_second": 50.208,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "loss": 0.4649,
            "grad_norm": 10.044836044311523,
            "learning_rate": 2.7160493827160493e-05,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "eval_loss": 2.626453161239624,
            "eval_pearson": 0.8677779024686089,
            "eval_spearman": 0.8640588218653719,
            "eval_runtime": 0.2432,
            "eval_samples_per_second": 6166.673,
            "eval_steps_per_second": 49.333,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "loss": 0.459,
            "grad_norm": 9.8627290725708,
            "learning_rate": 2.2222222222222223e-05,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "eval_loss": 2.6934072971343994,
            "eval_pearson": 0.8679237622597833,
            "eval_spearman": 0.8642961956279571,
            "eval_runtime": 0.2391,
            "eval_samples_per_second": 6272.732,
            "eval_steps_per_second": 50.182,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "loss": 0.4519,
            "grad_norm": 5.83811092376709,
            "learning_rate": 1.728395061728395e-05,
            "epoch": 84.44444444444444,
            "step": 3800
        },
        {
            "eval_loss": 2.7086305618286133,
            "eval_pearson": 0.867609875152434,
            "eval_spearman": 0.8646604451587464,
            "eval_runtime": 0.2904,
            "eval_samples_per_second": 5164.559,
            "eval_steps_per_second": 41.316,
            "epoch": 84.44444444444444,
            "step": 3800
        },
        {
            "loss": 0.4481,
            "grad_norm": 7.392370223999023,
            "learning_rate": 1.2345679012345678e-05,
            "epoch": 88.88888888888889,
            "step": 4000
        },
        {
            "eval_loss": 2.7167844772338867,
            "eval_pearson": 0.8678433689653405,
            "eval_spearman": 0.8646511730488462,
            "eval_runtime": 0.2375,
            "eval_samples_per_second": 6315.918,
            "eval_steps_per_second": 50.527,
            "epoch": 88.88888888888889,
            "step": 4000
        },
        {
            "train_runtime": 237.6967,
            "train_samples_per_second": 2418.628,
            "train_steps_per_second": 18.932,
            "total_flos": 1.724606805180416e+16,
            "train_loss": 0.9612627658843994,
            "epoch": 88.88888888888889,
            "step": 4000
        },
        {
            "eval_loss": 2.782660722732544,
            "eval_pearson": 0.867491132220986,
            "eval_spearman": 0.8645420884623535,
            "eval_runtime": 0.2368,
            "eval_samples_per_second": 6333.587,
            "eval_steps_per_second": 50.669,
            "epoch": 88.88888888888889,
            "step": 4000
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./ablation",
        "lora_dropout": 0.05,
        "use_rslora": false,
        "use_olora": true,
        "lora_alpha": 16,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "model_family": "roberta",
        "task": "stsb",
        "peft": "mrlora-olora",
        "seed": 42,
        "rank": 8,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "use_lcoef": false,
        "use_bias": false,
        "train_size": 5749
    },
    "train": {
        "train_time": 237.6967,
        "trainable_params_count": 0.738817,
        "memory_allocated": [
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584,
            358.723584
        ],
        "memory_reserved": [
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672,
            1333.788672
        ]
    },
    "variant": "kd-lora"
}