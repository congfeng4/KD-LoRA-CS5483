{
    "eval_loss": 2.0228190422058105,
    "eval_pearson": 0.44553295698724243,
    "eval_spearman": 0.4246535826872823,
    "eval_runtime": 0.4819,
    "eval_samples_per_second": 3112.852,
    "eval_steps_per_second": 24.903,
    "epoch": 100.0,
    "log_history": [
        {
            "loss": 4.9373,
            "grad_norm": 7.431033134460449,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 2.3633029460906982,
            "eval_pearson": -0.08614706942903698,
            "eval_spearman": -0.09867471738490709,
            "eval_runtime": 0.4944,
            "eval_samples_per_second": 3034.272,
            "eval_steps_per_second": 24.274,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 2.1796,
            "grad_norm": 8.350759506225586,
            "learning_rate": 0.00017777777777777779,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.34122371673584,
            "eval_pearson": 0.11563531496837853,
            "eval_spearman": 0.11053721794815478,
            "eval_runtime": 0.4785,
            "eval_samples_per_second": 3134.958,
            "eval_steps_per_second": 25.08,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 2.1099,
            "grad_norm": 1.6178977489471436,
            "learning_rate": 0.0001925925925925926,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 2.3410699367523193,
            "eval_pearson": 0.21414295835015035,
            "eval_spearman": 0.20948261594706324,
            "eval_runtime": 0.4761,
            "eval_samples_per_second": 3150.567,
            "eval_steps_per_second": 25.205,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 2.0749,
            "grad_norm": 5.005497932434082,
            "learning_rate": 0.00018271604938271605,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 2.363299608230591,
            "eval_pearson": 0.2571882618926071,
            "eval_spearman": 0.25489362830833184,
            "eval_runtime": 0.4592,
            "eval_samples_per_second": 3266.278,
            "eval_steps_per_second": 26.13,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 2.0526,
            "grad_norm": 4.708850860595703,
            "learning_rate": 0.0001728395061728395,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 2.2971863746643066,
            "eval_pearson": 0.29626184432282926,
            "eval_spearman": 0.2709919801283468,
            "eval_runtime": 0.4907,
            "eval_samples_per_second": 3056.596,
            "eval_steps_per_second": 24.453,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 2.0254,
            "grad_norm": 4.228166580200195,
            "learning_rate": 0.00016296296296296295,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 2.334095001220703,
            "eval_pearson": 0.30923480186139074,
            "eval_spearman": 0.28969960124566985,
            "eval_runtime": 0.4813,
            "eval_samples_per_second": 3116.557,
            "eval_steps_per_second": 24.932,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 2.019,
            "grad_norm": 7.674828052520752,
            "learning_rate": 0.0001530864197530864,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 2.2073915004730225,
            "eval_pearson": 0.335232593013,
            "eval_spearman": 0.31161830772117394,
            "eval_runtime": 0.4718,
            "eval_samples_per_second": 3179.334,
            "eval_steps_per_second": 25.435,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 2.0018,
            "grad_norm": 5.013833999633789,
            "learning_rate": 0.00014320987654320989,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 2.2577946186065674,
            "eval_pearson": 0.3466021086048986,
            "eval_spearman": 0.32024960935817376,
            "eval_runtime": 0.4865,
            "eval_samples_per_second": 3083.347,
            "eval_steps_per_second": 24.667,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 1.98,
            "grad_norm": 4.94942045211792,
            "learning_rate": 0.00013333333333333334,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 2.2156612873077393,
            "eval_pearson": 0.365500027720399,
            "eval_spearman": 0.34099313508806706,
            "eval_runtime": 0.4994,
            "eval_samples_per_second": 3003.817,
            "eval_steps_per_second": 24.031,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 1.9689,
            "grad_norm": 4.052741050720215,
            "learning_rate": 0.0001234567901234568,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 2.0523290634155273,
            "eval_pearson": 0.38418928177796796,
            "eval_spearman": 0.3548028191937486,
            "eval_runtime": 0.4532,
            "eval_samples_per_second": 3309.725,
            "eval_steps_per_second": 26.478,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 1.9497,
            "grad_norm": 1.8906034231185913,
            "learning_rate": 0.00011358024691358025,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 2.1057331562042236,
            "eval_pearson": 0.403176739130754,
            "eval_spearman": 0.37846719750391405,
            "eval_runtime": 0.4759,
            "eval_samples_per_second": 3151.982,
            "eval_steps_per_second": 25.216,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 1.9525,
            "grad_norm": 2.8679516315460205,
            "learning_rate": 0.0001037037037037037,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 2.158327341079712,
            "eval_pearson": 0.4139249742547436,
            "eval_spearman": 0.391947475050509,
            "eval_runtime": 0.4996,
            "eval_samples_per_second": 3002.311,
            "eval_steps_per_second": 24.018,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 1.9273,
            "grad_norm": 2.9677999019622803,
            "learning_rate": 9.382716049382717e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 2.1603636741638184,
            "eval_pearson": 0.4143784855084333,
            "eval_spearman": 0.38855512284920024,
            "eval_runtime": 0.5059,
            "eval_samples_per_second": 2964.991,
            "eval_steps_per_second": 23.72,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "loss": 1.9251,
            "grad_norm": 2.8533315658569336,
            "learning_rate": 8.395061728395062e-05,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 2.1564366817474365,
            "eval_pearson": 0.4202806740775749,
            "eval_spearman": 0.3964111773533454,
            "eval_runtime": 0.4795,
            "eval_samples_per_second": 3128.138,
            "eval_steps_per_second": 25.025,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "loss": 1.9015,
            "grad_norm": 2.328876495361328,
            "learning_rate": 7.407407407407407e-05,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 2.105203866958618,
            "eval_pearson": 0.42577400746662925,
            "eval_spearman": 0.4046578251228438,
            "eval_runtime": 0.508,
            "eval_samples_per_second": 2952.881,
            "eval_steps_per_second": 23.623,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "loss": 1.91,
            "grad_norm": 5.5144219398498535,
            "learning_rate": 6.419753086419753e-05,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "eval_loss": 2.0334670543670654,
            "eval_pearson": 0.4331673923335787,
            "eval_spearman": 0.41042631041373934,
            "eval_runtime": 0.4394,
            "eval_samples_per_second": 3413.626,
            "eval_steps_per_second": 27.309,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "loss": 1.8949,
            "grad_norm": 7.369036674499512,
            "learning_rate": 5.4320987654320986e-05,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "eval_loss": 2.08451771736145,
            "eval_pearson": 0.4381707832599667,
            "eval_spearman": 0.4153771517623821,
            "eval_runtime": 0.4805,
            "eval_samples_per_second": 3122.007,
            "eval_steps_per_second": 24.976,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "loss": 1.89,
            "grad_norm": 9.82292366027832,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "eval_loss": 2.094045400619507,
            "eval_pearson": 0.44073725039440215,
            "eval_spearman": 0.4195972837644114,
            "eval_runtime": 0.4717,
            "eval_samples_per_second": 3180.243,
            "eval_steps_per_second": 25.442,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "loss": 1.8872,
            "grad_norm": 3.210836410522461,
            "learning_rate": 3.45679012345679e-05,
            "epoch": 84.44444444444444,
            "step": 3800
        },
        {
            "eval_loss": 2.0674819946289062,
            "eval_pearson": 0.4407117328769309,
            "eval_spearman": 0.4177086431572563,
            "eval_runtime": 0.496,
            "eval_samples_per_second": 3024.407,
            "eval_steps_per_second": 24.195,
            "epoch": 84.44444444444444,
            "step": 3800
        },
        {
            "loss": 1.8954,
            "grad_norm": 2.0810341835021973,
            "learning_rate": 2.4691358024691357e-05,
            "epoch": 88.88888888888889,
            "step": 4000
        },
        {
            "eval_loss": 2.022991895675659,
            "eval_pearson": 0.4458893978680031,
            "eval_spearman": 0.425203304298578,
            "eval_runtime": 0.4992,
            "eval_samples_per_second": 3005.041,
            "eval_steps_per_second": 24.04,
            "epoch": 88.88888888888889,
            "step": 4000
        },
        {
            "loss": 1.8946,
            "grad_norm": 6.258578300476074,
            "learning_rate": 1.4814814814814815e-05,
            "epoch": 93.33333333333333,
            "step": 4200
        },
        {
            "eval_loss": 2.085947275161743,
            "eval_pearson": 0.4440900127506302,
            "eval_spearman": 0.423183783725929,
            "eval_runtime": 0.4975,
            "eval_samples_per_second": 3015.216,
            "eval_steps_per_second": 24.122,
            "epoch": 93.33333333333333,
            "step": 4200
        },
        {
            "loss": 1.876,
            "grad_norm": 4.390258312225342,
            "learning_rate": 4.938271604938272e-06,
            "epoch": 97.77777777777777,
            "step": 4400
        },
        {
            "eval_loss": 2.0480458736419678,
            "eval_pearson": 0.44555870837237627,
            "eval_spearman": 0.42457461419415543,
            "eval_runtime": 0.4454,
            "eval_samples_per_second": 3368.064,
            "eval_steps_per_second": 26.945,
            "epoch": 97.77777777777777,
            "step": 4400
        },
        {
            "train_runtime": 462.7414,
            "train_samples_per_second": 1242.379,
            "train_steps_per_second": 9.725,
            "total_flos": 3.839394645540864e+16,
            "train_loss": 2.0978324347601998,
            "epoch": 100.0,
            "step": 4500
        },
        {
            "eval_loss": 2.0228190422058105,
            "eval_pearson": 0.44553295698724243,
            "eval_spearman": 0.4246535826872823,
            "eval_runtime": 0.4819,
            "eval_samples_per_second": 3112.852,
            "eval_steps_per_second": 24.903,
            "epoch": 100.0,
            "step": 4500
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./ablation",
        "lora_dropout": 0.05,
        "use_rslora": true,
        "use_olora": true,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "model_family": "roberta",
        "task": "stsb",
        "peft": "mrlora-olora-rs-lcoef",
        "seed": 42,
        "rank": 8,
        "lora_alpha": 16,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "train_size": 5749,
        "use_lcoef": true,
        "use_bias": false
    },
    "train": {
        "train_time": 462.7414,
        "trainable_params_count": 0.591457,
        "memory_allocated": [
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504
        ],
        "memory_reserved": [
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552
        ]
    },
    "variant": "lora"
}