{
    "eval_loss": 1.5432952642440796,
    "eval_pearson": 0.6334879278827464,
    "eval_spearman": 0.6408512885361028,
    "eval_runtime": 0.4465,
    "eval_samples_per_second": 3359.742,
    "eval_steps_per_second": 26.878,
    "epoch": 97.77777777777777,
    "log_history": [
        {
            "loss": 4.952,
            "grad_norm": 7.138137340545654,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 2.315966844558716,
            "eval_pearson": 0.06132253136446816,
            "eval_spearman": 0.06243523417541399,
            "eval_runtime": 0.4707,
            "eval_samples_per_second": 3186.569,
            "eval_steps_per_second": 25.493,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 2.0844,
            "grad_norm": 7.001313209533691,
            "learning_rate": 0.00017777777777777779,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.2290380001068115,
            "eval_pearson": 0.3032227778475728,
            "eval_spearman": 0.29303541410371486,
            "eval_runtime": 0.5103,
            "eval_samples_per_second": 2939.64,
            "eval_steps_per_second": 23.517,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 1.9298,
            "grad_norm": 1.5248264074325562,
            "learning_rate": 0.0001925925925925926,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 2.0431413650512695,
            "eval_pearson": 0.4158647209580366,
            "eval_spearman": 0.39170326732424654,
            "eval_runtime": 0.4764,
            "eval_samples_per_second": 3148.912,
            "eval_steps_per_second": 25.191,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 1.8361,
            "grad_norm": 6.902829647064209,
            "learning_rate": 0.00018271604938271605,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 1.9486716985702515,
            "eval_pearson": 0.4889264282018555,
            "eval_spearman": 0.48008602685683405,
            "eval_runtime": 0.4997,
            "eval_samples_per_second": 3001.625,
            "eval_steps_per_second": 24.013,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 1.7641,
            "grad_norm": 2.992302179336548,
            "learning_rate": 0.0001728395061728395,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 1.9704170227050781,
            "eval_pearson": 0.5337804518867508,
            "eval_spearman": 0.5347724554161711,
            "eval_runtime": 0.5002,
            "eval_samples_per_second": 2999.076,
            "eval_steps_per_second": 23.993,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 1.7041,
            "grad_norm": 6.654784679412842,
            "learning_rate": 0.00016296296296296295,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 1.7603082656860352,
            "eval_pearson": 0.5660610386020368,
            "eval_spearman": 0.5729156957243752,
            "eval_runtime": 0.526,
            "eval_samples_per_second": 2851.791,
            "eval_steps_per_second": 22.814,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 1.6861,
            "grad_norm": 9.32281494140625,
            "learning_rate": 0.0001530864197530864,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 1.7063074111938477,
            "eval_pearson": 0.5877491777392124,
            "eval_spearman": 0.5953038967451308,
            "eval_runtime": 0.5251,
            "eval_samples_per_second": 2856.669,
            "eval_steps_per_second": 22.853,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 1.6463,
            "grad_norm": 4.609157562255859,
            "learning_rate": 0.00014320987654320989,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 1.6994303464889526,
            "eval_pearson": 0.5944692410674096,
            "eval_spearman": 0.5997291288838372,
            "eval_runtime": 0.5116,
            "eval_samples_per_second": 2932.005,
            "eval_steps_per_second": 23.456,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 1.6441,
            "grad_norm": 3.973710536956787,
            "learning_rate": 0.00013333333333333334,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 1.7316559553146362,
            "eval_pearson": 0.594754628530867,
            "eval_spearman": 0.6005692323171806,
            "eval_runtime": 0.5068,
            "eval_samples_per_second": 2959.865,
            "eval_steps_per_second": 23.679,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 1.6263,
            "grad_norm": 1.4938808679580688,
            "learning_rate": 0.0001234567901234568,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 1.6386598348617554,
            "eval_pearson": 0.6052238662006819,
            "eval_spearman": 0.6105164090527501,
            "eval_runtime": 0.4845,
            "eval_samples_per_second": 3096.262,
            "eval_steps_per_second": 24.77,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 1.6167,
            "grad_norm": 1.870405912399292,
            "learning_rate": 0.00011358024691358025,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 1.586822271347046,
            "eval_pearson": 0.6172528195579752,
            "eval_spearman": 0.6255590007703509,
            "eval_runtime": 0.5341,
            "eval_samples_per_second": 2808.554,
            "eval_steps_per_second": 22.468,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 1.6176,
            "grad_norm": 2.9876396656036377,
            "learning_rate": 0.0001037037037037037,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 1.587450385093689,
            "eval_pearson": 0.6230119585693127,
            "eval_spearman": 0.6317344376932238,
            "eval_runtime": 0.4875,
            "eval_samples_per_second": 3077.143,
            "eval_steps_per_second": 24.617,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 1.6022,
            "grad_norm": 4.96532678604126,
            "learning_rate": 9.382716049382717e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 1.551756501197815,
            "eval_pearson": 0.6225010576891253,
            "eval_spearman": 0.6297821345447869,
            "eval_runtime": 0.5081,
            "eval_samples_per_second": 2952.237,
            "eval_steps_per_second": 23.618,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "loss": 1.589,
            "grad_norm": 3.5837650299072266,
            "learning_rate": 8.395061728395062e-05,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 1.5593786239624023,
            "eval_pearson": 0.6231897372013958,
            "eval_spearman": 0.6292580285680538,
            "eval_runtime": 0.5137,
            "eval_samples_per_second": 2920.187,
            "eval_steps_per_second": 23.361,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "loss": 1.5856,
            "grad_norm": 3.5335872173309326,
            "learning_rate": 7.407407407407407e-05,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 1.543402075767517,
            "eval_pearson": 0.6257125236258995,
            "eval_spearman": 0.6322802635123139,
            "eval_runtime": 0.5086,
            "eval_samples_per_second": 2949.437,
            "eval_steps_per_second": 23.595,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "loss": 1.5924,
            "grad_norm": 5.475862979888916,
            "learning_rate": 6.419753086419753e-05,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "eval_loss": 1.5127434730529785,
            "eval_pearson": 0.628643210730948,
            "eval_spearman": 0.636110949770504,
            "eval_runtime": 0.5119,
            "eval_samples_per_second": 2930.024,
            "eval_steps_per_second": 23.44,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "loss": 1.5867,
            "grad_norm": 7.658061504364014,
            "learning_rate": 5.4320987654320986e-05,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "eval_loss": 1.5807133913040161,
            "eval_pearson": 0.6326388071814449,
            "eval_spearman": 0.6403683531590534,
            "eval_runtime": 0.5019,
            "eval_samples_per_second": 2988.476,
            "eval_steps_per_second": 23.908,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "loss": 1.5748,
            "grad_norm": 6.4410834312438965,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "eval_loss": 1.5891737937927246,
            "eval_pearson": 0.6295013172449322,
            "eval_spearman": 0.6373141936346479,
            "eval_runtime": 0.53,
            "eval_samples_per_second": 2829.998,
            "eval_steps_per_second": 22.64,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "loss": 1.576,
            "grad_norm": 4.526229381561279,
            "learning_rate": 3.45679012345679e-05,
            "epoch": 84.44444444444444,
            "step": 3800
        },
        {
            "eval_loss": 1.5946146249771118,
            "eval_pearson": 0.6305630028704472,
            "eval_spearman": 0.6379445373004979,
            "eval_runtime": 0.4561,
            "eval_samples_per_second": 3288.472,
            "eval_steps_per_second": 26.308,
            "epoch": 84.44444444444444,
            "step": 3800
        },
        {
            "loss": 1.5724,
            "grad_norm": 4.021644592285156,
            "learning_rate": 2.4691358024691357e-05,
            "epoch": 88.88888888888889,
            "step": 4000
        },
        {
            "eval_loss": 1.5484249591827393,
            "eval_pearson": 0.6323865631600701,
            "eval_spearman": 0.6399559359148544,
            "eval_runtime": 0.5164,
            "eval_samples_per_second": 2904.558,
            "eval_steps_per_second": 23.236,
            "epoch": 88.88888888888889,
            "step": 4000
        },
        {
            "loss": 1.583,
            "grad_norm": 3.11698842048645,
            "learning_rate": 1.4814814814814815e-05,
            "epoch": 93.33333333333333,
            "step": 4200
        },
        {
            "eval_loss": 1.5547996759414673,
            "eval_pearson": 0.6328806892847163,
            "eval_spearman": 0.640383476478758,
            "eval_runtime": 0.5299,
            "eval_samples_per_second": 2830.813,
            "eval_steps_per_second": 22.647,
            "epoch": 93.33333333333333,
            "step": 4200
        },
        {
            "loss": 1.5697,
            "grad_norm": 4.913952827453613,
            "learning_rate": 4.938271604938272e-06,
            "epoch": 97.77777777777777,
            "step": 4400
        },
        {
            "eval_loss": 1.5432952642440796,
            "eval_pearson": 0.6334879278827464,
            "eval_spearman": 0.6408512885361028,
            "eval_runtime": 0.4819,
            "eval_samples_per_second": 3112.755,
            "eval_steps_per_second": 24.902,
            "epoch": 97.77777777777777,
            "step": 4400
        },
        {
            "train_runtime": 443.6615,
            "train_samples_per_second": 1295.808,
            "train_steps_per_second": 10.143,
            "total_flos": 3.754074764528845e+16,
            "train_loss": 1.8154366996071556,
            "epoch": 97.77777777777777,
            "step": 4400
        },
        {
            "eval_loss": 1.5432952642440796,
            "eval_pearson": 0.6334879278827464,
            "eval_spearman": 0.6408512885361028,
            "eval_runtime": 0.4465,
            "eval_samples_per_second": 3359.742,
            "eval_steps_per_second": 26.878,
            "epoch": 97.77777777777777,
            "step": 4400
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./ablation",
        "lora_dropout": 0.05,
        "use_rslora": false,
        "use_olora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "model_family": "roberta",
        "task": "stsb",
        "peft": "mrlora",
        "seed": 42,
        "rank": 8,
        "lora_alpha": 16,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "train_size": 5749,
        "use_lcoef": false,
        "use_bias": false
    },
    "train": {
        "train_time": 443.6615,
        "trainable_params_count": 0.591457,
        "memory_allocated": [
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504,
            528.21504
        ],
        "memory_reserved": [
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552
        ]
    },
    "variant": "lora"
}