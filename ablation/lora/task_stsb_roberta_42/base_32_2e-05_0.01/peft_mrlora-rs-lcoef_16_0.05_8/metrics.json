{
    "eval_loss": 1.5432952642440796,
    "eval_pearson": 0.6334879278827464,
    "eval_spearman": 0.6408512885361028,
    "eval_runtime": 0.4937,
    "eval_samples_per_second": 3038.27,
    "eval_steps_per_second": 24.306,
    "epoch": 97.77777777777777,
    "log_history": [
        {
            "loss": 4.952,
            "grad_norm": 7.138137340545654,
            "learning_rate": 8.888888888888889e-05,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "eval_loss": 2.315966844558716,
            "eval_pearson": 0.06132253136446816,
            "eval_spearman": 0.06243523417541399,
            "eval_runtime": 0.4783,
            "eval_samples_per_second": 3136.063,
            "eval_steps_per_second": 25.089,
            "epoch": 4.444444444444445,
            "step": 200
        },
        {
            "loss": 2.0844,
            "grad_norm": 7.001313209533691,
            "learning_rate": 0.00017777777777777779,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "eval_loss": 2.2290380001068115,
            "eval_pearson": 0.3032227778475728,
            "eval_spearman": 0.29303541410371486,
            "eval_runtime": 0.5118,
            "eval_samples_per_second": 2930.648,
            "eval_steps_per_second": 23.445,
            "epoch": 8.88888888888889,
            "step": 400
        },
        {
            "loss": 1.9298,
            "grad_norm": 1.5248264074325562,
            "learning_rate": 0.0001925925925925926,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "eval_loss": 2.0431413650512695,
            "eval_pearson": 0.4158647209580366,
            "eval_spearman": 0.39170326732424654,
            "eval_runtime": 0.5387,
            "eval_samples_per_second": 2784.637,
            "eval_steps_per_second": 22.277,
            "epoch": 13.333333333333334,
            "step": 600
        },
        {
            "loss": 1.8361,
            "grad_norm": 6.902829647064209,
            "learning_rate": 0.00018271604938271605,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "eval_loss": 1.9486716985702515,
            "eval_pearson": 0.4889264282018555,
            "eval_spearman": 0.48008602685683405,
            "eval_runtime": 0.4785,
            "eval_samples_per_second": 3134.656,
            "eval_steps_per_second": 25.077,
            "epoch": 17.77777777777778,
            "step": 800
        },
        {
            "loss": 1.7641,
            "grad_norm": 2.992302179336548,
            "learning_rate": 0.0001728395061728395,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "eval_loss": 1.9704170227050781,
            "eval_pearson": 0.5337804518867508,
            "eval_spearman": 0.5347724554161711,
            "eval_runtime": 0.5012,
            "eval_samples_per_second": 2993.102,
            "eval_steps_per_second": 23.945,
            "epoch": 22.22222222222222,
            "step": 1000
        },
        {
            "loss": 1.7041,
            "grad_norm": 6.654784679412842,
            "learning_rate": 0.00016296296296296295,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "eval_loss": 1.7603082656860352,
            "eval_pearson": 0.5660610386020368,
            "eval_spearman": 0.5729156957243752,
            "eval_runtime": 0.5006,
            "eval_samples_per_second": 2996.461,
            "eval_steps_per_second": 23.972,
            "epoch": 26.666666666666668,
            "step": 1200
        },
        {
            "loss": 1.6861,
            "grad_norm": 9.32281494140625,
            "learning_rate": 0.0001530864197530864,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "eval_loss": 1.7063074111938477,
            "eval_pearson": 0.5877491777392124,
            "eval_spearman": 0.5953038967451308,
            "eval_runtime": 0.4761,
            "eval_samples_per_second": 3150.274,
            "eval_steps_per_second": 25.202,
            "epoch": 31.11111111111111,
            "step": 1400
        },
        {
            "loss": 1.6463,
            "grad_norm": 4.609157562255859,
            "learning_rate": 0.00014320987654320989,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "eval_loss": 1.6994303464889526,
            "eval_pearson": 0.5944692410674096,
            "eval_spearman": 0.5997291288838372,
            "eval_runtime": 0.5001,
            "eval_samples_per_second": 2999.233,
            "eval_steps_per_second": 23.994,
            "epoch": 35.55555555555556,
            "step": 1600
        },
        {
            "loss": 1.6441,
            "grad_norm": 3.973710536956787,
            "learning_rate": 0.00013333333333333334,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "eval_loss": 1.7316559553146362,
            "eval_pearson": 0.594754628530867,
            "eval_spearman": 0.6005692323171806,
            "eval_runtime": 0.4887,
            "eval_samples_per_second": 3069.418,
            "eval_steps_per_second": 24.555,
            "epoch": 40.0,
            "step": 1800
        },
        {
            "loss": 1.6263,
            "grad_norm": 1.4938808679580688,
            "learning_rate": 0.0001234567901234568,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "eval_loss": 1.6386598348617554,
            "eval_pearson": 0.6052238662006819,
            "eval_spearman": 0.6105164090527501,
            "eval_runtime": 0.5012,
            "eval_samples_per_second": 2992.722,
            "eval_steps_per_second": 23.942,
            "epoch": 44.44444444444444,
            "step": 2000
        },
        {
            "loss": 1.6167,
            "grad_norm": 1.870405912399292,
            "learning_rate": 0.00011358024691358025,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "eval_loss": 1.586822271347046,
            "eval_pearson": 0.6172528195579752,
            "eval_spearman": 0.6255590007703509,
            "eval_runtime": 0.4906,
            "eval_samples_per_second": 3057.226,
            "eval_steps_per_second": 24.458,
            "epoch": 48.888888888888886,
            "step": 2200
        },
        {
            "loss": 1.6176,
            "grad_norm": 2.9876396656036377,
            "learning_rate": 0.0001037037037037037,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "eval_loss": 1.587450385093689,
            "eval_pearson": 0.6230119585693127,
            "eval_spearman": 0.6317344376932238,
            "eval_runtime": 0.4516,
            "eval_samples_per_second": 3321.614,
            "eval_steps_per_second": 26.573,
            "epoch": 53.333333333333336,
            "step": 2400
        },
        {
            "loss": 1.6022,
            "grad_norm": 4.96532678604126,
            "learning_rate": 9.382716049382717e-05,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "eval_loss": 1.551756501197815,
            "eval_pearson": 0.6225010576891253,
            "eval_spearman": 0.6297821345447869,
            "eval_runtime": 0.4657,
            "eval_samples_per_second": 3221.026,
            "eval_steps_per_second": 25.768,
            "epoch": 57.77777777777778,
            "step": 2600
        },
        {
            "loss": 1.589,
            "grad_norm": 3.5837650299072266,
            "learning_rate": 8.395061728395062e-05,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "eval_loss": 1.5593786239624023,
            "eval_pearson": 0.6231897372013958,
            "eval_spearman": 0.6292580285680538,
            "eval_runtime": 0.5441,
            "eval_samples_per_second": 2756.87,
            "eval_steps_per_second": 22.055,
            "epoch": 62.22222222222222,
            "step": 2800
        },
        {
            "loss": 1.5856,
            "grad_norm": 3.5335872173309326,
            "learning_rate": 7.407407407407407e-05,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "eval_loss": 1.543402075767517,
            "eval_pearson": 0.6257125236258995,
            "eval_spearman": 0.6322802635123139,
            "eval_runtime": 0.4909,
            "eval_samples_per_second": 3055.717,
            "eval_steps_per_second": 24.446,
            "epoch": 66.66666666666667,
            "step": 3000
        },
        {
            "loss": 1.5924,
            "grad_norm": 5.475862979888916,
            "learning_rate": 6.419753086419753e-05,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "eval_loss": 1.5127434730529785,
            "eval_pearson": 0.628643210730948,
            "eval_spearman": 0.636110949770504,
            "eval_runtime": 0.4806,
            "eval_samples_per_second": 3120.87,
            "eval_steps_per_second": 24.967,
            "epoch": 71.11111111111111,
            "step": 3200
        },
        {
            "loss": 1.5867,
            "grad_norm": 7.658061504364014,
            "learning_rate": 5.4320987654320986e-05,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "eval_loss": 1.5807133913040161,
            "eval_pearson": 0.6326388071814449,
            "eval_spearman": 0.6403683531590534,
            "eval_runtime": 0.5129,
            "eval_samples_per_second": 2924.509,
            "eval_steps_per_second": 23.396,
            "epoch": 75.55555555555556,
            "step": 3400
        },
        {
            "loss": 1.5748,
            "grad_norm": 6.4410834312438965,
            "learning_rate": 4.4444444444444447e-05,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "eval_loss": 1.5891737937927246,
            "eval_pearson": 0.6295013172449322,
            "eval_spearman": 0.6373141936346479,
            "eval_runtime": 0.51,
            "eval_samples_per_second": 2941.241,
            "eval_steps_per_second": 23.53,
            "epoch": 80.0,
            "step": 3600
        },
        {
            "loss": 1.576,
            "grad_norm": 4.526229381561279,
            "learning_rate": 3.45679012345679e-05,
            "epoch": 84.44444444444444,
            "step": 3800
        },
        {
            "eval_loss": 1.5946146249771118,
            "eval_pearson": 0.6305630028704472,
            "eval_spearman": 0.6379445373004979,
            "eval_runtime": 0.4472,
            "eval_samples_per_second": 3354.377,
            "eval_steps_per_second": 26.835,
            "epoch": 84.44444444444444,
            "step": 3800
        },
        {
            "loss": 1.5724,
            "grad_norm": 4.021644592285156,
            "learning_rate": 2.4691358024691357e-05,
            "epoch": 88.88888888888889,
            "step": 4000
        },
        {
            "eval_loss": 1.5484249591827393,
            "eval_pearson": 0.6323865631600701,
            "eval_spearman": 0.6399559359148544,
            "eval_runtime": 0.4837,
            "eval_samples_per_second": 3101.014,
            "eval_steps_per_second": 24.808,
            "epoch": 88.88888888888889,
            "step": 4000
        },
        {
            "loss": 1.583,
            "grad_norm": 3.11698842048645,
            "learning_rate": 1.4814814814814815e-05,
            "epoch": 93.33333333333333,
            "step": 4200
        },
        {
            "eval_loss": 1.5547996759414673,
            "eval_pearson": 0.6328806892847163,
            "eval_spearman": 0.640383476478758,
            "eval_runtime": 0.4805,
            "eval_samples_per_second": 3121.895,
            "eval_steps_per_second": 24.975,
            "epoch": 93.33333333333333,
            "step": 4200
        },
        {
            "loss": 1.5697,
            "grad_norm": 4.913952827453613,
            "learning_rate": 4.938271604938272e-06,
            "epoch": 97.77777777777777,
            "step": 4400
        },
        {
            "eval_loss": 1.5432952642440796,
            "eval_pearson": 0.6334879278827464,
            "eval_spearman": 0.6408512885361028,
            "eval_runtime": 0.4949,
            "eval_samples_per_second": 3030.628,
            "eval_steps_per_second": 24.245,
            "epoch": 97.77777777777777,
            "step": 4400
        },
        {
            "train_runtime": 452.7503,
            "train_samples_per_second": 1269.795,
            "train_steps_per_second": 9.939,
            "total_flos": 3.754074764528845e+16,
            "train_loss": 1.8154366996071556,
            "epoch": 97.77777777777777,
            "step": 4400
        },
        {
            "eval_loss": 1.5432952642440796,
            "eval_pearson": 0.6334879278827464,
            "eval_spearman": 0.6408512885361028,
            "eval_runtime": 0.4937,
            "eval_samples_per_second": 3038.27,
            "eval_steps_per_second": 24.306,
            "epoch": 97.77777777777777,
            "step": 4400
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./ablation",
        "lora_dropout": 0.05,
        "use_rslora": true,
        "use_olora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "model_family": "roberta",
        "task": "stsb",
        "peft": "mrlora-rs-lcoef",
        "seed": 42,
        "rank": 8,
        "lora_alpha": 16,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "train_size": 5749,
        "use_lcoef": true,
        "use_bias": false
    },
    "train": {
        "train_time": 452.7503,
        "trainable_params_count": 0.591457,
        "memory_allocated": [
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608,
            527.428608
        ],
        "memory_reserved": [
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552,
            2518.679552
        ]
    },
    "variant": "lora"
}