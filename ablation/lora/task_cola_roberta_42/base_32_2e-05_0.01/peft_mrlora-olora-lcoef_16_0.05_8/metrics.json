{
    "eval_loss": 0.509477436542511,
    "eval_matthews_correlation": 0.6036344190543846,
    "eval_runtime": 0.3657,
    "eval_samples_per_second": 2851.709,
    "eval_steps_per_second": 24.607,
    "epoch": 35.82089552238806,
    "log_history": [
        {
            "loss": 0.6451,
            "grad_norm": 9.066981315612793,
            "learning_rate": 5.970149253731343e-05,
            "epoch": 2.9850746268656714,
            "step": 200
        },
        {
            "eval_loss": 0.6185586452484131,
            "eval_matthews_correlation": 0.0,
            "eval_runtime": 0.3783,
            "eval_samples_per_second": 2757.045,
            "eval_steps_per_second": 23.79,
            "epoch": 2.9850746268656714,
            "step": 200
        },
        {
            "loss": 0.5916,
            "grad_norm": 7.3033013343811035,
            "learning_rate": 0.00011940298507462686,
            "epoch": 5.970149253731344,
            "step": 400
        },
        {
            "eval_loss": 0.5649318099021912,
            "eval_matthews_correlation": 0.2593416356162653,
            "eval_runtime": 0.3461,
            "eval_samples_per_second": 3013.222,
            "eval_steps_per_second": 26.001,
            "epoch": 5.970149253731344,
            "step": 400
        },
        {
            "loss": 0.4687,
            "grad_norm": 6.3693623542785645,
            "learning_rate": 0.0001791044776119403,
            "epoch": 8.955223880597014,
            "step": 600
        },
        {
            "eval_loss": 0.5188686847686768,
            "eval_matthews_correlation": 0.4894438592062726,
            "eval_runtime": 0.3514,
            "eval_samples_per_second": 2967.736,
            "eval_steps_per_second": 25.608,
            "epoch": 8.955223880597014,
            "step": 600
        },
        {
            "loss": 0.3903,
            "grad_norm": 6.231255531311035,
            "learning_rate": 0.00019568822553897182,
            "epoch": 11.940298507462687,
            "step": 800
        },
        {
            "eval_loss": 0.44661110639572144,
            "eval_matthews_correlation": 0.5416549448511832,
            "eval_runtime": 0.3571,
            "eval_samples_per_second": 2920.544,
            "eval_steps_per_second": 25.201,
            "epoch": 11.940298507462687,
            "step": 800
        },
        {
            "loss": 0.3326,
            "grad_norm": 6.02459192276001,
            "learning_rate": 0.00018905472636815922,
            "epoch": 14.925373134328359,
            "step": 1000
        },
        {
            "eval_loss": 0.45210424065589905,
            "eval_matthews_correlation": 0.567550266689718,
            "eval_runtime": 0.3502,
            "eval_samples_per_second": 2978.615,
            "eval_steps_per_second": 25.702,
            "epoch": 14.925373134328359,
            "step": 1000
        },
        {
            "loss": 0.2869,
            "grad_norm": 7.571832180023193,
            "learning_rate": 0.0001824212271973466,
            "epoch": 17.91044776119403,
            "step": 1200
        },
        {
            "eval_loss": 0.5001892447471619,
            "eval_matthews_correlation": 0.5628601575342307,
            "eval_runtime": 0.3402,
            "eval_samples_per_second": 3066.016,
            "eval_steps_per_second": 26.457,
            "epoch": 17.91044776119403,
            "step": 1200
        },
        {
            "loss": 0.2523,
            "grad_norm": 5.3437819480896,
            "learning_rate": 0.000175787728026534,
            "epoch": 20.895522388059703,
            "step": 1400
        },
        {
            "eval_loss": 0.4187857210636139,
            "eval_matthews_correlation": 0.6285125993469022,
            "eval_runtime": 0.34,
            "eval_samples_per_second": 3067.2,
            "eval_steps_per_second": 26.467,
            "epoch": 20.895522388059703,
            "step": 1400
        },
        {
            "loss": 0.2178,
            "grad_norm": 9.856614112854004,
            "learning_rate": 0.0001691542288557214,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "eval_loss": 0.5920054316520691,
            "eval_matthews_correlation": 0.5705518999283685,
            "eval_runtime": 0.3949,
            "eval_samples_per_second": 2641.015,
            "eval_steps_per_second": 22.789,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "loss": 0.1899,
            "grad_norm": 6.833392143249512,
            "learning_rate": 0.00016252072968490878,
            "epoch": 26.865671641791046,
            "step": 1800
        },
        {
            "eval_loss": 0.5660707354545593,
            "eval_matthews_correlation": 0.5932805322494611,
            "eval_runtime": 0.376,
            "eval_samples_per_second": 2773.961,
            "eval_steps_per_second": 23.936,
            "epoch": 26.865671641791046,
            "step": 1800
        },
        {
            "loss": 0.1739,
            "grad_norm": 10.509747505187988,
            "learning_rate": 0.00015588723051409618,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "eval_loss": 0.6378549337387085,
            "eval_matthews_correlation": 0.605784652517054,
            "eval_runtime": 0.3156,
            "eval_samples_per_second": 3305.064,
            "eval_steps_per_second": 28.519,
            "epoch": 29.850746268656717,
            "step": 2000
        },
        {
            "loss": 0.1522,
            "grad_norm": 8.152183532714844,
            "learning_rate": 0.0001492537313432836,
            "epoch": 32.83582089552239,
            "step": 2200
        },
        {
            "eval_loss": 0.7043634057044983,
            "eval_matthews_correlation": 0.5931314229261737,
            "eval_runtime": 0.4237,
            "eval_samples_per_second": 2461.614,
            "eval_steps_per_second": 21.241,
            "epoch": 32.83582089552239,
            "step": 2200
        },
        {
            "loss": 0.1405,
            "grad_norm": 12.501526832580566,
            "learning_rate": 0.000142620232172471,
            "epoch": 35.82089552238806,
            "step": 2400
        },
        {
            "eval_loss": 0.5807623863220215,
            "eval_matthews_correlation": 0.6015706950519473,
            "eval_runtime": 0.344,
            "eval_samples_per_second": 3032.062,
            "eval_steps_per_second": 26.164,
            "epoch": 35.82089552238806,
            "step": 2400
        },
        {
            "train_runtime": 289.0817,
            "train_samples_per_second": 2957.988,
            "train_steps_per_second": 23.177,
            "total_flos": 2.041624300565299e+16,
            "train_loss": 0.3201612114906311,
            "epoch": 35.82089552238806,
            "step": 2400
        },
        {
            "eval_loss": 0.509477436542511,
            "eval_matthews_correlation": 0.6036344190543846,
            "eval_runtime": 0.3657,
            "eval_samples_per_second": 2851.709,
            "eval_steps_per_second": 24.607,
            "epoch": 35.82089552238806,
            "step": 2400
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./ablation",
        "lora_dropout": 0.05,
        "use_rslora": false,
        "use_olora": true,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "model_family": "roberta",
        "task": "cola",
        "peft": "mrlora-olora-lcoef",
        "seed": 42,
        "rank": 8,
        "lora_alpha": 16,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "use_lcoef": true,
        "use_bias": false,
        "train_size": 8551
    },
    "train": {
        "train_time": 289.0817,
        "trainable_params_count": 0.887114,
        "memory_allocated": [
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552,
            530.327552
        ],
        "memory_reserved": [
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048,
            2829.058048
        ]
    },
    "variant": "lora"
}