{
    "eval_loss": 0.5330840945243835,
    "eval_matthews_correlation": 0.5981333967857257,
    "eval_runtime": 0.3765,
    "eval_samples_per_second": 2770.025,
    "eval_steps_per_second": 23.902,
    "epoch": 23.880597014925375,
    "log_history": [
        {
            "loss": 0.5672,
            "grad_norm": 8.664241790771484,
            "learning_rate": 5.970149253731343e-05,
            "epoch": 2.9850746268656714,
            "step": 200
        },
        {
            "eval_loss": 0.5250376462936401,
            "eval_matthews_correlation": 0.4748241723006093,
            "eval_runtime": 0.3807,
            "eval_samples_per_second": 2740.041,
            "eval_steps_per_second": 23.644,
            "epoch": 2.9850746268656714,
            "step": 200
        },
        {
            "loss": 0.4129,
            "grad_norm": 10.780552864074707,
            "learning_rate": 0.00011940298507462686,
            "epoch": 5.970149253731344,
            "step": 400
        },
        {
            "eval_loss": 0.5155697464942932,
            "eval_matthews_correlation": 0.4911657094625286,
            "eval_runtime": 0.3833,
            "eval_samples_per_second": 2721.287,
            "eval_steps_per_second": 23.482,
            "epoch": 5.970149253731344,
            "step": 400
        },
        {
            "loss": 0.3425,
            "grad_norm": 6.865674018859863,
            "learning_rate": 0.0001791044776119403,
            "epoch": 8.955223880597014,
            "step": 600
        },
        {
            "eval_loss": 0.4210081696510315,
            "eval_matthews_correlation": 0.6102702929385095,
            "eval_runtime": 0.3779,
            "eval_samples_per_second": 2759.901,
            "eval_steps_per_second": 23.815,
            "epoch": 8.955223880597014,
            "step": 600
        },
        {
            "loss": 0.261,
            "grad_norm": 5.161693096160889,
            "learning_rate": 0.00019568822553897182,
            "epoch": 11.940298507462687,
            "step": 800
        },
        {
            "eval_loss": 0.5875052809715271,
            "eval_matthews_correlation": 0.5379847122053982,
            "eval_runtime": 0.371,
            "eval_samples_per_second": 2811.333,
            "eval_steps_per_second": 24.259,
            "epoch": 11.940298507462687,
            "step": 800
        },
        {
            "loss": 0.187,
            "grad_norm": 7.437027454376221,
            "learning_rate": 0.00018905472636815922,
            "epoch": 14.925373134328359,
            "step": 1000
        },
        {
            "eval_loss": 0.8170185089111328,
            "eval_matthews_correlation": 0.5363286623915381,
            "eval_runtime": 0.3798,
            "eval_samples_per_second": 2746.328,
            "eval_steps_per_second": 23.698,
            "epoch": 14.925373134328359,
            "step": 1000
        },
        {
            "loss": 0.1391,
            "grad_norm": 8.701310157775879,
            "learning_rate": 0.0001824212271973466,
            "epoch": 17.91044776119403,
            "step": 1200
        },
        {
            "eval_loss": 0.7343276143074036,
            "eval_matthews_correlation": 0.5756584003915552,
            "eval_runtime": 0.3678,
            "eval_samples_per_second": 2835.784,
            "eval_steps_per_second": 24.47,
            "epoch": 17.91044776119403,
            "step": 1200
        },
        {
            "loss": 0.1041,
            "grad_norm": 11.45147705078125,
            "learning_rate": 0.000175787728026534,
            "epoch": 20.895522388059703,
            "step": 1400
        },
        {
            "eval_loss": 0.7134947180747986,
            "eval_matthews_correlation": 0.5806284057433418,
            "eval_runtime": 0.3487,
            "eval_samples_per_second": 2991.422,
            "eval_steps_per_second": 25.813,
            "epoch": 20.895522388059703,
            "step": 1400
        },
        {
            "loss": 0.0858,
            "grad_norm": 7.609888553619385,
            "learning_rate": 0.0001691542288557214,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "eval_loss": 0.8155698180198669,
            "eval_matthews_correlation": 0.6009540363352786,
            "eval_runtime": 0.3728,
            "eval_samples_per_second": 2797.569,
            "eval_steps_per_second": 24.14,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "train_runtime": 178.425,
            "train_samples_per_second": 4792.489,
            "train_steps_per_second": 37.551,
            "total_flos": 1.3657221833424896e+16,
            "train_loss": 0.26246716737747194,
            "epoch": 23.880597014925375,
            "step": 1600
        },
        {
            "eval_loss": 0.5330840945243835,
            "eval_matthews_correlation": 0.5981333967857257,
            "eval_runtime": 0.3765,
            "eval_samples_per_second": 2770.025,
            "eval_steps_per_second": 23.902,
            "epoch": 23.880597014925375,
            "step": 1600
        }
    ],
    "args": {
        "dataset_path": "./dataset",
        "train_batch_size": 32,
        "eval_batch_size": 32,
        "weight_decay": 0.01,
        "dir_name": "./ablation",
        "lora_dropout": 0.05,
        "use_rslora": true,
        "use_olora": false,
        "teacher_learning_rate": 2e-05,
        "student_learning_rate": 0.0001,
        "lora_learning_rate": 0.0002,
        "model_family": "roberta",
        "task": "cola",
        "peft": "mrlora-rs",
        "seed": 42,
        "rank": 16,
        "lora_alpha": 32,
        "student_model_name": "./models/distilroberta-base",
        "teacher_model_name": "./models/roberta-base",
        "use_lcoef": false,
        "use_bias": false,
        "train_size": 8551
    },
    "train": {
        "train_time": 178.425,
        "trainable_params_count": 1.181954,
        "memory_allocated": [
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176,
            535.666176
        ],
        "memory_reserved": [
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464,
            2531.262464
        ]
    },
    "variant": "lora"
}