{
 "cells": [
  {
   "cell_type": "code",
   "id": "95587a1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.154483Z",
     "start_time": "2026-02-08T10:56:27.151577Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 确保所有列都能显示出来\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# 确保列宽足够，不会把长字符串（比如 Method 名）截断\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# 确保表格的总宽度足够，不会换行显示\n",
    "pd.set_option('display.width', 1000)"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "id": "9375c819",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.175136Z",
     "start_time": "2026-02-08T10:56:27.172301Z"
    }
   },
   "source": [
    "TASK_METRIC = {\n",
    "    \"cola\": [\"eval_matthews_correlation\"],\n",
    "    \"mnli\": [\"matched_accuracy\", \"mismatched_accuracy\"],\n",
    "    \"mrpc\": [\"eval_accuracy\", \"eval_f1\"],\n",
    "    \"qnli\": [\"eval_accuracy\"],\n",
    "    \"qqp\": [\"eval_accuracy\", \"eval_f1\"],\n",
    "    \"rte\": [\"eval_accuracy\"],\n",
    "    \"sst2\": [\"eval_accuracy\"],\n",
    "    \"stsb\": [\"eval_pearson\", \"eval_spearman\"],\n",
    "    \"wnli\": [\"eval_accuracy\"],\n",
    "}\n",
    "\n",
    "METRIC_NAME_MAP = {\n",
    "    'eval_matthews_correlation': 'Mcc',\n",
    "    'matched_accuracy': 'm',\n",
    "    'mismatched_accuracy': 'mm',\n",
    "    'eval_accuracy': 'Acc',\n",
    "    'eval_f1': 'F1',\n",
    "    'eval_pearson': 'Corr_p',\n",
    "    'eval_spearman': 'Corr_s',\n",
    "}\n",
    "\n",
    "TASK_NAME_MAP = {\n",
    "    'mnli': 'MNLI',\n",
    "    'sst2': 'SST-2',\n",
    "    'cola': 'CoLA',\n",
    "    'qqp': 'QQP',\n",
    "    'qnli': 'QNLI',\n",
    "    'rte': 'RTE',\n",
    "    'mrpc': 'MRPC',\n",
    "    'stsb': 'STS-B',\n",
    "    'wnli': 'WNLI',\n",
    "}\n",
    "\n",
    "FAMILY_NAME_MAP = {\n",
    "    'bert': 'BERT-b',\n",
    "    'roberta': 'RoB-b',\n",
    "    'deberta': 'DeB-b',\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.454827Z",
     "start_time": "2026-02-08T10:56:27.180519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dictor import dictor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import  NA\n",
    "\n",
    "def extract_experiment_data(json_file, root_dir):\n",
    "    variant = Path(json_file).relative_to(root_dir).parts[0]\n",
    "\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract metadata\n",
    "    model_family = dictor(data, 'args.model_family')\n",
    "    peft_method = dictor(data, 'args.peft')\n",
    "    task = dictor(data, 'args.task')\n",
    "\n",
    "    # for mnli, need patching.\n",
    "    if 'eval_runtime' in data:\n",
    "        eval_runtime = data.get('eval_runtime')\n",
    "    else:\n",
    "        eval_runtime_history = []\n",
    "        for item in data['log_history']:\n",
    "            if 'eval_runtime' in item:\n",
    "                eval_runtime_history.append(item['eval_runtime'])\n",
    "        eval_runtime = sum(eval_runtime_history) / len(eval_runtime_history)\n",
    "\n",
    "    # Get training-specific metrics\n",
    "    trainable_params = dictor(data, 'train.trainable_params_count', NA)\n",
    "    train_runtime = dictor(data, 'train.train_time', NA)\n",
    "\n",
    "    # Calculate Average GPU Memory (Allocated)\n",
    "    memory_list = dictor(data, 'train.memory_allocated', [])\n",
    "    avg_memory = np.mean(memory_list) if memory_list else NA\n",
    "\n",
    "    rank = dictor(data, 'args.rank')\n",
    "\n",
    "    # Get metrics\n",
    "    # Some tasks use eval_accuracy, others eval_matthews_correlation\n",
    "    for key in TASK_METRIC[task]:\n",
    "        if key in data:\n",
    "            accuracy = data[key]\n",
    "            yield {\n",
    "                \"family\": model_family,\n",
    "                \"peft\": peft_method,\n",
    "                \"task\": task,\n",
    "                \"variant\": variant,\n",
    "                \"value\": round(accuracy, 4),\n",
    "                \"metric\": key,\n",
    "                \"params\": round(trainable_params, 4),\n",
    "                \"traintime\": round(train_runtime, 2),\n",
    "                \"evaltime\": round(eval_runtime, 2),\n",
    "                \"gpumem\": round(avg_memory, 2),\n",
    "                \"rank\": rank, # total rank.\n",
    "                'seed': dictor(data, 'args.seed'),\n",
    "                'path': str(json_file)\n",
    "            }\n",
    "\n",
    "\n",
    "def aggregate_experiment_results(root_dir):\n",
    "    \"\"\"\n",
    "    Finds all .json files under a directory recursively, extracts data,\n",
    "    and concatenates them into one large DataFrame.\n",
    "    \"\"\"\n",
    "    root_path = Path(root_dir)\n",
    "    # Recursively find all JSON files\n",
    "    json_files = list(root_path.rglob(\"*.json\"))\n",
    "\n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {root_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_dfs = []\n",
    "    for f in json_files:\n",
    "        try:\n",
    "            rows = extract_experiment_data(f, root_dir)\n",
    "            all_dfs.extend(rows)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract data from {f}\")\n",
    "            raise e\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(\"No valid data extracted from found files.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Concatenate all individual DataFrames by row\n",
    "    final_df = pd.DataFrame.from_records(all_dfs)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "df = aggregate_experiment_results('./results/')"
   ],
   "id": "1d2b9421d16b2b9",
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "id": "dfcb26f5",
   "metadata": {},
   "source": [
    "## FFT, KD-LoRA, LoRA"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.459980Z",
     "start_time": "2026-02-08T10:56:27.458585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TOTAL_RANKS = [8]\n",
    "CAPTION = \"Results on GLUE development set for BERT-base (BERT-b), DeBERTa-v3-base (DeB-b), and RoBERTa-base (RoB-b). \" +\\\n",
    "    \"We compare different fine-tuning strategies: Fully Fine-Tuning (FFT), MR-LoRA Fine-Tuning (MR), and Knowledge Distillation MR-LoRA Fine-Tuning (KD). \" \\\n",
    "    \"Results of two total ranks $r={}$ are reported. \".format(', '.join(map(str, TOTAL_RANKS))) +\\\n",
    "    \"We report the average correlation for STS-B. \" +\\\n",
    "    \"We report mean of 3 runs using different random seeds. \"\n",
    "LABEL = 'tab:perf-params'"
   ],
   "id": "153f9936bfbb0171",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.465553Z",
     "start_time": "2026-02-08T10:56:27.463538Z"
    }
   },
   "cell_type": "code",
   "source": "CAPTION",
   "id": "5a6c6b93dfb6b7ca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Results on GLUE development set for BERT-base (BERT-b), DeBERTa-v3-base (DeB-b), and RoBERTa-base (RoB-b). We compare different fine-tuning strategies: Fully Fine-Tuning (FFT), MR-LoRA Fine-Tuning (MR), and Knowledge Distillation MR-LoRA Fine-Tuning (KD). Results of two total ranks $r=8$ are reported. We report the average correlation for STS-B. We report mean of 3 runs using different random seeds. '"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.478927Z",
     "start_time": "2026-02-08T10:56:27.476102Z"
    }
   },
   "cell_type": "code",
   "source": "df = df[(df.variant == 'fft') | (df.peft.str.contains('mrlora') & df['rank'].isin(TOTAL_RANKS))]",
   "id": "d301eb18ce97dbe0",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.498255Z",
     "start_time": "2026-02-08T10:56:27.496219Z"
    }
   },
   "cell_type": "code",
   "source": "df.family.unique()",
   "id": "5766745ac8c086e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bert', 'deberta', 'roberta'], dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.506142Z",
     "start_time": "2026-02-08T10:56:27.501608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for key, value in METRIC_NAME_MAP.items():\n",
    "    df.replace(key, value, inplace=True)\n",
    "for key, value in TASK_NAME_MAP.items():\n",
    "    df.replace(key, value, inplace=True)\n",
    "for key, value in FAMILY_NAME_MAP.items():\n",
    "    df.replace(key, value, inplace=True)"
   ],
   "id": "1fe82d0cc3671180",
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "id": "6cf5213c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.516532Z",
     "start_time": "2026-02-08T10:56:27.514906Z"
    }
   },
   "source": [
    "df['value'] = df.value * 100"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "id": "3c252280",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.530745Z",
     "start_time": "2026-02-08T10:56:27.528353Z"
    }
   },
   "source": [
    "df['rank'].unique(), df.family.unique(), df.peft.unique(), df.task.unique(), df.metric.unique()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8]),\n",
       " array(['BERT-b', 'DeB-b', 'RoB-b'], dtype=object),\n",
       " array(['mrlora-lcoef', 'lora', 'mrlora-rs-olora', 'mrlora', 'mrlora-rs',\n",
       "        'mrlora-olora'], dtype=object),\n",
       " array(['QQP', 'CoLA', 'MNLI', 'SST-2', 'MRPC', 'STS-B', 'QNLI', 'RTE'],\n",
       "       dtype=object),\n",
       " array(['Acc', 'F1', 'Mcc', 'm', 'mm', 'Corr_p', 'Corr_s'], dtype=object))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "id": "dd94db60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.543742Z",
     "start_time": "2026-02-08T10:56:27.542233Z"
    }
   },
   "source": [
    "# 1. 格式化 params 的函数\n",
    "def format_params(x):\n",
    "    val = float(x)\n",
    "    # 如果是整数（如 184.0），显示为 184M\n",
    "    if val.is_integer():\n",
    "        return f\"{int(val)}M\"\n",
    "    # 如果有小数（如 0.312），保留两位显示为 0.31M\n",
    "    else:\n",
    "        return f\"{val:.2f}M\"\n"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "id": "5790017e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.583992Z",
     "start_time": "2026-02-08T10:56:27.564270Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Standardize Method Names\n",
    "def get_method_name(row):\n",
    "    if row['variant'] == 'fft': return 'FFT'\n",
    "    prefix = 'MR' if row['variant'] == 'lora' else 'KD'\n",
    "    return f\"{prefix}$_{{r={int(row['rank'])}}}$\"\n",
    "\n",
    "df['Method'] = df.apply(get_method_name, axis=1)\n",
    "\n",
    "df['params'] = df.groupby(['family', 'variant', 'rank'])['params'].transform('mean')\n",
    "\n",
    "# 2. SEED AVERAGING (The \"Fix\")\n",
    "# We group by the configuration identifying columns. \n",
    "# We include 'params' here NOT as a grouper, but to average it \n",
    "# (though it should be constant across seeds).\n",
    "# We exclude 'metric' from the grouper so we can aggregate different metrics later.\n",
    "df_agged = df.groupby(\n",
    "    ['family', 'variant', 'rank', 'Method', 'task', 'metric'], \n",
    "    as_index=False\n",
    ").agg({\n",
    "    'value': 'mean',  # Performance Metric\n",
    "    'params': 'mean'  # Efficiency Metric\n",
    "})\n",
    "\n",
    "# 3. Handle Multi-Metric Tasks (MNLI, QQP, STS-B)\n",
    "def format_task_entries(group):\n",
    "    task = group['task'].iloc[0]\n",
    "    # Create a map for the averaged performance metrics\n",
    "    perf_map = dict(zip(group['metric'], group['value']))\n",
    "    \n",
    "    # Take the mean of params for this group (efficiency is constant for the method)\n",
    "    avg_params = group['params'].mean()\n",
    "    \n",
    "    if task == 'MNLI':\n",
    "        val = f\"{perf_map.get('m', 0):.2f}/{perf_map.get('mm', 0):.2f}\"\n",
    "        met = 'm/mm'\n",
    "    elif task == 'QQP':\n",
    "        val = f\"{perf_map.get('Acc', 0):.2f}/{perf_map.get('F1', 0):.2f}\"\n",
    "        met = 'Acc/F1'\n",
    "    elif task == 'STS-B':\n",
    "        # GLUE standard: average of Pearson and Spearman\n",
    "        avg_corr = (perf_map.get('Corr_s', 0) + perf_map.get('Corr_p', 0)) / 2\n",
    "        val, met = f\"{avg_corr:.2f}\", 'Corr'\n",
    "    else:\n",
    "        val, met = f\"{group['value'].iloc[0]:.2f}\", group['metric'].iloc[0]\n",
    "    \n",
    "    return pd.Series({\n",
    "        'val': val, \n",
    "        'met': met, \n",
    "        'params': avg_params,\n",
    "        'numeric_score': group['value'].mean() # Used for 'All Ave.'\n",
    "    })\n",
    "\n",
    "# Apply the formatting logic\n",
    "df_transformed = df_agged.groupby(\n",
    "    ['family', 'variant', 'rank', 'Method', 'task'], \n",
    "    as_index=False\n",
    ").apply(format_task_entries)\n",
    "\n",
    "# 4. Calculate 'All Ave.' Column\n",
    "# Grouping by the method configuration to average performance across all tasks\n",
    "all_avg = df_transformed.groupby(\n",
    "    ['family', 'variant', 'rank', 'Method']\n",
    ").agg({\n",
    "    'numeric_score': 'mean',\n",
    "    'params': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "all_avg['task'], all_avg['met'] = 'All', 'Ave.'\n",
    "all_avg['val'] = all_avg['numeric_score'].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "# 5. Pivot and Final Formatting\n",
    "df_final = pd.concat([df_transformed, all_avg], ignore_index=True)\n",
    "\n",
    "# Formatting the Efficiency Metric (Params) for the Index\n",
    "df_final['# Params'] = df_final['params'].apply(format_params)\n",
    "\n",
    "pivot_df = df_final.pivot(\n",
    "    index=['family', 'variant', 'rank', 'Method', '# Params'],\n",
    "    columns=['task', 'met'],\n",
    "    values='val'\n",
    ")\n",
    "\n",
    "# Sorting and Cleaning\n",
    "pivot_df = pivot_df.sort_index(level=['family', 'variant', 'rank'], ascending=[True, True, False])\n",
    "pivot_df.index = pivot_df.index.droplevel(['variant', 'rank'])\n",
    "pivot_df.index.names = [None, None, None]\n",
    "pivot_df.columns.names = [None, None]\n",
    "\n",
    "# Column Ordering\n",
    "task_order = ['MNLI', 'SST-2', 'CoLA', 'QQP', 'QNLI', 'RTE', 'MRPC', 'STS-B', 'WNLI', 'All']\n",
    "existing_tasks = [t for t in task_order if t in pivot_df.columns.get_level_values(0)]\n",
    "pivot_df = pivot_df.reindex(columns=existing_tasks, level=0)\n",
    "\n",
    "print(pivot_df)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  MNLI  SST-2   CoLA          QQP   QNLI    RTE   MRPC  STS-B    All\n",
      "                                  m/mm    Acc    Mcc       Acc/F1    Acc    Acc    Acc   Corr   Ave.\n",
      "BERT-b FFT        109.48M  82.47/82.74  92.18  60.51  88.97/85.62  90.95  66.48  83.99  89.13  81.96\n",
      "       KD$_{r=8}$ 0.74M    76.29/77.81  89.10  49.65  75.73/69.72  87.77  58.56  82.79  85.25  75.70\n",
      "       MR$_{r=8}$ 0.29M    78.63/79.66  91.03  56.55  85.92/81.24  89.95  68.95  84.92  88.47  80.59\n",
      "DeB-b  FFT        184.42M  89.61/89.60  95.85  70.84  90.45/87.52  93.82  82.73  89.34  91.06  87.97\n",
      "       KD$_{r=8}$ 0.15M    43.67/43.61  93.53  61.74   63.35/2.53  90.84  72.13  88.09  88.21  71.60\n",
      "       MR$_{r=8}$ 0.30M    88.35/88.59  94.91  66.96  75.32/55.43  93.06  82.67  87.99  89.92  83.89\n",
      "RoB-b  FFT        124.65M  86.19/86.20  94.31  63.13  89.05/85.91  92.54  78.28  88.60  90.87  85.38\n",
      "       KD$_{r=8}$ 0.74M    69.71/70.30  91.38  57.90  72.89/41.15  89.28  62.10  86.08  86.92  75.34\n",
      "       MR$_{r=8}$ 0.89M    83.21/83.72  93.61  58.68  86.13/82.28  91.71  75.45  88.11  90.31  83.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/82/tr0t3jp906z8k_q9pbcg_jd40000gn/T/ipykernel_89420/66257272.py:60: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ).apply(format_task_entries)\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "id": "1fd63c69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.598086Z",
     "start_time": "2026-02-08T10:56:27.595324Z"
    }
   },
   "source": [
    "# 2. Use Styler to generate the LaTeX code\n",
    "latex_code = pivot_df.style.to_latex(\n",
    "    column_format='l|l|c|' + 'c' * len(pivot_df.columns),\n",
    "    hrules=True,\n",
    "    multicol_align=\"c\",\n",
    "    multirow_align=\"c\"\n",
    ").strip()\n",
    "\n",
    "# 3. Adjust spacing for the 'tight' look in the image\n",
    "final_latex = (\n",
    "    \"\\\\begin{table}[h]\\n\"\n",
    "    \"\\\\centering\\n\"\n",
    "    \"\\\\setlength{\\\\tabcolsep}{4pt} % Smaller column gap\\n\"\n",
    "    \"\\\\renewcommand{\\\\arraystretch}{1.2} % Better vertical spacing\\n\"\n",
    "    \"\\\\caption{\" + CAPTION + \"}\\n\"\n",
    "    \"\\\\label{\" + LABEL + \"}\\n\"\n",
    "    \"\\\\resizebox{\\\\textwidth}{!}{% <--- Start resize\\n\"\n",
    "    f\"{latex_code}\"\n",
    "    \"% <--- End resize\\n}\\n\"\n",
    "    \"\\\\end{table}\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "id": "f8ab321c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.601721Z",
     "start_time": "2026-02-08T10:56:27.600560Z"
    }
   },
   "source": [
    "final_latex = final_latex.replace('&  &  &',\n",
    " r'\\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{Method}} & \\multirow{2}{*}{\\textbf{\\# Params}} &', 1)"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "id": "719df74b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.604990Z",
     "start_time": "2026-02-08T10:56:27.603577Z"
    }
   },
   "source": [
    "task_values = list(TASK_NAME_MAP.values()) + ['All']\n",
    "for task in task_values:\n",
    "    final_latex = final_latex.replace('& ' + task, r'& \\textbf{'+task+'}')\n",
    "\n",
    "print(final_latex)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\setlength{\\tabcolsep}{4pt} % Smaller column gap\n",
      "\\renewcommand{\\arraystretch}{1.2} % Better vertical spacing\n",
      "\\caption{Results on GLUE development set for BERT-base (BERT-b), DeBERTa-v3-base (DeB-b), and RoBERTa-base (RoB-b). We compare different fine-tuning strategies: Fully Fine-Tuning (FFT), MR-LoRA Fine-Tuning (MR), and Knowledge Distillation MR-LoRA Fine-Tuning (KD). Results of two total ranks $r=8$ are reported. We report the average correlation for STS-B. We report mean of 3 runs using different random seeds. }\n",
      "\\label{tab:perf-params}\n",
      "\\resizebox{\\textwidth}{!}{% <--- Start resize\n",
      "\\begin{tabular}{l|l|c|ccccccccc}\n",
      "\\toprule\n",
      " \\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{Method}} & \\multirow{2}{*}{\\textbf{\\# Params}} & \\textbf{MNLI} & \\textbf{SST-2} & \\textbf{CoLA} & \\textbf{QQP} & \\textbf{QNLI} & \\textbf{RTE} & \\textbf{MRPC} & \\textbf{STS-B} & \\textbf{All} \\\\\n",
      " &  &  & m/mm & Acc & Mcc & Acc/F1 & Acc & Acc & Acc & Corr & Ave. \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{BERT-b} & FFT & 109.48M & 82.47/82.74 & 92.18 & 60.51 & 88.97/85.62 & 90.95 & 66.48 & 83.99 & 89.13 & 81.96 \\\\\n",
      " & KD$_{r=8}$ & 0.74M & 76.29/77.81 & 89.10 & 49.65 & 75.73/69.72 & 87.77 & 58.56 & 82.79 & 85.25 & 75.70 \\\\\n",
      " & MR$_{r=8}$ & 0.29M & 78.63/79.66 & 91.03 & 56.55 & 85.92/81.24 & 89.95 & 68.95 & 84.92 & 88.47 & 80.59 \\\\\n",
      "\\multirow[c]{3}{*}{DeB-b} & FFT & 184.42M & 89.61/89.60 & 95.85 & 70.84 & 90.45/87.52 & 93.82 & 82.73 & 89.34 & 91.06 & 87.97 \\\\\n",
      " & KD$_{r=8}$ & 0.15M & 43.67/43.61 & 93.53 & 61.74 & 63.35/2.53 & 90.84 & 72.13 & 88.09 & 88.21 & 71.60 \\\\\n",
      " & MR$_{r=8}$ & 0.30M & 88.35/88.59 & 94.91 & 66.96 & 75.32/55.43 & 93.06 & 82.67 & 87.99 & 89.92 & 83.89 \\\\\n",
      "\\multirow[c]{3}{*}{RoB-b} & FFT & 124.65M & 86.19/86.20 & 94.31 & 63.13 & 89.05/85.91 & 92.54 & 78.28 & 88.60 & 90.87 & 85.38 \\\\\n",
      " & KD$_{r=8}$ & 0.74M & 69.71/70.30 & 91.38 & 57.90 & 72.89/41.15 & 89.28 & 62.10 & 86.08 & 86.92 & 75.34 \\\\\n",
      " & MR$_{r=8}$ & 0.89M & 83.21/83.72 & 93.61 & 58.68 & 86.13/82.28 & 91.71 & 75.45 & 88.11 & 90.31 & 83.39 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}% <--- End resize\n",
      "}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "id": "b0899991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.624015Z",
     "start_time": "2026-02-08T10:56:27.621822Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "def add_latex_family_dividers(latex_str):\n",
    "    # This regex looks for a line starting with a family name (not an empty cell)\n",
    "    # It avoids the header by looking for rows after the first \\midrule\n",
    "    \n",
    "    # 1. Find the start of the data rows (usually after the metric header)\n",
    "    header_end_split = latex_str.split('\\\\midrule', 1)\n",
    "    if len(header_end_split) < 2:\n",
    "        return latex_str\n",
    "    \n",
    "    header = header_end_split[0] + '\\\\midrule'\n",
    "    body = header_end_split[1]\n",
    "\n",
    "    # 2. Regex to find the start of a NEW family block.\n",
    "    # In MultiIndex LaTeX, a new index level starts with text, \n",
    "    # while sub-rows start with ' &' or '  &'.\n",
    "    # We look for: Start of line + word characters + '&'\n",
    "    # But we skip the very first line of the body to avoid double midrules.\n",
    "    \n",
    "    lines = body.split('\\n')\n",
    "    new_body = []\n",
    "    \n",
    "    # We track if we are at the very first data line\n",
    "    first_data_line = True\n",
    "    \n",
    "    for line in lines:\n",
    "        # Check if the line starts a new family (e.g., 'BERT-b &')\n",
    "        # This regex matches lines that start with text before the first '&'\n",
    "        if re.match(r'\\\\multirow.* &', line.strip()):\n",
    "            if not first_data_line:\n",
    "                # Insert a midrule before this line\n",
    "                new_body.append('\\\\midrule')\n",
    "            first_data_line = False\n",
    "        \n",
    "        new_body.append(line)\n",
    "\n",
    "    return header + '\\n'.join(new_body)\n",
    "\n",
    "# Usage:\n",
    "final_latex = add_latex_family_dividers(final_latex)\n",
    "print(final_latex)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\setlength{\\tabcolsep}{4pt} % Smaller column gap\n",
      "\\renewcommand{\\arraystretch}{1.2} % Better vertical spacing\n",
      "\\caption{Results on GLUE development set for BERT-base (BERT-b), DeBERTa-v3-base (DeB-b), and RoBERTa-base (RoB-b). We compare different fine-tuning strategies: Fully Fine-Tuning (FFT), MR-LoRA Fine-Tuning (MR), and Knowledge Distillation MR-LoRA Fine-Tuning (KD). Results of two total ranks $r=8$ are reported. We report the average correlation for STS-B. We report mean of 3 runs using different random seeds. }\n",
      "\\label{tab:perf-params}\n",
      "\\resizebox{\\textwidth}{!}{% <--- Start resize\n",
      "\\begin{tabular}{l|l|c|ccccccccc}\n",
      "\\toprule\n",
      " \\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{Method}} & \\multirow{2}{*}{\\textbf{\\# Params}} & \\textbf{MNLI} & \\textbf{SST-2} & \\textbf{CoLA} & \\textbf{QQP} & \\textbf{QNLI} & \\textbf{RTE} & \\textbf{MRPC} & \\textbf{STS-B} & \\textbf{All} \\\\\n",
      " &  &  & m/mm & Acc & Mcc & Acc/F1 & Acc & Acc & Acc & Corr & Ave. \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{BERT-b} & FFT & 109.48M & 82.47/82.74 & 92.18 & 60.51 & 88.97/85.62 & 90.95 & 66.48 & 83.99 & 89.13 & 81.96 \\\\\n",
      " & KD$_{r=8}$ & 0.74M & 76.29/77.81 & 89.10 & 49.65 & 75.73/69.72 & 87.77 & 58.56 & 82.79 & 85.25 & 75.70 \\\\\n",
      " & MR$_{r=8}$ & 0.29M & 78.63/79.66 & 91.03 & 56.55 & 85.92/81.24 & 89.95 & 68.95 & 84.92 & 88.47 & 80.59 \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{DeB-b} & FFT & 184.42M & 89.61/89.60 & 95.85 & 70.84 & 90.45/87.52 & 93.82 & 82.73 & 89.34 & 91.06 & 87.97 \\\\\n",
      " & KD$_{r=8}$ & 0.15M & 43.67/43.61 & 93.53 & 61.74 & 63.35/2.53 & 90.84 & 72.13 & 88.09 & 88.21 & 71.60 \\\\\n",
      " & MR$_{r=8}$ & 0.30M & 88.35/88.59 & 94.91 & 66.96 & 75.32/55.43 & 93.06 & 82.67 & 87.99 & 89.92 & 83.89 \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{RoB-b} & FFT & 124.65M & 86.19/86.20 & 94.31 & 63.13 & 89.05/85.91 & 92.54 & 78.28 & 88.60 & 90.87 & 85.38 \\\\\n",
      " & KD$_{r=8}$ & 0.74M & 69.71/70.30 & 91.38 & 57.90 & 72.89/41.15 & 89.28 & 62.10 & 86.08 & 86.92 & 75.34 \\\\\n",
      " & MR$_{r=8}$ & 0.89M & 83.21/83.72 & 93.61 & 58.68 & 86.13/82.28 & 91.71 & 75.45 & 88.11 & 90.31 & 83.39 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}% <--- End resize\n",
      "}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "id": "5287253d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T10:56:27.634487Z",
     "start_time": "2026-02-08T10:56:27.632104Z"
    }
   },
   "source": [
    "Path('./MrLoRA/table1.tex').write_text(final_latex)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2109"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
