{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "95587a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 确保所有列都能显示出来\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# 确保列宽足够，不会把长字符串（比如 Method 名）截断\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# 确保表格的总宽度足够，不会换行显示\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9375c819",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_METRIC = {\n",
    "    \"cola\": [\"eval_matthews_correlation\"],\n",
    "    \"mnli\": [\"matched_accuracy\", \"mismatched_accuracy\"],\n",
    "    \"mrpc\": [\"eval_accuracy\", \"eval_f1\"],\n",
    "    \"qnli\": [\"eval_accuracy\"],\n",
    "    \"qqp\": [\"eval_accuracy\", \"eval_f1\"],\n",
    "    \"rte\": [\"eval_accuracy\"],\n",
    "    \"sst2\": [\"eval_accuracy\"],\n",
    "    \"stsb\": [\"eval_pearson\", \"eval_spearman\"],\n",
    "    \"wnli\": [\"eval_accuracy\"],\n",
    "}\n",
    "\n",
    "METRIC_NAME_MAP = {\n",
    "    'eval_matthews_correlation': 'Mcc',\n",
    "    'matched_accuracy': 'm',\n",
    "    'mismatched_accuracy': 'mm',\n",
    "    'eval_accuracy': 'Acc',\n",
    "    'eval_f1': 'F1',\n",
    "    'eval_pearson': 'Corr_p',\n",
    "    'eval_spearman': 'Corr_s',\n",
    "}\n",
    "\n",
    "TASK_NAME_MAP = {\n",
    "    'mnli': 'MNLI',\n",
    "    'sst2': 'SST-2',\n",
    "    'cola': 'CoLA',\n",
    "    'qqp': 'QQP',\n",
    "    'qnli': 'QNLI',\n",
    "    'rte': 'RTE',\n",
    "    'mrpc': 'MRPC',\n",
    "    'stsb': 'STS-B',\n",
    "    'wnli': 'WNLI',\n",
    "}\n",
    "\n",
    "FAMILY_NAME_MAP = {\n",
    "    'bert': 'BERT-b',\n",
    "    'roberta': 'RoB-b',\n",
    "    'deberta': 'DeB-b',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T07:17:15.910759Z",
     "start_time": "2026-01-26T07:17:15.782704Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dictor import dictor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_experiment_data(json_file):\n",
    "    variant = Path(json_file).relative_to('./results').parts[0]\n",
    "\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    data['variant'] = variant\n",
    "    # with open(json_file, 'w') as f:\n",
    "    #     json.dump(data, f, indent=4)\n",
    "\n",
    "    # Extract metadata\n",
    "    model_family = dictor(data, 'args.model_family')\n",
    "    peft_method = dictor(data, 'args.peft')\n",
    "    task = dictor(data, 'args.task')\n",
    "\n",
    "    eval_runtime = data.get('eval_runtime', -1)\n",
    "\n",
    "    # Get training-specific metrics\n",
    "    trainable_params = dictor(data, 'train.trainable_params_count', -1)\n",
    "    train_runtime = dictor(data, 'train.train_time', -1)\n",
    "\n",
    "    # Calculate Average GPU Memory (Allocated)\n",
    "    memory_list = dictor(data, 'train.memory_allocated', [])\n",
    "    avg_memory = np.mean(memory_list) if memory_list else -1\n",
    "\n",
    "    rank = dictor(data, 'args.rank')\n",
    "    if 'mrlora' in peft_method:\n",
    "        rank = 2*rank - 1 # r = 2*R - 1\n",
    "        \n",
    "    # Get metrics\n",
    "    # Some tasks use eval_accuracy, others eval_matthews_correlation\n",
    "    for key in TASK_METRIC[task]:\n",
    "        if key in data:\n",
    "            accuracy = data[key]\n",
    "            yield {\n",
    "                \"family\": model_family,\n",
    "                \"peft\": peft_method,\n",
    "                \"task\": task,\n",
    "                \"variant\": variant,\n",
    "                \"value\": round(accuracy, 4),\n",
    "                \"metric\": key,\n",
    "                \"params\": round(trainable_params, 4),\n",
    "                \"traintime\": round(train_runtime, 2),\n",
    "                \"evaltime\": round(eval_runtime, 2),\n",
    "                \"gpumem\": round(avg_memory, 2),\n",
    "                \"rank\": rank, # total rank.\n",
    "            }\n",
    "\n",
    "\n",
    "def aggregate_experiment_results(root_dir):\n",
    "    \"\"\"\n",
    "    Finds all .json files under a directory recursively, extracts data,\n",
    "    and concatenates them into one large DataFrame.\n",
    "    \"\"\"\n",
    "    root_path = Path(root_dir)\n",
    "    # Recursively find all JSON files\n",
    "    json_files = list(root_path.rglob(\"*.json\"))\n",
    "\n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {root_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_dfs = []\n",
    "    for f in json_files:\n",
    "        try:\n",
    "            rows = extract_experiment_data(f)\n",
    "            all_dfs.extend(rows)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract data from {f}\")\n",
    "            raise e\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(\"No valid data extracted from found files.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Concatenate all individual DataFrames by row\n",
    "    final_df = pd.DataFrame.from_records(all_dfs)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "df = aggregate_experiment_results('./results/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb26f5",
   "metadata": {},
   "source": [
    "## FFT, KD-LoRA, LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "59549e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_RANKS = [15, 31]\n",
    "CAPTION = \"Results on GLUE development set for BERT-base (BERT-b), DeBERTa-v3-base (DeB-b), and RoBERTa-base (RoB-b). \" +\\\n",
    "    \"We compare different fine-tuning strategies: Fully Fine-Tuning (FFT), MR-LoRA Fine-Tuning (MR), and Knowledge Distillation MR-LoRA Fine-Tuning (KD). \" \\\n",
    "    \"Results of two total ranks $r={}$ are reported. \".format(', '.join(map(str, TOTAL_RANKS))) +\\\n",
    "    \"We report the average correlation for STS-B. \" +\\\n",
    "    \"We report mean of 3 runs using different random seeds. \"\n",
    "LABEL = 'tab:perf-params'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "968d8c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Results on GLUE development set for BERT-base (BERT-b), DeBERTa-v3-base (DeB-b), and RoBERTa-base (RoB-b). We compare different fine-tuning strategies: Fully Fine-Tuning (FFT), MR-LoRA Fine-Tuning (MR), and Knowledge Distillation MR-LoRA Fine-Tuning (KD). Results of two total ranks $r=15, 31$ are reported. We report the average correlation for STS-B. We report mean of 3 runs using different random seeds. '"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0363da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.variant == 'fft') | (df.peft.str.contains('mrlora-rs') & df['rank'].isin(TOTAL_RANKS))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b780cea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['roberta', 'bert', 'deberta'], dtype=object)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.family.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6c3aeb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in METRIC_NAME_MAP.items():\n",
    "    df.replace(key, value, inplace=True)\n",
    "for key, value in TASK_NAME_MAP.items():\n",
    "    df.replace(key, value, inplace=True)\n",
    "for key, value in FAMILY_NAME_MAP.items():\n",
    "    df.replace(key, value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6cf5213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['value'] = df.value * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3c252280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8]),\n",
       " array(['RoB-b', 'BERT-b', 'DeB-b'], dtype=object),\n",
       " array(['lora'], dtype=object),\n",
       " array(['MRPC', 'RTE', 'STS-B', 'QNLI', 'QQP'], dtype=object),\n",
       " array(['Acc', 'F1', 'Corr_p', 'Corr_s'], dtype=object))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rank'].unique(), df.family.unique(), df.peft.unique(), df.task.unique(), df.metric.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "dd94db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 格式化 params 的函数\n",
    "def format_params(x):\n",
    "    val = float(x)\n",
    "    # 如果是整数（如 184.0），显示为 184M\n",
    "    if val.is_integer():\n",
    "        return f\"{int(val)}M\"\n",
    "    # 如果有小数（如 0.312），保留两位显示为 0.31M\n",
    "    else:\n",
    "        return f\"{val:.2f}M\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5790017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            QQP   QNLI    RTE   MRPC  STS-B    All\n",
      "                         Acc/F1    Acc    Acc    Acc   Corr   Ave.\n",
      "BERT-b FFT 109.48M  88.81/85.45  91.14  64.98  84.56  89.34  83.92\n",
      "DeB-b  FFT 184.42M  90.39/87.39  93.81  83.03  89.22  91.16  89.53\n",
      "RoB-b  FFT 124.65M  87.66/84.44  92.37  79.06  88.48  90.93  87.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/82/tr0t3jp906z8k_q9pbcg_jd40000gn/T/ipykernel_74239/446213983.py:60: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ).apply(format_task_entries)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Standardize Method Names\n",
    "def get_method_name(row):\n",
    "    if row['variant'] == 'fft': return 'FFT'\n",
    "    prefix = 'MR' if row['variant'] == 'lora' else 'KD'\n",
    "    return f\"{prefix}$_{{r={int(row['rank'])}}}$\"\n",
    "\n",
    "df['Method'] = df.apply(get_method_name, axis=1)\n",
    "\n",
    "df['params'] = df.groupby(['family', 'variant', 'rank'])['params'].transform('mean')\n",
    "\n",
    "# 2. SEED AVERAGING (The \"Fix\")\n",
    "# We group by the configuration identifying columns. \n",
    "# We include 'params' here NOT as a grouper, but to average it \n",
    "# (though it should be constant across seeds).\n",
    "# We exclude 'metric' from the grouper so we can aggregate different metrics later.\n",
    "df_agged = df.groupby(\n",
    "    ['family', 'variant', 'rank', 'Method', 'task', 'metric'], \n",
    "    as_index=False\n",
    ").agg({\n",
    "    'value': 'mean',  # Performance Metric\n",
    "    'params': 'mean'  # Efficiency Metric\n",
    "})\n",
    "\n",
    "# 3. Handle Multi-Metric Tasks (MNLI, QQP, STS-B)\n",
    "def format_task_entries(group):\n",
    "    task = group['task'].iloc[0]\n",
    "    # Create a map for the averaged performance metrics\n",
    "    perf_map = dict(zip(group['metric'], group['value']))\n",
    "    \n",
    "    # Take the mean of params for this group (efficiency is constant for the method)\n",
    "    avg_params = group['params'].mean()\n",
    "    \n",
    "    if task == 'MNLI':\n",
    "        val = f\"{perf_map.get('m', 0):.2f}/{perf_map.get('mm', 0):.2f}\"\n",
    "        met = 'm/mm'\n",
    "    elif task == 'QQP':\n",
    "        val = f\"{perf_map.get('Acc', 0):.2f}/{perf_map.get('F1', 0):.2f}\"\n",
    "        met = 'Acc/F1'\n",
    "    elif task == 'STS-B':\n",
    "        # GLUE standard: average of Pearson and Spearman\n",
    "        avg_corr = (perf_map.get('Corr_s', 0) + perf_map.get('Corr_p', 0)) / 2\n",
    "        val, met = f\"{avg_corr:.2f}\", 'Corr'\n",
    "    else:\n",
    "        val, met = f\"{group['value'].iloc[0]:.2f}\", group['metric'].iloc[0]\n",
    "    \n",
    "    return pd.Series({\n",
    "        'val': val, \n",
    "        'met': met, \n",
    "        'params': avg_params,\n",
    "        'numeric_score': group['value'].mean() # Used for 'All Ave.'\n",
    "    })\n",
    "\n",
    "# Apply the formatting logic\n",
    "df_transformed = df_agged.groupby(\n",
    "    ['family', 'variant', 'rank', 'Method', 'task'], \n",
    "    as_index=False\n",
    ").apply(format_task_entries)\n",
    "\n",
    "# 4. Calculate 'All Ave.' Column\n",
    "# Grouping by the method configuration to average performance across all tasks\n",
    "all_avg = df_transformed.groupby(\n",
    "    ['family', 'variant', 'rank', 'Method']\n",
    ").agg({\n",
    "    'numeric_score': 'mean',\n",
    "    'params': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "all_avg['task'], all_avg['met'] = 'All', 'Ave.'\n",
    "all_avg['val'] = all_avg['numeric_score'].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "# 5. Pivot and Final Formatting\n",
    "df_final = pd.concat([df_transformed, all_avg], ignore_index=True)\n",
    "\n",
    "# Formatting the Efficiency Metric (Params) for the Index\n",
    "df_final['# Params'] = df_final['params'].apply(format_params)\n",
    "\n",
    "pivot_df = df_final.pivot(\n",
    "    index=['family', 'variant', 'rank', 'Method', '# Params'],\n",
    "    columns=['task', 'met'],\n",
    "    values='val'\n",
    ")\n",
    "\n",
    "# Sorting and Cleaning\n",
    "pivot_df = pivot_df.sort_index(level=['family', 'variant', 'rank'], ascending=[True, True, False])\n",
    "pivot_df.index = pivot_df.index.droplevel(['variant', 'rank'])\n",
    "pivot_df.index.names = [None, None, None]\n",
    "pivot_df.columns.names = [None, None]\n",
    "\n",
    "# Column Ordering\n",
    "task_order = ['MNLI', 'SST-2', 'CoLA', 'QQP', 'QNLI', 'RTE', 'MRPC', 'STS-B', 'WNLI', 'All']\n",
    "existing_tasks = [t for t in task_order if t in pivot_df.columns.get_level_values(0)]\n",
    "pivot_df = pivot_df.reindex(columns=existing_tasks, level=0)\n",
    "\n",
    "print(pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1fd63c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Use Styler to generate the LaTeX code\n",
    "latex_code = pivot_df.style.to_latex(\n",
    "    column_format='l|l|c|' + 'c' * len(pivot_df.columns),\n",
    "    hrules=True,\n",
    "    multicol_align=\"c\",\n",
    "    multirow_align=\"c\"\n",
    ").strip()\n",
    "\n",
    "# 3. Adjust spacing for the 'tight' look in the image\n",
    "final_latex = (\n",
    "    \"\\\\begin{table}[h]\\n\"\n",
    "    \"\\\\centering\\n\"\n",
    "    \"\\\\setlength{\\\\tabcolsep}{4pt} % Smaller column gap\\n\"\n",
    "    \"\\\\renewcommand{\\\\arraystretch}{1.2} % Better vertical spacing\\n\"\n",
    "    \"\\\\caption{\" + CAPTION + \"}\\n\"\n",
    "    \"\\\\label{\" + LABEL + \"}\\n\"\n",
    "    \"\\\\resizebox{\\\\textwidth}{!}{% <--- Start resize\\n\"\n",
    "    f\"{latex_code}\"\n",
    "    \"% <--- End resize\\n}\\n\"\n",
    "    \"\\\\end{table}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "f8ab321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_latex = final_latex.replace('&  &  &',\n",
    " r'\\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{Method}} & \\multirow{2}{*}{\\textbf{\\# Params}} &', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "719df74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\setlength{\\tabcolsep}{4pt} % Smaller column gap\n",
      "\\renewcommand{\\arraystretch}{1.2} % Better vertical spacing\n",
      "\\caption{Results on GLUE development set for BERT-base (BERT-b), DeBERTa-v3-base (DeB-b), and RoBERTa-base (RoB-b). We compare different fine-tuning strategies: Fully Fine-Tuning (FFT), MR-LoRA Fine-Tuning (MR), and Knowledge Distillation MR-LoRA Fine-Tuning (KD). Results of two total ranks $r=15, 31$ are reported. We report the average correlation for STS-B. We report mean of 3 runs using different random seeds. }\n",
      "\\label{tab:perf-params}\n",
      "\\resizebox{\\textwidth}{!}{% <--- Start resize\n",
      "\\begin{tabular}{l|l|c|cccccc}\n",
      "\\toprule\n",
      " \\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{Method}} & \\multirow{2}{*}{\\textbf{\\# Params}} & \\textbf{QQP} & \\textbf{QNLI} & \\textbf{RTE} & \\textbf{MRPC} & \\textbf{STS-B} & \\textbf{All} \\\\\n",
      " &  &  & Acc/F1 & Acc & Acc & Acc & Corr & Ave. \\\\\n",
      "\\midrule\n",
      "BERT-b & FFT & 109.48M & 88.81/85.45 & 91.14 & 64.98 & 84.56 & 89.34 & 83.92 \\\\\n",
      "DeB-b & FFT & 184.42M & 90.39/87.39 & 93.81 & 83.03 & 89.22 & 91.16 & 89.53 \\\\\n",
      "RoB-b & FFT & 124.65M & 87.66/84.44 & 92.37 & 79.06 & 88.48 & 90.93 & 87.72 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}% <--- End resize\n",
      "}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "task_values = list(TASK_NAME_MAP.values()) + ['All']\n",
    "for task in task_values:\n",
    "    final_latex = final_latex.replace('& ' + task, r'& \\textbf{'+task+'}')\n",
    "\n",
    "print(final_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b0899991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[h]\n",
      "\\centering\n",
      "\\setlength{\\tabcolsep}{4pt} % Smaller column gap\n",
      "\\renewcommand{\\arraystretch}{1.2} % Better vertical spacing\n",
      "\\caption{Results on GLUE development set for BERT-base (BERT-b), DeBERTa-v3-base (DeB-b), and RoBERTa-base (RoB-b). We compare different fine-tuning strategies: Fully Fine-Tuning (FFT), MR-LoRA Fine-Tuning (MR), and Knowledge Distillation MR-LoRA Fine-Tuning (KD). Results of two total ranks $r=15, 31$ are reported. We report the average correlation for STS-B. We report mean of 3 runs using different random seeds. }\n",
      "\\label{tab:perf-params}\n",
      "\\resizebox{\\textwidth}{!}{% <--- Start resize\n",
      "\\begin{tabular}{l|l|c|cccccc}\n",
      "\\toprule\n",
      " \\multirow{2}{*}{\\textbf{Model}} & \\multirow{2}{*}{\\textbf{Method}} & \\multirow{2}{*}{\\textbf{\\# Params}} & \\textbf{QQP} & \\textbf{QNLI} & \\textbf{RTE} & \\textbf{MRPC} & \\textbf{STS-B} & \\textbf{All} \\\\\n",
      " &  &  & Acc/F1 & Acc & Acc & Acc & Corr & Ave. \\\\\n",
      "\\midrule\n",
      "BERT-b & FFT & 109.48M & 88.81/85.45 & 91.14 & 64.98 & 84.56 & 89.34 & 83.92 \\\\\n",
      "DeB-b & FFT & 184.42M & 90.39/87.39 & 93.81 & 83.03 & 89.22 & 91.16 & 89.53 \\\\\n",
      "RoB-b & FFT & 124.65M & 87.66/84.44 & 92.37 & 79.06 & 88.48 & 90.93 & 87.72 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}% <--- End resize\n",
      "}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def add_latex_family_dividers(latex_str):\n",
    "    # This regex looks for a line starting with a family name (not an empty cell)\n",
    "    # It avoids the header by looking for rows after the first \\midrule\n",
    "    \n",
    "    # 1. Find the start of the data rows (usually after the metric header)\n",
    "    header_end_split = latex_str.split('\\\\midrule', 1)\n",
    "    if len(header_end_split) < 2:\n",
    "        return latex_str\n",
    "    \n",
    "    header = header_end_split[0] + '\\\\midrule'\n",
    "    body = header_end_split[1]\n",
    "\n",
    "    # 2. Regex to find the start of a NEW family block.\n",
    "    # In MultiIndex LaTeX, a new index level starts with text, \n",
    "    # while sub-rows start with ' &' or '  &'.\n",
    "    # We look for: Start of line + word characters + '&'\n",
    "    # But we skip the very first line of the body to avoid double midrules.\n",
    "    \n",
    "    lines = body.split('\\n')\n",
    "    new_body = []\n",
    "    \n",
    "    # We track if we are at the very first data line\n",
    "    first_data_line = True\n",
    "    \n",
    "    for line in lines:\n",
    "        # Check if the line starts a new family (e.g., 'BERT-b &')\n",
    "        # This regex matches lines that start with text before the first '&'\n",
    "        if re.match(r'\\\\multirow.* &', line.strip()):\n",
    "            if not first_data_line:\n",
    "                # Insert a midrule before this line\n",
    "                new_body.append('\\\\midrule')\n",
    "            first_data_line = False\n",
    "        \n",
    "        new_body.append(line)\n",
    "\n",
    "    return header + '\\n'.join(new_body)\n",
    "\n",
    "# Usage:\n",
    "final_latex = add_latex_family_dividers(final_latex)\n",
    "print(final_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5287253d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1222"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path('./MrLoRA/table1.tex').write_text(final_latex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
