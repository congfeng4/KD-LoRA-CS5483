{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95587a1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:57:50.112595Z",
     "start_time": "2026-02-09T04:57:50.110148Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 确保所有列都能显示出来\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# 确保列宽足够，不会把长字符串（比如 Method 名）截断\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# 确保表格的总宽度足够，不会换行显示\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9375c819",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:57:51.062695Z",
     "start_time": "2026-02-09T04:57:51.046231Z"
    }
   },
   "outputs": [],
   "source": [
    "TASK_METRIC = {\n",
    "    \"cola\": [\"eval_matthews_correlation\"],\n",
    "    \"mnli\": [\"matched_accuracy\", \"mismatched_accuracy\"],\n",
    "    \"mrpc\": [\"eval_accuracy\", \"eval_f1\"],\n",
    "    \"qnli\": [\"eval_accuracy\"],\n",
    "    \"qqp\": [\"eval_accuracy\", \"eval_f1\"],\n",
    "    \"rte\": [\"eval_accuracy\"],\n",
    "    \"sst2\": [\"eval_accuracy\"],\n",
    "    \"stsb\": [\"eval_pearson\", \"eval_spearman\"],\n",
    "    \"wnli\": [\"eval_accuracy\"],\n",
    "}\n",
    "\n",
    "METRIC_NAME_MAP = {\n",
    "    'eval_matthews_correlation': 'Mcc',\n",
    "    'matched_accuracy': 'm',\n",
    "    'mismatched_accuracy': 'mm',\n",
    "    'eval_accuracy': 'Acc',\n",
    "    'eval_f1': 'F1',\n",
    "    'eval_pearson': 'Corr_p',\n",
    "    'eval_spearman': 'Corr_s',\n",
    "}\n",
    "\n",
    "TASK_NAME_MAP = {\n",
    "    'mnli': 'MNLI',\n",
    "    'sst2': 'SST-2',\n",
    "    'cola': 'CoLA',\n",
    "    'qqp': 'QQP',\n",
    "    'qnli': 'QNLI',\n",
    "    'rte': 'RTE',\n",
    "    'mrpc': 'MRPC',\n",
    "    'stsb': 'STS-B',\n",
    "}\n",
    "\n",
    "FAMILY_NAME_MAP = {\n",
    "    'bert': 'BERT-b',\n",
    "    'roberta': 'RoB-b',\n",
    "    'deberta': 'DeB-b',\n",
    "}\n",
    "\n",
    "METHOD_NAME_MAP = {\n",
    "    'lora': 'LoRA',\n",
    "    'olora': 'OLoRA',\n",
    "    'dora': 'DoRA',\n",
    "    'mrlora': 'MR-LoRA',\n",
    "    'adalora': 'AdaLoRA',\n",
    "    'mrlora-rs': 'MR-LoRA-RS',\n",
    "    'rslora': 'RS-LoRA'\n",
    "}\n",
    "VARIANT_NAME_MAP = {\n",
    "    'fft': 'FFT',\n",
    "    'lora': 'LoRA-Finetuning',\n",
    "    'kd-lora': 'KD-LoRA-Finetuning'\n",
    "}\n",
    "\n",
    "REMOVE_PEFT = ['mrlora-rs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:57:51.191694Z",
     "start_time": "2026-02-09T04:57:51.138931Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from dictor import dictor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import  NA\n",
    "\n",
    "def extract_experiment_data(json_file, root_dir):\n",
    "    variant = Path(json_file).relative_to(root_dir).parts[0]\n",
    "\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract metadata\n",
    "    model_family = dictor(data, 'args.model_family')\n",
    "    peft_method = dictor(data, 'args.peft')\n",
    "    task = dictor(data, 'args.task')\n",
    "\n",
    "    # for mnli, need patching.\n",
    "    if 'eval_runtime' in data:\n",
    "        eval_runtime = data.get('eval_runtime')\n",
    "    else:\n",
    "        eval_runtime_history = []\n",
    "        for item in data['log_history']:\n",
    "            if 'eval_runtime' in item:\n",
    "                eval_runtime_history.append(item['eval_runtime'])\n",
    "        eval_runtime = sum(eval_runtime_history) / len(eval_runtime_history)\n",
    "\n",
    "    # Get training-specific metrics\n",
    "    trainable_params = dictor(data, 'train.trainable_params_count', NA)\n",
    "    train_runtime = dictor(data, 'train.train_time', NA)\n",
    "\n",
    "    # Calculate Average GPU Memory (Allocated)\n",
    "    memory_list = dictor(data, 'train.memory_allocated', [])\n",
    "    avg_memory = np.mean(memory_list) if memory_list else NA\n",
    "\n",
    "    rank = dictor(data, 'args.rank')\n",
    "\n",
    "    # Get metrics\n",
    "    # Some tasks use eval_accuracy, others eval_matthews_correlation\n",
    "    for key in TASK_METRIC[task]:\n",
    "        if key in data:\n",
    "            accuracy = data[key]\n",
    "            yield {\n",
    "                \"family\": model_family,\n",
    "                \"peft\": peft_method,\n",
    "                \"task\": task,\n",
    "                \"variant\": variant,\n",
    "                \"value\": round(accuracy, 4),\n",
    "                \"metric\": key,\n",
    "                \"params\": round(trainable_params, 4),\n",
    "                \"traintime\": round(train_runtime, 2),\n",
    "                \"evaltime\": round(eval_runtime, 2),\n",
    "                \"gpumem\": round(avg_memory, 2),\n",
    "                \"rank\": rank, # total rank.\n",
    "                'seed': dictor(data, 'args.seed'),\n",
    "                'path': str(json_file)\n",
    "            }\n",
    "\n",
    "\n",
    "def aggregate_experiment_results(root_dir):\n",
    "    \"\"\"\n",
    "    Finds all .json files under a directory recursively, extracts data,\n",
    "    and concatenates them into one large DataFrame.\n",
    "    \"\"\"\n",
    "    root_path = Path(root_dir)\n",
    "    # Recursively find all JSON files\n",
    "    json_files = list(root_path.rglob(\"metrics.json\"))\n",
    "\n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in {root_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_dfs = []\n",
    "    for f in json_files:\n",
    "        try:\n",
    "            rows = extract_experiment_data(f, root_dir)\n",
    "            all_dfs.extend(rows)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract data from {f}\")\n",
    "            raise e\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(\"No valid data extracted from found files.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Concatenate all individual DataFrames by row\n",
    "    final_df = pd.DataFrame.from_records(all_dfs)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "df = aggregate_experiment_results('./ablation3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "51ab95559e913b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:57:51.673632Z",
     "start_time": "2026-02-09T04:57:51.667832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['deberta'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.family.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14a30665e40b194d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:57:52.765619Z",
     "start_time": "2026-02-09T04:57:52.762058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mrlora-rs-olora', 'mrlora-rs-lcoef', 'mrlora', 'mrlora-rs',\n",
       "       'mrlora-rs-olora-lcoef', 'mrlora-olora-lcoef', 'mrlora-olora',\n",
       "       'mrlora-lcoef'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.peft.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8fbe73398833aec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:57:55.404732Z",
     "start_time": "2026-02-09T04:57:55.398687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rank'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "44c0212dc379992b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:57:56.090823Z",
     "start_time": "2026-02-09T04:57:56.074050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['qnli', 'cola', 'rte'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.task.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "440790b4a0f827b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:57:56.706572Z",
     "start_time": "2026-02-09T04:57:56.696235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>peft</th>\n",
       "      <th>variant</th>\n",
       "      <th>value</th>\n",
       "      <th>metric</th>\n",
       "      <th>params</th>\n",
       "      <th>traintime</th>\n",
       "      <th>evaltime</th>\n",
       "      <th>gpumem</th>\n",
       "      <th>rank</th>\n",
       "      <th>seed</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cola</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qnli</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rte</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      family  peft  variant  value  metric  params  traintime  evaltime  gpumem  rank  seed  path\n",
       "task                                                                                             \n",
       "cola      10    10       10     10      10      10         10        10      10    10    10    10\n",
       "qnli       8     8        8      8       8       8          8         8       8     8     8     8\n",
       "rte        4     4        4      4       4       4          4         4       4     4     4     4"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('task').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57a931016c9ee7f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:57:57.885715Z",
     "start_time": "2026-02-09T04:57:57.881657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['eval_accuracy', 'eval_matthews_correlation'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.metric.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c34344c47bffaa56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:57:58.600312Z",
     "start_time": "2026-02-09T04:57:58.596519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.seed.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a075dbd861ae3d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:57:59.829409Z",
     "start_time": "2026-02-09T04:57:59.826027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['lora', 'kd-lora'], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.variant.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1798adbfcbc94c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:00.436344Z",
     "start_time": "2026-02-09T04:58:00.430225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2964, 0.2965, 0.149 ])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.params.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97703ef3d88747f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:01.086444Z",
     "start_time": "2026-02-09T04:58:01.068406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>params</th>\n",
       "      <th>traintime</th>\n",
       "      <th>evaltime</th>\n",
       "      <th>gpumem</th>\n",
       "      <th>rank</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.858683</td>\n",
       "      <td>0.296442</td>\n",
       "      <td>1198.749167</td>\n",
       "      <td>2.289167</td>\n",
       "      <td>761.64250</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.096065</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>694.598565</td>\n",
       "      <td>1.511859</td>\n",
       "      <td>0.05446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.671500</td>\n",
       "      <td>0.296400</td>\n",
       "      <td>292.340000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>761.53000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.759000</td>\n",
       "      <td>0.296400</td>\n",
       "      <td>391.390000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>761.65000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.919200</td>\n",
       "      <td>0.296400</td>\n",
       "      <td>1370.380000</td>\n",
       "      <td>3.140000</td>\n",
       "      <td>761.65000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.921950</td>\n",
       "      <td>0.296500</td>\n",
       "      <td>1585.565000</td>\n",
       "      <td>3.295000</td>\n",
       "      <td>761.68000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.296500</td>\n",
       "      <td>2439.520000</td>\n",
       "      <td>3.630000</td>\n",
       "      <td>761.68000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           value     params    traintime   evaltime     gpumem  rank  seed\n",
       "count  12.000000  12.000000    12.000000  12.000000   12.00000  12.0  12.0\n",
       "mean    0.858683   0.296442  1198.749167   2.289167  761.64250   8.0  42.0\n",
       "std     0.096065   0.000051   694.598565   1.511859    0.05446   0.0   0.0\n",
       "min     0.671500   0.296400   292.340000   0.230000  761.53000   8.0  42.0\n",
       "25%     0.759000   0.296400   391.390000   0.270000  761.65000   8.0  42.0\n",
       "50%     0.919200   0.296400  1370.380000   3.140000  761.65000   8.0  42.0\n",
       "75%     0.921950   0.296500  1585.565000   3.295000  761.68000   8.0  42.0\n",
       "max     0.927500   0.296500  2439.520000   3.630000  761.68000   8.0  42.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.metric == 'eval_accuracy'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c05606017ed6c66f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:02.217068Z",
     "start_time": "2026-02-09T04:58:02.187377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>peft</th>\n",
       "      <th>task</th>\n",
       "      <th>variant</th>\n",
       "      <th>value</th>\n",
       "      <th>metric</th>\n",
       "      <th>params</th>\n",
       "      <th>traintime</th>\n",
       "      <th>evaltime</th>\n",
       "      <th>gpumem</th>\n",
       "      <th>rank</th>\n",
       "      <th>seed</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [family, peft, task, variant, value, metric, params, traintime, evaltime, gpumem, rank, seed, path]\n",
       "Index: []"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.value == 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "59c649ba972b5c51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:03.154327Z",
     "start_time": "2026-02-09T04:58:03.150492Z"
    }
   },
   "outputs": [],
   "source": [
    "df_simple = df[(df.task != 'stsb') & (df['rank'] == 8) & (df.variant == 'kd-lora')]\n",
    "df_simple = df_simple[df_simple.metric != 'eval_f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e62273245330afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:03.895450Z",
     "start_time": "2026-02-09T04:58:03.835295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5b4b6_row0_col0 {\n",
       "  background-color: #ffffd9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5b4b6_row1_col0 {\n",
       "  background-color: #081d58;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5b4b6\">\n",
       "  <caption>MrLoRA Feature Ablation Study</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"blank\" >&nbsp;</th>\n",
       "      <th class=\"index_name level0\" >task</th>\n",
       "      <th id=\"T_5b4b6_level0_col0\" class=\"col_heading level0 col0\" >cola</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >rs</th>\n",
       "      <th class=\"index_name level1\" >lcoef</th>\n",
       "      <th class=\"index_name level2\" >olora</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5b4b6_level0_row0\" class=\"row_heading level0 row0\" >False</th>\n",
       "      <th id=\"T_5b4b6_level1_row0\" class=\"row_heading level1 row0\" >False</th>\n",
       "      <th id=\"T_5b4b6_level2_row0\" class=\"row_heading level2 row0\" >False</th>\n",
       "      <td id=\"T_5b4b6_row0_col0\" class=\"data row0 col0\" >0.5804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5b4b6_level0_row1\" class=\"row_heading level0 row1\" >True</th>\n",
       "      <th id=\"T_5b4b6_level1_row1\" class=\"row_heading level1 row1\" >False</th>\n",
       "      <th id=\"T_5b4b6_level2_row1\" class=\"row_heading level2 row1\" >False</th>\n",
       "      <td id=\"T_5b4b6_row1_col0\" class=\"data row1 col0\" >0.5957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f973c7620a0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Expand the 'peft' strings into feature columns\n",
    "features = [ 'rs', 'lcoef', 'olora', ]\n",
    "\n",
    "for f in features:\n",
    "    # Checks if the feature name exists as a standalone word in the string\n",
    "    df_simple[f] = df_simple['peft'].apply(lambda x: f in x.split('-'))\n",
    "\n",
    "# 2. Create a Pivot Table\n",
    "# We group by the feature flags and show the mean 'value' for each 'task'\n",
    "pivot_df = df_simple.pivot_table(\n",
    "    index=features,\n",
    "    columns='task',\n",
    "    values='value',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# 3. Apply Styling (Conditional Formatting)\n",
    "styled_table = pivot_df.style.background_gradient(axis=0, cmap='YlGnBu') \\\n",
    "                             .format(\"{:.4f}\") \\\n",
    "                             .set_caption(\"MrLoRA Feature Ablation Study\")\n",
    "\n",
    "# Display in Jupyter/Colab\n",
    "styled_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9a54ce04a4a75ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:04.594457Z",
     "start_time": "2026-02-09T04:58:04.537143Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of features to isolate\n",
    "features = ['olora', 'rs', 'lcoef']\n",
    "\n",
    "# Create boolean columns: True if feature name is in the 'peft' string\n",
    "for f in features:\n",
    "    df_simple[f] = df_simple['peft'].apply(lambda x: f in x.split('-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae3e0cfa4b46888",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bcb5854cd105ad44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:05.185286Z",
     "start_time": "2026-02-09T04:58:05.178940Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/lora/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/lora/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/lora/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5846\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.UInt8HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5870\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.UInt8HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Calculate Mean for 'On' vs 'Off' across all tasks\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     summary \u001b[38;5;241m=\u001b[39m df_simple\u001b[38;5;241m.\u001b[39mgroupby(f)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      6\u001b[0m     impact_results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m: f,\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOff_Avg\u001b[39m\u001b[38;5;124m'\u001b[39m: summary[\u001b[38;5;28;01mFalse\u001b[39;00m],\n\u001b[0;32m----> 9\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOn_Avg\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43msummary\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m]\u001b[49m,\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDelta\u001b[39m\u001b[38;5;124m'\u001b[39m: summary[\u001b[38;5;28;01mTrue\u001b[39;00m] \u001b[38;5;241m-\u001b[39m summary[\u001b[38;5;28;01mFalse\u001b[39;00m]\n\u001b[1;32m     11\u001b[0m     })\n\u001b[1;32m     13\u001b[0m df_impact \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(impact_results)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_impact)\n",
      "File \u001b[0;32m~/anaconda3/envs/lora/lib/python3.8/site-packages/pandas/core/series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lora/lib/python3.8/site-packages/pandas/core/series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1116\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/anaconda3/envs/lora/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: True"
     ]
    }
   ],
   "source": [
    "impact_results = []\n",
    "\n",
    "for f in features:\n",
    "    # Calculate Mean for 'On' vs 'Off' across all tasks\n",
    "    summary = df_simple.groupby(f)['value'].mean()\n",
    "    impact_results.append({\n",
    "        'Feature': f,\n",
    "        'Off_Avg': summary[False],\n",
    "        'On_Avg': summary[True],\n",
    "        'Delta': summary[True] - summary[False]\n",
    "    })\n",
    "\n",
    "df_impact = pd.DataFrame(impact_results)\n",
    "print(df_impact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f145e2e877ecd1",
   "metadata": {},
   "source": [
    "1. bias cause obvious drop.\n",
    "2. rs and lcoef boost perf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b207fde3f9e13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:06.505857Z",
     "start_time": "2026-02-09T04:58:05.983960Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_564228/1667137800.py:13: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  g = sns.catplot(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f973c58fd00>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 840x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Melt the data for visualization\n",
    "plot_data = []\n",
    "for f in features:\n",
    "    subset = df_simple[[f, 'value', 'task']].copy()\n",
    "    subset['Feature_Name'] = f\n",
    "    subset = subset.rename(columns={f: 'Status'})\n",
    "    subset['Status'] = subset['Status'].map({True: 'On', False: 'Off'})\n",
    "    plot_data.append(subset)\n",
    "\n",
    "df_plot = pd.concat(plot_data)\n",
    "\n",
    "# Create a FacetGrid to see On/Off for each feature across tasks\n",
    "g = sns.catplot(\n",
    "    data=df_plot, x='Status', y='value',\n",
    "    col='Feature_Name', kind='bar',\n",
    "    palette='muted', height=4, aspect=0.7\n",
    ")\n",
    "g.set_titles(\"{col_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f370cc5235e224ab",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf48a86140a816",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:10.595609Z",
     "start_time": "2026-02-09T04:58:10.582585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature   Off_Avg    On_Avg     Delta\n",
      "0   olora  0.296445  0.296444 -0.000001\n",
      "1      rs  0.296445  0.296444 -0.000001\n",
      "2   lcoef  0.296400  0.296500  0.000100\n"
     ]
    }
   ],
   "source": [
    "impact_results = []\n",
    "\n",
    "for f in features:\n",
    "    # Calculate Mean for 'On' vs 'Off' across all tasks\n",
    "    summary = df_simple.groupby(f)['params'].mean()\n",
    "    impact_results.append({\n",
    "        'Feature': f,\n",
    "        'Off_Avg': summary[False],\n",
    "        'On_Avg': summary[True],\n",
    "        'Delta': summary[True] - summary[False]\n",
    "    })\n",
    "\n",
    "df_impact = pd.DataFrame(impact_results)\n",
    "print(df_impact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8570453e5699aa3",
   "metadata": {},
   "source": [
    "### Traintime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c002fa7d60b95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:11.256511Z",
     "start_time": "2026-02-09T04:58:11.231191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature     Off_Avg       On_Avg       Delta\n",
      "0   olora  710.749091  1150.764444  440.015354\n",
      "1      rs  969.123636   834.973333 -134.150303\n",
      "2   lcoef  886.142727   936.394444   50.251717\n"
     ]
    }
   ],
   "source": [
    "impact_results = []\n",
    "\n",
    "for f in features:\n",
    "    # Calculate Mean for 'On' vs 'Off' across all tasks\n",
    "    summary = df_simple.groupby(f)['traintime'].mean()\n",
    "    impact_results.append({\n",
    "        'Feature': f,\n",
    "        'Off_Avg': summary[False],\n",
    "        'On_Avg': summary[True],\n",
    "        'Delta': summary[True] - summary[False]\n",
    "    })\n",
    "\n",
    "df_impact = pd.DataFrame(impact_results)\n",
    "print(df_impact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a0571e908c87d",
   "metadata": {},
   "source": [
    "1. Olora cuts traintime.\n",
    "2. rs add traintime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6357b36f540c8e5",
   "metadata": {},
   "source": [
    "### GPUMEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155dd12a1a8cea22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:11.416928Z",
     "start_time": "2026-02-09T04:58:11.408028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature     Off_Avg      On_Avg     Delta\n",
      "0   olora  761.630909  761.663333  0.032424\n",
      "1      rs  761.630909  761.663333  0.032424\n",
      "2   lcoef  761.617273  761.680000  0.062727\n"
     ]
    }
   ],
   "source": [
    "impact_results = []\n",
    "\n",
    "for f in features:\n",
    "    # Calculate Mean for 'On' vs 'Off' across all tasks\n",
    "    summary = df_simple.groupby(f)['gpumem'].mean()\n",
    "    impact_results.append({\n",
    "        'Feature': f,\n",
    "        'Off_Avg': summary[False],\n",
    "        'On_Avg': summary[True],\n",
    "        'Delta': summary[True] - summary[False]\n",
    "    })\n",
    "\n",
    "df_impact = pd.DataFrame(impact_results)\n",
    "print(df_impact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c37fed59d5101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:12.147491Z",
     "start_time": "2026-02-09T04:58:12.143853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([761.65, 761.68, 761.53])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_simple.gpumem.unique() # Almost the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c736e089edac5e7",
   "metadata": {},
   "source": [
    "## Compare with SOTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998732af3a0030e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:36.666812Z",
     "start_time": "2026-02-09T04:58:35.763401Z"
    }
   },
   "outputs": [],
   "source": [
    "df_base = aggregate_experiment_results('./results')\n",
    "df_base = df_base[df_base.variant == 'lora']\n",
    "df_base = df_base[df_base.task.isin(df.task.unique())]\n",
    "df_base = df_base[df_base.family.isin(df.family.unique())]\n",
    "df_base = df_base[df_base.seed == 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f61536854053fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:39.339410Z",
     "start_time": "2026-02-09T04:58:39.337459Z"
    }
   },
   "outputs": [],
   "source": [
    "df_our = df_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70f0f541cd088db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:40.957626Z",
     "start_time": "2026-02-09T04:58:40.917101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>peft</th>\n",
       "      <th>task</th>\n",
       "      <th>variant</th>\n",
       "      <th>value</th>\n",
       "      <th>metric</th>\n",
       "      <th>params</th>\n",
       "      <th>traintime</th>\n",
       "      <th>evaltime</th>\n",
       "      <th>gpumem</th>\n",
       "      <th>rank</th>\n",
       "      <th>seed</th>\n",
       "      <th>path</th>\n",
       "      <th>rs</th>\n",
       "      <th>lcoef</th>\n",
       "      <th>olora</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-rs-olora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9275</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>1831.38</td>\n",
       "      <td>3.43</td>\n",
       "      <td>761.65</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-olora_16_0.05_8/metrics.json</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-olora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9270</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>2439.52</td>\n",
       "      <td>3.15</td>\n",
       "      <td>761.65</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-olora_16_0.05_8/metrics.json</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-rs-olora-lcoef</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9251</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>1567.67</td>\n",
       "      <td>3.25</td>\n",
       "      <td>761.68</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-olora-lcoef_16_0.05_8/metr...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-rs</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9209</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>1240.95</td>\n",
       "      <td>3.06</td>\n",
       "      <td>761.65</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs_16_0.05_8/metrics.json</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-rs-lcoef</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9204</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>1276.00</td>\n",
       "      <td>3.63</td>\n",
       "      <td>761.68</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-lcoef_16_0.05_8/metrics.json</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9193</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>1464.76</td>\n",
       "      <td>3.23</td>\n",
       "      <td>761.53</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora_16_0.05_8/metrics.json</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-lcoef</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9191</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>1510.17</td>\n",
       "      <td>3.57</td>\n",
       "      <td>761.68</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-lcoef_16_0.05_8/metrics.json</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-olora-lcoef</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>1639.25</td>\n",
       "      <td>3.13</td>\n",
       "      <td>761.68</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-olora-lcoef_16_0.05_8/metrics...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-lcoef</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.7617</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>378.37</td>\n",
       "      <td>0.23</td>\n",
       "      <td>761.68</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_mrlora-lcoef_16_0.05_8/metrics.json</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.7509</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>395.73</td>\n",
       "      <td>0.27</td>\n",
       "      <td>761.53</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_mrlora_16_0.05_8/metrics.json</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-rs</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.7437</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>348.85</td>\n",
       "      <td>0.27</td>\n",
       "      <td>761.65</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs_16_0.05_8/metrics.json</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-olora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6715</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>292.34</td>\n",
       "      <td>0.25</td>\n",
       "      <td>761.65</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_mrlora-olora_16_0.05_8/metrics.json</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-rs-olora-lcoef</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6700</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>403.36</td>\n",
       "      <td>0.73</td>\n",
       "      <td>761.68</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-olora-lcoef_16_0.05_8/metr...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-olora-lcoef</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6632</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>1048.18</td>\n",
       "      <td>0.54</td>\n",
       "      <td>761.68</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-olora-lcoef_16_0.05_8/metrics...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-lcoef</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6606</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>313.23</td>\n",
       "      <td>0.51</td>\n",
       "      <td>761.68</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-lcoef_16_0.05_8/metrics.json</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6580</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>358.69</td>\n",
       "      <td>0.53</td>\n",
       "      <td>761.53</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora_16_0.05_8/metrics.json</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-rs-olora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6579</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>315.06</td>\n",
       "      <td>0.57</td>\n",
       "      <td>761.65</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-olora_16_0.05_8/metrics.json</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-rs</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6579</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>240.17</td>\n",
       "      <td>0.50</td>\n",
       "      <td>761.65</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs_16_0.05_8/metrics.json</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-olora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6567</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>820.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>761.65</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-olora_16_0.05_8/metrics.json</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-rs-lcoef</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6555</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>291.32</td>\n",
       "      <td>0.60</td>\n",
       "      <td>761.68</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-lcoef_16_0.05_8/metrics.json</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     family                   peft  task variant   value                     metric  params  traintime  evaltime  gpumem  rank  seed                                                                                                 path     rs  lcoef  olora\n",
       "0   deberta        mrlora-rs-olora  qnli    lora  0.9275              eval_accuracy  0.2964    1831.38      3.43  761.65     8    42   ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-olora_16_0.05_8/metrics.json   True  False   True\n",
       "6   deberta           mrlora-olora  qnli    lora  0.9270              eval_accuracy  0.2964    2439.52      3.15  761.65     8    42      ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-olora_16_0.05_8/metrics.json  False  False   True\n",
       "4   deberta  mrlora-rs-olora-lcoef  qnli    lora  0.9251              eval_accuracy  0.2965    1567.67      3.25  761.68     8    42  ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-olora-lcoef_16_0.05_8/metr...   True   True   True\n",
       "3   deberta              mrlora-rs  qnli    lora  0.9209              eval_accuracy  0.2964    1240.95      3.06  761.65     8    42         ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs_16_0.05_8/metrics.json   True  False  False\n",
       "1   deberta        mrlora-rs-lcoef  qnli    lora  0.9204              eval_accuracy  0.2965    1276.00      3.63  761.68     8    42   ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-lcoef_16_0.05_8/metrics.json   True   True  False\n",
       "2   deberta                 mrlora  qnli    lora  0.9193              eval_accuracy  0.2964    1464.76      3.23  761.53     8    42            ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora_16_0.05_8/metrics.json  False  False  False\n",
       "7   deberta           mrlora-lcoef  qnli    lora  0.9191              eval_accuracy  0.2965    1510.17      3.57  761.68     8    42      ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-lcoef_16_0.05_8/metrics.json  False   True  False\n",
       "5   deberta     mrlora-olora-lcoef  qnli    lora  0.9171              eval_accuracy  0.2965    1639.25      3.13  761.68     8    42  ablation3/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-olora-lcoef_16_0.05_8/metrics...  False   True   True\n",
       "19  deberta           mrlora-lcoef   rte    lora  0.7617              eval_accuracy  0.2965     378.37      0.23  761.68     8    42       ablation3/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_mrlora-lcoef_16_0.05_8/metrics.json  False   True  False\n",
       "16  deberta                 mrlora   rte    lora  0.7509              eval_accuracy  0.2964     395.73      0.27  761.53     8    42             ablation3/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_mrlora_16_0.05_8/metrics.json  False  False  False\n",
       "17  deberta              mrlora-rs   rte    lora  0.7437              eval_accuracy  0.2964     348.85      0.27  761.65     8    42          ablation3/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs_16_0.05_8/metrics.json   True  False  False\n",
       "18  deberta           mrlora-olora   rte    lora  0.6715              eval_accuracy  0.2964     292.34      0.25  761.65     8    42       ablation3/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_mrlora-olora_16_0.05_8/metrics.json  False  False   True\n",
       "12  deberta  mrlora-rs-olora-lcoef  cola    lora  0.6700  eval_matthews_correlation  0.2965     403.36      0.73  761.68     8    42  ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-olora-lcoef_16_0.05_8/metr...   True   True   True\n",
       "13  deberta     mrlora-olora-lcoef  cola    lora  0.6632  eval_matthews_correlation  0.2965    1048.18      0.54  761.68     8    42  ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-olora-lcoef_16_0.05_8/metrics...  False   True   True\n",
       "15  deberta           mrlora-lcoef  cola    lora  0.6606  eval_matthews_correlation  0.2965     313.23      0.51  761.68     8    42      ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-lcoef_16_0.05_8/metrics.json  False   True  False\n",
       "10  deberta                 mrlora  cola    lora  0.6580  eval_matthews_correlation  0.2964     358.69      0.53  761.53     8    42            ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora_16_0.05_8/metrics.json  False  False  False\n",
       "8   deberta        mrlora-rs-olora  cola    lora  0.6579  eval_matthews_correlation  0.2964     315.06      0.57  761.65     8    42   ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-olora_16_0.05_8/metrics.json   True  False   True\n",
       "11  deberta              mrlora-rs  cola    lora  0.6579  eval_matthews_correlation  0.2964     240.17      0.50  761.65     8    42         ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs_16_0.05_8/metrics.json   True  False  False\n",
       "14  deberta           mrlora-olora  cola    lora  0.6567  eval_matthews_correlation  0.2964     820.12      0.50  761.65     8    42      ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-olora_16_0.05_8/metrics.json  False  False   True\n",
       "9   deberta        mrlora-rs-lcoef  cola    lora  0.6555  eval_matthews_correlation  0.2965     291.32      0.60  761.68     8    42   ablation3/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-lcoef_16_0.05_8/metrics.json   True   True  False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_our.sort_values('value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e08081d96b22fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T04:58:42.997489Z",
     "start_time": "2026-02-09T04:58:42.988918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>peft</th>\n",
       "      <th>task</th>\n",
       "      <th>variant</th>\n",
       "      <th>value</th>\n",
       "      <th>metric</th>\n",
       "      <th>params</th>\n",
       "      <th>traintime</th>\n",
       "      <th>evaltime</th>\n",
       "      <th>gpumem</th>\n",
       "      <th>rank</th>\n",
       "      <th>seed</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9390</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>1.1812</td>\n",
       "      <td>1638.15</td>\n",
       "      <td>2.48</td>\n",
       "      <td>775.63</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_lora_32_0.05_32/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>deberta</td>\n",
       "      <td>rslora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9389</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>1663.23</td>\n",
       "      <td>1.91</td>\n",
       "      <td>761.37</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_rslora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9381</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>2.3608</td>\n",
       "      <td>1385.57</td>\n",
       "      <td>2.05</td>\n",
       "      <td>794.41</td>\n",
       "      <td>64</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_lora_64_0.05_64/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>deberta</td>\n",
       "      <td>dora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9379</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.3149</td>\n",
       "      <td>2775.19</td>\n",
       "      <td>3.27</td>\n",
       "      <td>761.91</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_dora_8_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9363</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>1838.87</td>\n",
       "      <td>1.79</td>\n",
       "      <td>762.15</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_lora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>deberta</td>\n",
       "      <td>rslora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9348</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>1548.28</td>\n",
       "      <td>2.89</td>\n",
       "      <td>761.63</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_rslora_8_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>deberta</td>\n",
       "      <td>olora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9339</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>1624.50</td>\n",
       "      <td>2.82</td>\n",
       "      <td>761.63</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_olora_8_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>deberta</td>\n",
       "      <td>dora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9337</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.3149</td>\n",
       "      <td>1962.25</td>\n",
       "      <td>2.87</td>\n",
       "      <td>761.65</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_dora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-rs-olora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9319</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>1606.84</td>\n",
       "      <td>4.00</td>\n",
       "      <td>761.66</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-olora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>deberta</td>\n",
       "      <td>olora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9317</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>1302.84</td>\n",
       "      <td>2.27</td>\n",
       "      <td>761.37</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_olora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9304</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>1549.37</td>\n",
       "      <td>2.97</td>\n",
       "      <td>761.63</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_lora_8_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9299</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.5914</td>\n",
       "      <td>1278.81</td>\n",
       "      <td>2.68</td>\n",
       "      <td>766.35</td>\n",
       "      <td>16</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_lora_16_0.05_16/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-lcoef</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.9202</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>1213.51</td>\n",
       "      <td>4.06</td>\n",
       "      <td>762.21</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-lcoef_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.8628</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>2.3608</td>\n",
       "      <td>359.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>794.41</td>\n",
       "      <td>64</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_lora_64_0.05_64/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.8628</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>1.1812</td>\n",
       "      <td>312.20</td>\n",
       "      <td>0.23</td>\n",
       "      <td>775.63</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_lora_32_0.05_32/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.8592</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.5914</td>\n",
       "      <td>345.41</td>\n",
       "      <td>0.19</td>\n",
       "      <td>767.29</td>\n",
       "      <td>16</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_lora_32_0.05_16/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.5914</td>\n",
       "      <td>383.08</td>\n",
       "      <td>0.21</td>\n",
       "      <td>766.51</td>\n",
       "      <td>16</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_lora_16_0.05_16/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>deberta</td>\n",
       "      <td>rslora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.8484</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>406.86</td>\n",
       "      <td>0.20</td>\n",
       "      <td>761.63</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_rslora_8_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>deberta</td>\n",
       "      <td>rslora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.8484</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>332.67</td>\n",
       "      <td>0.09</td>\n",
       "      <td>761.37</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_rslora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.8448</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>445.43</td>\n",
       "      <td>0.20</td>\n",
       "      <td>761.51</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_lora_8_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>deberta</td>\n",
       "      <td>dora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.8448</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.3149</td>\n",
       "      <td>475.15</td>\n",
       "      <td>0.13</td>\n",
       "      <td>761.65</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_dora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>deberta</td>\n",
       "      <td>dora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.8412</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.3149</td>\n",
       "      <td>487.67</td>\n",
       "      <td>0.24</td>\n",
       "      <td>761.91</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_dora_8_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.8375</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>384.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>762.56</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_lora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-lcoef</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.8303</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>460.06</td>\n",
       "      <td>0.27</td>\n",
       "      <td>761.82</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_mrlora-lcoef_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>deberta</td>\n",
       "      <td>olora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.8195</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>388.83</td>\n",
       "      <td>0.10</td>\n",
       "      <td>761.37</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_olora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>deberta</td>\n",
       "      <td>olora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.8123</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>452.41</td>\n",
       "      <td>0.17</td>\n",
       "      <td>761.63</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_olora_8_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-rs-olora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.7473</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>441.34</td>\n",
       "      <td>0.30</td>\n",
       "      <td>762.06</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-olora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.7029</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>2.3608</td>\n",
       "      <td>821.97</td>\n",
       "      <td>0.54</td>\n",
       "      <td>794.41</td>\n",
       "      <td>64</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_lora_64_0.05_64/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6990</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.5914</td>\n",
       "      <td>752.58</td>\n",
       "      <td>0.57</td>\n",
       "      <td>766.35</td>\n",
       "      <td>16</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_lora_16_0.05_16/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>deberta</td>\n",
       "      <td>rslora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6978</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>772.49</td>\n",
       "      <td>0.26</td>\n",
       "      <td>762.42</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_rslora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6957</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>1.1812</td>\n",
       "      <td>759.95</td>\n",
       "      <td>0.53</td>\n",
       "      <td>775.63</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_lora_32_0.05_32/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>deberta</td>\n",
       "      <td>olora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6944</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>440.49</td>\n",
       "      <td>0.47</td>\n",
       "      <td>762.42</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_olora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6878</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>648.49</td>\n",
       "      <td>0.27</td>\n",
       "      <td>763.20</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_lora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>deberta</td>\n",
       "      <td>dora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6866</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.3149</td>\n",
       "      <td>765.19</td>\n",
       "      <td>0.69</td>\n",
       "      <td>761.91</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_dora_8_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>deberta</td>\n",
       "      <td>rslora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6857</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>569.86</td>\n",
       "      <td>0.63</td>\n",
       "      <td>761.63</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_rslora_8_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>deberta</td>\n",
       "      <td>olora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6850</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>455.84</td>\n",
       "      <td>0.60</td>\n",
       "      <td>761.63</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_olora_8_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>deberta</td>\n",
       "      <td>dora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6835</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.3149</td>\n",
       "      <td>853.40</td>\n",
       "      <td>0.37</td>\n",
       "      <td>762.70</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_dora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>deberta</td>\n",
       "      <td>adalora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6799</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.5917</td>\n",
       "      <td>1538.02</td>\n",
       "      <td>0.35</td>\n",
       "      <td>767.34</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_adalora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>deberta</td>\n",
       "      <td>lora</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6797</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2964</td>\n",
       "      <td>567.52</td>\n",
       "      <td>0.57</td>\n",
       "      <td>761.63</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_lora_8_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>deberta</td>\n",
       "      <td>mrlora-lcoef</td>\n",
       "      <td>cola</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.6627</td>\n",
       "      <td>eval_matthews_correlation</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>1413.60</td>\n",
       "      <td>0.89</td>\n",
       "      <td>762.21</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-lcoef_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>deberta</td>\n",
       "      <td>adalora</td>\n",
       "      <td>rte</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.5957</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.5917</td>\n",
       "      <td>477.23</td>\n",
       "      <td>0.11</td>\n",
       "      <td>766.30</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_adalora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>deberta</td>\n",
       "      <td>adalora</td>\n",
       "      <td>qnli</td>\n",
       "      <td>lora</td>\n",
       "      <td>0.5504</td>\n",
       "      <td>eval_accuracy</td>\n",
       "      <td>0.5917</td>\n",
       "      <td>409.90</td>\n",
       "      <td>2.62</td>\n",
       "      <td>766.30</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_adalora_16_0.05_8/metrics.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      family             peft  task variant   value                     metric  params  traintime  evaltime  gpumem  rank  seed                                                                                              path\n",
       "331  deberta             lora  qnli    lora  0.9390              eval_accuracy  1.1812    1638.15      2.48  775.63    32    42            results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_lora_32_0.05_32/metrics.json\n",
       "337  deberta           rslora  qnli    lora  0.9389              eval_accuracy  0.2964    1663.23      1.91  761.37     8    42           results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_rslora_16_0.05_8/metrics.json\n",
       "333  deberta             lora  qnli    lora  0.9381              eval_accuracy  2.3608    1385.57      2.05  794.41    64    42            results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_lora_64_0.05_64/metrics.json\n",
       "338  deberta             dora  qnli    lora  0.9379              eval_accuracy  0.3149    2775.19      3.27  761.91     8    42              results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_dora_8_0.05_8/metrics.json\n",
       "334  deberta             lora  qnli    lora  0.9363              eval_accuracy  0.2964    1838.87      1.79  762.15     8    42             results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_lora_16_0.05_8/metrics.json\n",
       "329  deberta           rslora  qnli    lora  0.9348              eval_accuracy  0.2964    1548.28      2.89  761.63     8    42            results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_rslora_8_0.05_8/metrics.json\n",
       "328  deberta            olora  qnli    lora  0.9339              eval_accuracy  0.2964    1624.50      2.82  761.63     8    42             results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_olora_8_0.05_8/metrics.json\n",
       "336  deberta             dora  qnli    lora  0.9337              eval_accuracy  0.3149    1962.25      2.87  761.65     8    42             results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_dora_16_0.05_8/metrics.json\n",
       "330  deberta  mrlora-rs-olora  qnli    lora  0.9319              eval_accuracy  0.2964    1606.84      4.00  761.66     8    42  results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-olora_16_0.05_8/metrics.json\n",
       "335  deberta            olora  qnli    lora  0.9317              eval_accuracy  0.2964    1302.84      2.27  761.37     8    42            results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_olora_16_0.05_8/metrics.json\n",
       "339  deberta             lora  qnli    lora  0.9304              eval_accuracy  0.2964    1549.37      2.97  761.63     8    42              results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_lora_8_0.05_8/metrics.json\n",
       "340  deberta             lora  qnli    lora  0.9299              eval_accuracy  0.5914    1278.81      2.68  766.35    16    42            results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_lora_16_0.05_16/metrics.json\n",
       "341  deberta     mrlora-lcoef  qnli    lora  0.9202              eval_accuracy  0.2965    1213.51      4.06  762.21     8    42     results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_mrlora-lcoef_16_0.05_8/metrics.json\n",
       "745  deberta             lora   rte    lora  0.8628              eval_accuracy  2.3608     359.14      0.15  794.41    64    42             results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_lora_64_0.05_64/metrics.json\n",
       "743  deberta             lora   rte    lora  0.8628              eval_accuracy  1.1812     312.20      0.23  775.63    32    42             results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_lora_32_0.05_32/metrics.json\n",
       "752  deberta             lora   rte    lora  0.8592              eval_accuracy  0.5914     345.41      0.19  767.29    16    42             results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_lora_32_0.05_16/metrics.json\n",
       "753  deberta             lora   rte    lora  0.8520              eval_accuracy  0.5914     383.08      0.21  766.51    16    42             results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_lora_16_0.05_16/metrics.json\n",
       "741  deberta           rslora   rte    lora  0.8484              eval_accuracy  0.2964     406.86      0.20  761.63     8    42             results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_rslora_8_0.05_8/metrics.json\n",
       "749  deberta           rslora   rte    lora  0.8484              eval_accuracy  0.2964     332.67      0.09  761.37     8    42            results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_rslora_16_0.05_8/metrics.json\n",
       "751  deberta             lora   rte    lora  0.8448              eval_accuracy  0.2964     445.43      0.20  761.51     8    42               results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_lora_8_0.05_8/metrics.json\n",
       "748  deberta             dora   rte    lora  0.8448              eval_accuracy  0.3149     475.15      0.13  761.65     8    42              results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_dora_16_0.05_8/metrics.json\n",
       "750  deberta             dora   rte    lora  0.8412              eval_accuracy  0.3149     487.67      0.24  761.91     8    42               results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_dora_8_0.05_8/metrics.json\n",
       "746  deberta             lora   rte    lora  0.8375              eval_accuracy  0.2964     384.14      0.09  762.56     8    42              results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_lora_16_0.05_8/metrics.json\n",
       "754  deberta     mrlora-lcoef   rte    lora  0.8303              eval_accuracy  0.2965     460.06      0.27  761.82     8    42      results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_mrlora-lcoef_16_0.05_8/metrics.json\n",
       "747  deberta            olora   rte    lora  0.8195              eval_accuracy  0.2964     388.83      0.10  761.37     8    42             results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_olora_16_0.05_8/metrics.json\n",
       "740  deberta            olora   rte    lora  0.8123              eval_accuracy  0.2964     452.41      0.17  761.63     8    42              results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_olora_8_0.05_8/metrics.json\n",
       "742  deberta  mrlora-rs-olora   rte    lora  0.7473              eval_accuracy  0.2964     441.34      0.30  762.06     8    42   results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_mrlora-rs-olora_16_0.05_8/metrics.json\n",
       "540  deberta             lora  cola    lora  0.7029  eval_matthews_correlation  2.3608     821.97      0.54  794.41    64    42            results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_lora_64_0.05_64/metrics.json\n",
       "547  deberta             lora  cola    lora  0.6990  eval_matthews_correlation  0.5914     752.58      0.57  766.35    16    42            results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_lora_16_0.05_16/metrics.json\n",
       "544  deberta           rslora  cola    lora  0.6978  eval_matthews_correlation  0.2964     772.49      0.26  762.42     8    42           results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_rslora_16_0.05_8/metrics.json\n",
       "538  deberta             lora  cola    lora  0.6957  eval_matthews_correlation  1.1812     759.95      0.53  775.63    32    42            results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_lora_32_0.05_32/metrics.json\n",
       "542  deberta            olora  cola    lora  0.6944  eval_matthews_correlation  0.2964     440.49      0.47  762.42     8    42            results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_olora_16_0.05_8/metrics.json\n",
       "541  deberta             lora  cola    lora  0.6878  eval_matthews_correlation  0.2964     648.49      0.27  763.20     8    42             results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_lora_16_0.05_8/metrics.json\n",
       "545  deberta             dora  cola    lora  0.6866  eval_matthews_correlation  0.3149     765.19      0.69  761.91     8    42              results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_dora_8_0.05_8/metrics.json\n",
       "537  deberta           rslora  cola    lora  0.6857  eval_matthews_correlation  0.2964     569.86      0.63  761.63     8    42            results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_rslora_8_0.05_8/metrics.json\n",
       "536  deberta            olora  cola    lora  0.6850  eval_matthews_correlation  0.2964     455.84      0.60  761.63     8    42             results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_olora_8_0.05_8/metrics.json\n",
       "543  deberta             dora  cola    lora  0.6835  eval_matthews_correlation  0.3149     853.40      0.37  762.70     8    42             results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_dora_16_0.05_8/metrics.json\n",
       "539  deberta          adalora  cola    lora  0.6799  eval_matthews_correlation  0.5917    1538.02      0.35  767.34     8    42          results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_adalora_16_0.05_8/metrics.json\n",
       "546  deberta             lora  cola    lora  0.6797  eval_matthews_correlation  0.2964     567.52      0.57  761.63     8    42              results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_lora_8_0.05_8/metrics.json\n",
       "548  deberta     mrlora-lcoef  cola    lora  0.6627  eval_matthews_correlation  0.2965    1413.60      0.89  762.21     8    42     results/lora/task_cola_deberta_42/base_32_2e-05_0.01/peft_mrlora-lcoef_16_0.05_8/metrics.json\n",
       "744  deberta          adalora   rte    lora  0.5957              eval_accuracy  0.5917     477.23      0.11  766.30     8    42           results/lora/task_rte_deberta_42/base_32_2e-05_0.01/peft_adalora_16_0.05_8/metrics.json\n",
       "332  deberta          adalora  qnli    lora  0.5504              eval_accuracy  0.5917     409.90      2.62  766.30     8    42          results/lora/task_qnli_deberta_42/base_32_2e-05_0.01/peft_adalora_16_0.05_8/metrics.json"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base.sort_values('value', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
