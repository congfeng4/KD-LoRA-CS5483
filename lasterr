d parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
 27%|█████████████████████████████████████████▌                                                                                                                  | 4/15 [00:01<00:03,  2.89it/s]Epoch 1.0: Allocated Memory: 467.16 MB, Reserved Memory: 11263.80 MB
Epoch 1.0: Allocated Memory: 467.16 MB, Reserved Memory: 11263.80 MB
 33%|████████████████████████████████████████████████████                                                                                                        | 5/15 [00:01<00:02,  3.40it/s]Epoch 1.0: Allocated Memory: 467.16 MB, Reserved Memory: 11263.80 MB
Epoch 1.0: Allocated Memory: 467.16 MB, Reserved Memory: 11263.80 MB
{'eval_runtime': 0.0936, 'eval_samples_per_second': 758.305, 'eval_steps_per_second': 10.68, 'epoch': 1.0}                                                                                      
 33%|████████████████████████████████████████████████████                                                                                                        | 5/15 [00:02<00:02,  3.40it/s"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3111, in _save_checkpoint
[rank2]:     metric_value = metrics[metric_to_check]
[rank2]: KeyError: 'eval_loss'

[rank2]: The above exception was the direct cause of the following exception:

[rank2]: Traceback (most recent call last):
[rank2]:   File "src/BERT_Distill_LoRA.py", line 456, in <module>
[rank2]:     main_lora(args_cmd, is_student=False)
[rank2]:   File "src/BERT_Distill_LoRA.py", line 413, in main_lora
[rank2]:     raise e
[rank2]:   File "src/BERT_Distill_LoRA.py", line 410, in main_lora
[rank2]:     pipe.run_teacher_lora()
[rank2]:   File "src/BERT_Distill_LoRA.py", line 310, in run_teacher_lora
[rank2]:     teacher_lora_trainer, train_metrics = self.train_lora(teacher_lora_model, teacher_train_dataset,
[rank2]:   File "src/BERT_Distill_LoRA.py", line 140, in train_lora
[rank2]:     train_output = trainer.train()
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2123, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2573, in _inner_training_loop
[rank2]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3007, in _maybe_log_save_evaluate
[rank2]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3113, in _save_checkpoint
[rank2]:     raise KeyError(
[rank2]: KeyError: "The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3111, in _save_checkpoint
[rank3]:     metric_value = metrics[metric_to_check]
[rank3]: KeyError: 'eval_loss'

[rank3]: The above exception was the direct cause of the following exception:

[rank3]: Traceback (most recent call last):
[rank3]:   File "src/BERT_Distill_LoRA.py", line 456, in <module>
[rank3]:     main_lora(args_cmd, is_student=False)
[rank3]:   File "src/BERT_Distill_LoRA.py", line 413, in main_lora
[rank3]:     raise e
[rank3]:   File "src/BERT_Distill_LoRA.py", line 410, in main_lora
[rank3]:     pipe.run_teacher_lora()
[rank3]:   File "src/BERT_Distill_LoRA.py", line 310, in run_teacher_lora
[rank3]:     teacher_lora_trainer, train_metrics = self.train_lora(teacher_lora_model, teacher_train_dataset,
[rank3]:   File "src/BERT_Distill_LoRA.py", line 140, in train_lora
[rank3]:     train_output = trainer.train()
[rank3]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2123, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2573, in _inner_training_loop
[rank3]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank3]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3007, in _maybe_log_save_evaluate
[rank3]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank3]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3113, in _save_checkpoint
[rank3]:     raise KeyError(
[rank3]: KeyError: "The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3111, in _save_checkpoint
[rank1]:     metric_value = metrics[metric_to_check]
[rank1]: KeyError: 'eval_loss'

[rank1]: The above exception was the direct cause of the following exception:

[rank1]: Traceback (most recent call last):
[rank1]:   File "src/BERT_Distill_LoRA.py", line 456, in <module>
[rank1]:     main_lora(args_cmd, is_student=False)
[rank1]:   File "src/BERT_Distill_LoRA.py", line 413, in main_lora
[rank1]:     raise e
[rank1]:   File "src/BERT_Distill_LoRA.py", line 410, in main_lora
[rank1]:     pipe.run_teacher_lora()
[rank1]:   File "src/BERT_Distill_LoRA.py", line 310, in run_teacher_lora
[rank1]:     teacher_lora_trainer, train_metrics = self.train_lora(teacher_lora_model, teacher_train_dataset,
[rank1]:   File "src/BERT_Distill_LoRA.py", line 140, in train_lora
[rank1]:     train_output = trainer.train()
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2123, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2573, in _inner_training_loop
[rank1]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3007, in _maybe_log_save_evaluate
[rank1]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3113, in _save_checkpoint
[rank1]:     raise KeyError(
[rank1]: KeyError: "The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3111, in _save_checkpoint
[rank0]:     metric_value = metrics[metric_to_check]
[rank0]: KeyError: 'eval_loss'

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "src/BERT_Distill_LoRA.py", line 456, in <module>
[rank0]:     main_lora(args_cmd, is_student=False)
[rank0]:   File "src/BERT_Distill_LoRA.py", line 413, in main_lora
[rank0]:     raise e
[rank0]:   File "src/BERT_Distill_LoRA.py", line 410, in main_lora
[rank0]:     pipe.run_teacher_lora()
[rank0]:   File "src/BERT_Distill_LoRA.py", line 310, in run_teacher_lora
[rank0]:     teacher_lora_trainer, train_metrics = self.train_lora(teacher_lora_model, teacher_train_dataset,
[rank0]:   File "src/BERT_Distill_LoRA.py", line 140, in train_lora
[rank0]:     train_output = trainer.train()
[rank0]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2123, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2573, in _inner_training_loop
[rank0]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank0]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3007, in _maybe_log_save_evaluate
[rank0]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank0]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3113, in _save_checkpoint
[rank0]:     raise KeyError(
[rank0]: KeyError: "The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
 33%|████████████████████████████████████████████████████                                                                                                        | 5/15 [00:02<00:04,  2.19it/s]

 
[rank1]:     output = eval_loop(
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 4169, in evaluation_loop
[rank1]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 4395, in prediction_step
[rank1]:     outputs = model(**inputs)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/utils/operations.py", line 820, in forward
[rank1]:     return model_forward(*args, **kwargs)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/utils/operations.py", line 808, in __call__
[rank1]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/home/user/fc/kd-lora/src/peft/peft_model.py", line 812, in forward
[rank1]:     return self.get_base_model()(*args, **kwargs)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]: TypeError: forward() got an unexpected keyword argument 'idx'
[rank2]: Traceback (most recent call last):
[rank2]:   File "src/BERT_Distill_LoRA.py", line 454, in <module>
[rank2]:     main_lora(args_cmd, is_student=True)
[rank2]:   File "src/BERT_Distill_LoRA.py", line 413, in main_lora
[rank2]:     raise e
[rank2]:   File "src/BERT_Distill_LoRA.py", line 408, in main_lora
[rank2]:     pipe.run_student_lora()
[rank2]:   File "src/BERT_Distill_LoRA.py", line 355, in run_student_lora
[rank2]:     student_trainer, train_metrics = self.train_distill_lora(student_model, student_train_dataset,
[rank2]:   File "src/BERT_Distill_LoRA.py", line 164, in train_distill_lora
[rank2]:     train_output = student_trainer.train()
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2123, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2573, in _inner_training_loop
[rank2]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3004, in _maybe_log_save_evaluate
[rank2]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2958, in _evaluate
[rank2]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3975, in evaluate
[rank2]:     output = eval_loop(
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 4169, in evaluation_loop
[rank2]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 4395, in prediction_step
[rank2]:     outputs = model(**inputs)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/utils/operations.py", line 820, in forward
[rank2]:     return model_forward(*args, **kwargs)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/utils/operations.py", line 808, in __call__
[rank2]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/home/user/fc/kd-lora/src/peft/peft_model.py", line 812, in forward
[rank2]:     return self.get_base_model()(*args, **kwargs)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]: TypeError: forward() got an unexpected keyword argument 'idx'


Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']
Traceback (most recent call last):
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py", line 1633, in convert_slow_tokenizer
    return TikTokenConverter(
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py", line 1533, in converted
    tokenizer = self.tokenizer()
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py", line 1526, in tokenizer
    vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab_file)
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py", line 1502, in extract_vocab_merges_from_model
    bpe_ranks = load_tiktoken_bpe(tiktoken_url)
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/tiktoken/load.py", line 147, in load_tiktoken_bpe
    contents = read_file_cached(tiktoken_bpe_file, expected_hash)
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/tiktoken/load.py", line 49, in read_file_cached
    cache_key = hashlib.sha1(blobpath.encode()).hexdigest()
AttributeError: 'NoneType' object has no attribute 'encode'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "src/BERT_Distill_LoRA.py", line 456, in <module>
    main_lora(args_cmd, is_student=False)
  File "src/BERT_Distill_LoRA.py", line 413, in main_lora
    raise e
  File "src/BERT_Distill_LoRA.py", line 410, in main_lora
    pipe.run_teacher_lora()
  File "src/BERT_Distill_LoRA.py", line 298, in run_teacher_lora
    tokenized_teacher_dataset = self.tokenize_teacher_dataset(teacher_dataset)
  File "src/BERT_Distill_LoRA.py", line 90, in tokenize_teacher_dataset
    teacher_tokenizer = AutoTokenizer.from_pretrained(args.teacher_model_name)
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py", line 939, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2213, in from_pretrained
    return cls._from_pretrained(
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2447, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py", line 103, in __init__
    super().__init__(
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py", line 138, in __init__
    fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py", line 1638, in convert_slow_tokenizer
    raise ValueError(
ValueError: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast convertors: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']
W0125 14:35:08.840046 140527309575360 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1637332 closing signal SIGTERM
W0125 14:35:08.841124 140527309575360 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1637339 closing signal SIGTERM
E0125 14:35:08.956007 140527309575360 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 1637338) of binary: /home/user/anaconda3/envs/lora/bin/python
Traceback (most recent call last):
  File "/home/user/anaconda3/envs/lora/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================

[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/data_loader.py", line 550, in __iter__
[rank2]:     current_batch = next(dataloader_iter)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
[rank2]:     data = self._next_data()
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
[rank2]:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
[rank2]:     return self.collate_fn(data)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/data/data_collator.py", line 92, in default_data_collator
[rank2]:     return torch_default_data_collator(features)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/data/data_collator.py", line 158, in torch_default_data_collator
[rank2]:     batch[k] = torch.tensor([f[k] for f in features])
[rank2]: ValueError: expected sequence of length 30 at dim 1 (got 25)
[rank1]: Traceback (most recent call last):
[rank1]:   File "src/BERT_Distill_LoRA.py", line 456, in <module>
[rank1]:     main_lora(args_cmd, is_student=False)
[rank1]:   File "src/BERT_Distill_LoRA.py", line 413, in main_lora
[rank1]:     raise e
[rank1]:   File "src/BERT_Distill_LoRA.py", line 410, in main_lora
[rank1]:     pipe.run_teacher_lora()
[rank1]:   File "src/BERT_Distill_LoRA.py", line 310, in run_teacher_lora
[rank1]:     teacher_lora_trainer, train_metrics = self.train_lora(teacher_lora_model, teacher_train_dataset,
[rank1]:   File "src/BERT_Distill_LoRA.py", line 140, in train_lora
[rank1]:     train_output = trainer.train()
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2123, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2427, in _inner_training_loop
[rank1]:     batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 5045, in get_batch_samples
[rank1]:     batch_samples += [next(epoch_iterator)]
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/data_loader.py", line 550, in __iter__
[rank1]:     current_batch = next(dataloader_iter)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
[rank1]:     data = self._next_data()
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 673, in _next_data
[rank1]:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
[rank1]:     return self.collate_fn(data)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/data/data_collator.py", line 92, in default_data_collator
[rank1]:     return torch_default_data_collator(features)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/data/data_collator.py", line 158, in torch_default_data_collator
[rank1]:     batch[k] = torch.tensor([f[k] for f in features])
[rank1]: ValueError: expected sequence of length 19 at dim 1 (got 74)
  0%|                                                                                                                                             | 0/15 [00:00<?, ?it/s]
W0125 14:40:08.409942 139864843846848 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1654300 closing signal SIGTERM
W0125 14:40:08.410661 139864843846848 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1654301 closing signal SIGTERM
E0125 14:40:08.538784 139864843846848 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 2 (pid: 1654307) of binary: /home/user/anaconda3/envs/lora/bin/python
Traceback (most recent call last):
  File "/home/user/anaconda3/envs/lora/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/commands/launch.py", line 1159, in launch_command
    multi_gpu_launcher(args)
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/commands/launch.py", line 793, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
src/BERT_Distill_LoRA.py FAILED