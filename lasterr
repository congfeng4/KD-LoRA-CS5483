d parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
 27%|█████████████████████████████████████████▌                                                                                                                  | 4/15 [00:01<00:03,  2.89it/s]Epoch 1.0: Allocated Memory: 467.16 MB, Reserved Memory: 11263.80 MB
Epoch 1.0: Allocated Memory: 467.16 MB, Reserved Memory: 11263.80 MB
 33%|████████████████████████████████████████████████████                                                                                                        | 5/15 [00:01<00:02,  3.40it/s]Epoch 1.0: Allocated Memory: 467.16 MB, Reserved Memory: 11263.80 MB
Epoch 1.0: Allocated Memory: 467.16 MB, Reserved Memory: 11263.80 MB
{'eval_runtime': 0.0936, 'eval_samples_per_second': 758.305, 'eval_steps_per_second': 10.68, 'epoch': 1.0}                                                                                      
 33%|████████████████████████████████████████████████████                                                                                                        | 5/15 [00:02<00:02,  3.40it/s"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
[rank2]: Traceback (most recent call last):
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3111, in _save_checkpoint
[rank2]:     metric_value = metrics[metric_to_check]
[rank2]: KeyError: 'eval_loss'

[rank2]: The above exception was the direct cause of the following exception:

[rank2]: Traceback (most recent call last):
[rank2]:   File "src/BERT_Distill_LoRA.py", line 456, in <module>
[rank2]:     main_lora(args_cmd, is_student=False)
[rank2]:   File "src/BERT_Distill_LoRA.py", line 413, in main_lora
[rank2]:     raise e
[rank2]:   File "src/BERT_Distill_LoRA.py", line 410, in main_lora
[rank2]:     pipe.run_teacher_lora()
[rank2]:   File "src/BERT_Distill_LoRA.py", line 310, in run_teacher_lora
[rank2]:     teacher_lora_trainer, train_metrics = self.train_lora(teacher_lora_model, teacher_train_dataset,
[rank2]:   File "src/BERT_Distill_LoRA.py", line 140, in train_lora
[rank2]:     train_output = trainer.train()
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2123, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2573, in _inner_training_loop
[rank2]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3007, in _maybe_log_save_evaluate
[rank2]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3113, in _save_checkpoint
[rank2]:     raise KeyError(
[rank2]: KeyError: "The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3111, in _save_checkpoint
[rank3]:     metric_value = metrics[metric_to_check]
[rank3]: KeyError: 'eval_loss'

[rank3]: The above exception was the direct cause of the following exception:

[rank3]: Traceback (most recent call last):
[rank3]:   File "src/BERT_Distill_LoRA.py", line 456, in <module>
[rank3]:     main_lora(args_cmd, is_student=False)
[rank3]:   File "src/BERT_Distill_LoRA.py", line 413, in main_lora
[rank3]:     raise e
[rank3]:   File "src/BERT_Distill_LoRA.py", line 410, in main_lora
[rank3]:     pipe.run_teacher_lora()
[rank3]:   File "src/BERT_Distill_LoRA.py", line 310, in run_teacher_lora
[rank3]:     teacher_lora_trainer, train_metrics = self.train_lora(teacher_lora_model, teacher_train_dataset,
[rank3]:   File "src/BERT_Distill_LoRA.py", line 140, in train_lora
[rank3]:     train_output = trainer.train()
[rank3]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2123, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2573, in _inner_training_loop
[rank3]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank3]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3007, in _maybe_log_save_evaluate
[rank3]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank3]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3113, in _save_checkpoint
[rank3]:     raise KeyError(
[rank3]: KeyError: "The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3111, in _save_checkpoint
[rank1]:     metric_value = metrics[metric_to_check]
[rank1]: KeyError: 'eval_loss'

[rank1]: The above exception was the direct cause of the following exception:

[rank1]: Traceback (most recent call last):
[rank1]:   File "src/BERT_Distill_LoRA.py", line 456, in <module>
[rank1]:     main_lora(args_cmd, is_student=False)
[rank1]:   File "src/BERT_Distill_LoRA.py", line 413, in main_lora
[rank1]:     raise e
[rank1]:   File "src/BERT_Distill_LoRA.py", line 410, in main_lora
[rank1]:     pipe.run_teacher_lora()
[rank1]:   File "src/BERT_Distill_LoRA.py", line 310, in run_teacher_lora
[rank1]:     teacher_lora_trainer, train_metrics = self.train_lora(teacher_lora_model, teacher_train_dataset,
[rank1]:   File "src/BERT_Distill_LoRA.py", line 140, in train_lora
[rank1]:     train_output = trainer.train()
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2123, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2573, in _inner_training_loop
[rank1]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3007, in _maybe_log_save_evaluate
[rank1]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3113, in _save_checkpoint
[rank1]:     raise KeyError(
[rank1]: KeyError: "The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3111, in _save_checkpoint
[rank0]:     metric_value = metrics[metric_to_check]
[rank0]: KeyError: 'eval_loss'

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "src/BERT_Distill_LoRA.py", line 456, in <module>
[rank0]:     main_lora(args_cmd, is_student=False)
[rank0]:   File "src/BERT_Distill_LoRA.py", line 413, in main_lora
[rank0]:     raise e
[rank0]:   File "src/BERT_Distill_LoRA.py", line 410, in main_lora
[rank0]:     pipe.run_teacher_lora()
[rank0]:   File "src/BERT_Distill_LoRA.py", line 310, in run_teacher_lora
[rank0]:     teacher_lora_trainer, train_metrics = self.train_lora(teacher_lora_model, teacher_train_dataset,
[rank0]:   File "src/BERT_Distill_LoRA.py", line 140, in train_lora
[rank0]:     train_output = trainer.train()
[rank0]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2123, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2573, in _inner_training_loop
[rank0]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank0]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3007, in _maybe_log_save_evaluate
[rank0]:     self._save_checkpoint(model, trial, metrics=metrics)
[rank0]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3113, in _save_checkpoint
[rank0]:     raise KeyError(
[rank0]: KeyError: "The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: ['eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch']. Please ensure that the `compute_metrics` function returns a dictionary that includes 'eval_loss' or consider changing the `metric_for_best_model` via the TrainingArguments."
 33%|████████████████████████████████████████████████████                                                                                                        | 5/15 [00:02<00:04,  2.19it/s]

 
[rank1]:     output = eval_loop(
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 4169, in evaluation_loop
[rank1]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 4395, in prediction_step
[rank1]:     outputs = model(**inputs)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/utils/operations.py", line 820, in forward
[rank1]:     return model_forward(*args, **kwargs)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/utils/operations.py", line 808, in __call__
[rank1]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/home/user/fc/kd-lora/src/peft/peft_model.py", line 812, in forward
[rank1]:     return self.get_base_model()(*args, **kwargs)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]: TypeError: forward() got an unexpected keyword argument 'idx'
[rank2]: Traceback (most recent call last):
[rank2]:   File "src/BERT_Distill_LoRA.py", line 454, in <module>
[rank2]:     main_lora(args_cmd, is_student=True)
[rank2]:   File "src/BERT_Distill_LoRA.py", line 413, in main_lora
[rank2]:     raise e
[rank2]:   File "src/BERT_Distill_LoRA.py", line 408, in main_lora
[rank2]:     pipe.run_student_lora()
[rank2]:   File "src/BERT_Distill_LoRA.py", line 355, in run_student_lora
[rank2]:     student_trainer, train_metrics = self.train_distill_lora(student_model, student_train_dataset,
[rank2]:   File "src/BERT_Distill_LoRA.py", line 164, in train_distill_lora
[rank2]:     train_output = student_trainer.train()
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2123, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2573, in _inner_training_loop
[rank2]:     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3004, in _maybe_log_save_evaluate
[rank2]:     metrics = self._evaluate(trial, ignore_keys_for_eval)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 2958, in _evaluate
[rank2]:     metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 3975, in evaluate
[rank2]:     output = eval_loop(
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 4169, in evaluation_loop
[rank2]:     losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/transformers/trainer.py", line 4395, in prediction_step
[rank2]:     outputs = model(**inputs)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/utils/operations.py", line 820, in forward
[rank2]:     return model_forward(*args, **kwargs)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/accelerate/utils/operations.py", line 808, in __call__
[rank2]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/home/user/fc/kd-lora/src/peft/peft_model.py", line 812, in forward
[rank2]:     return self.get_base_model()(*args, **kwargs)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/home/user/anaconda3/envs/lora/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]: TypeError: forward() got an unexpected keyword argument 'idx'